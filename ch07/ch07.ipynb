{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.7 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Structure of CNN\n",
    "\n",
    "Sample of CNN\n",
    "![img](./fig/fig7_1.drawio.svg)\n",
    "\n",
    "## 7.2 Convolution layers\n",
    "\n",
    "### 7.2.1 Limitations of fully connected layers\n",
    "\n",
    "The problem of Affine layer is that 2D (or more multiple dimentions) information is lost since all the data is transformed to 1D.\n",
    "This means that important information especially images are lost.\n",
    "\n",
    "Normaly 2D image data contains 3D information such as RGB color information.\n",
    "\n",
    "## 7.3 Pooling layers\n",
    "\n",
    "Pooling layer is used to reduce the size of the feature map.\n",
    "Max value or Average value in the filter area is taken as the specific value in the feature map.\n",
    "\n",
    "## 7.4 Convolutional Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n",
      "(1, 28, 28)\n",
      "(28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# 7.4 Convolutiaon / Pooling Layer\n",
    "import numpy as np\n",
    "x = np.random.rand(10, 1, 28, 28)\n",
    "\n",
    "print(x.shape)\n",
    "print(x[0].shape)\n",
    "print(x[0,0].shape)\n",
    "print(x[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.2 im2col\n",
    "\n",
    "The im2col function is used to convert 4D array (N, C, H, W) into 2D array (N*H*W, C*filter_h*filter_w).\n",
    "im2col is optimized to transform 4D array into 2D array so that the calculation is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    #print(col.shape)\n",
    "    #print(N, C, filter_h, filter_w, out_h, out_w)\n",
    "    #print(col.transpose(0, 4, 5, 1, 2, 3).shape)\n",
    "    #print(col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1).shape)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "#col = im2col(x, 5, 5, stride=1, pad=0)\n",
    "#print(col.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y:y_max:strideについて、\n",
    "\"0:4:2\"という表記は、「0列目から4列目まで、2つ飛ばしで」という意味です。\n",
    "-> 実直にforを回そうと思うと、3x3のフィルタ(9マス)の走査をOH　x OWの回数分行うところを、一回にまとめる実装になっている\n",
    "\n",
    "[im2colが非常によくわかる](https://qiita.com/kurumen-b/items/236c6255959a266cefaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.3 implementation of convolution layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m col \u001b[38;5;241m=\u001b[39m im2col(x, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(col\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[0;32mIn[64], line 16\u001b[0m, in \u001b[0;36mim2col\u001b[0;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mim2col\u001b[39m(input_data, filter_h, filter_w, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    col : 2次元配列\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     N, C, H, W \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     17\u001b[0m     out_h \u001b[38;5;241m=\u001b[39m (H \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mpad \u001b[38;5;241m-\u001b[39m filter_h)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     18\u001b[0m     out_w \u001b[38;5;241m=\u001b[39m (W \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mpad \u001b[38;5;241m-\u001b[39m filter_w)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mstride \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "col = im2col(x, 5, 5, stride=1, pad=0)\n",
    "print(col.shape)\n",
    "\n",
    "print(x.reshape(10, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\n",
    "    filter_h :\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) # (5760, 25)\n",
    "        col_W = self.W.reshape(FN, -1).T # Col_W (FN=30, C*FH*FW = 25) => .T = (25, 30)\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        # (5760, FN=3) -> (batch_N=10, OH=24, OW=24, FN=30) -> (N, FN, OH, OW)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN) \n",
    "        # (N, FN, OH, OW) -> (N=10, OH=24, OW=24, FN=30) -> reshape (5760, FN=30)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0) # Shape(30, )\n",
    "        self.dW = np.dot(self.col.T, dout) # W.T*dout = (25, 5760) * (5760, 30) = (C*FH*FW=25, FN. 30)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T) #(5760, 30)*col_W.T=(30,25) = (5760, 25 c.fh.fw)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad) # (N=10, C=1, 28, 28)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.20921943e-03 9.96735872e-03 3.65423547e-03 ... 2.44282867e-03\n",
      "  9.25925387e-04 5.09423785e-03]\n",
      " [9.61972201e-05 4.96510314e-03 4.38271388e-03 ... 3.80231335e-03\n",
      "  9.45619902e-03 5.45885777e-04]\n",
      " [8.88213341e-03 7.59812333e-03 6.68286489e-03 ... 8.69561094e-03\n",
      "  9.94672452e-03 9.54201264e-03]\n",
      " ...\n",
      " [3.31858452e-03 8.29061798e-04 6.07921308e-03 ... 1.38363962e-03\n",
      "  4.00503474e-03 1.15145075e-03]\n",
      " [4.29905774e-03 5.92205691e-03 1.96561229e-03 ... 5.83790334e-03\n",
      "  6.14939596e-03 8.33789816e-03]\n",
      " [8.83323829e-03 3.37078683e-03 9.89587765e-03 ... 7.89498932e-03\n",
      "  4.38590418e-03 2.28095914e-03]]\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "# create nparray which has 2 dim and shape (5760, 30) and filled with radom numbers\n",
    "dout_t = np.random.rand(5760, 30)*0.01\n",
    "print(dout_t)\n",
    "print(np.sum(dout_t, axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.4 implementation of pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.5 implementation of fully connected layer\n",
    "\n",
    "![img](./fig/fig7_5.drawio.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 重み・バイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # テンソル対応\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "\n",
    "    \"\"\"確率的勾配降下法（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum:\n",
    "\n",
    "    \"\"\"Momentum SGD\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "\n",
    "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "\n",
    "    \"\"\"AdaGrad\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "\n",
    "    \"\"\"RMSprop\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer:\n",
    "    \"\"\"ニューラルネットの訓練を行うクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimizer\n",
    "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
    "                                'adagrad':AdaGrad, 'rmsprop':RMSprop, 'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            print('current iter num: ', self.current_iter)\n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4.5 implemenation of CNN\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                conv_param={'filter_num':30, 'filter_size':5,'pad':0, 'stride':1},\n",
    "                hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / \\\n",
    "                            filter_stride +1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * \n",
    "                               (conv_output_size/2))\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0],\n",
    "                                            filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],\n",
    "                                           self.params['b1'],\n",
    "                                           conv_param['stride'],\n",
    "                                           conv_param['pad'])\n",
    "        self.layers['Relu'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],\n",
    "                                        self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'],\n",
    "                                         self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        # configuration\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    # ここから解説になかった部分\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.300205036860067\n",
      "current iter num:  0\n",
      "=== epoch:1, train acc:0.438, test acc:0.394 ===\n",
      "train loss:2.2975927173754194\n",
      "train loss:2.2927964223350887\n",
      "train loss:2.28632448182457\n",
      "train loss:2.2821174219751397\n",
      "train loss:2.2747342611501464\n",
      "train loss:2.2712842284449524\n",
      "train loss:2.246479586174833\n",
      "train loss:2.2338463539847764\n",
      "train loss:2.1958679288666616\n",
      "train loss:2.188009998021936\n",
      "train loss:2.127692441536307\n",
      "train loss:2.107623458934975\n",
      "train loss:2.0542760371121833\n",
      "train loss:1.9548573497525592\n",
      "train loss:1.9474307563861664\n",
      "train loss:1.8784206519942341\n",
      "train loss:1.817232994870078\n",
      "train loss:1.750140586298993\n",
      "train loss:1.6597056359932771\n",
      "train loss:1.6292461638930213\n",
      "train loss:1.3918234382257786\n",
      "train loss:1.4058891385368795\n",
      "train loss:1.365027540411961\n",
      "train loss:1.2794225370395205\n",
      "train loss:1.2993331789081781\n",
      "train loss:1.0145414024784989\n",
      "train loss:1.0761527642289332\n",
      "train loss:1.087314374665644\n",
      "train loss:0.8863097505990636\n",
      "train loss:1.026430437697171\n",
      "train loss:0.8728197993445681\n",
      "train loss:0.8779620682081188\n",
      "train loss:0.8494510117570442\n",
      "train loss:0.7914266500081432\n",
      "train loss:0.7869105722620432\n",
      "train loss:0.6293474853092906\n",
      "train loss:0.717559331590847\n",
      "train loss:0.7495203879566675\n",
      "train loss:0.76737298072007\n",
      "train loss:0.6841210867714652\n",
      "train loss:0.6741719670280528\n",
      "train loss:0.6001845661361975\n",
      "train loss:0.5602140356200515\n",
      "train loss:0.5388210010517462\n",
      "train loss:0.6243621205708937\n",
      "train loss:0.5928413958008799\n",
      "train loss:0.8575264856717459\n",
      "train loss:0.7413126807309244\n",
      "train loss:0.4981007760881175\n",
      "train loss:0.5120039768728066\n",
      "train loss:0.4532294872504533\n",
      "train loss:0.7003438488689383\n",
      "train loss:0.8197582581500462\n",
      "train loss:0.48691262203703334\n",
      "train loss:0.6793679804337361\n",
      "train loss:0.536165269150151\n",
      "train loss:0.6839004547357602\n",
      "train loss:0.466834967031677\n",
      "train loss:0.3552122438804212\n",
      "train loss:0.460655503054244\n",
      "train loss:0.5467304983798136\n",
      "train loss:0.3939550843992722\n",
      "train loss:0.40174819613602186\n",
      "train loss:0.5896097899410845\n",
      "train loss:0.7985812527822906\n",
      "train loss:0.5516494838140001\n",
      "train loss:0.5075847162465658\n",
      "train loss:0.3902467634129486\n",
      "train loss:0.47389722606318435\n",
      "train loss:0.5809342124750669\n",
      "train loss:0.46880603669335136\n",
      "train loss:0.41225843398578954\n",
      "train loss:0.3480484682870527\n",
      "train loss:0.40400766357234646\n",
      "train loss:0.5191454641732334\n",
      "train loss:0.4938379458391095\n",
      "train loss:0.6143244071728783\n",
      "train loss:0.36707828822916083\n",
      "train loss:0.3283521516964051\n",
      "train loss:0.537602731165\n",
      "train loss:0.3805008164260817\n",
      "train loss:0.4063403374867162\n",
      "train loss:0.4455325373902359\n",
      "train loss:0.3077405548690813\n",
      "train loss:0.3297915525122532\n",
      "train loss:0.42855798340743334\n",
      "train loss:0.5293002076168941\n",
      "train loss:0.46390056930267226\n",
      "train loss:0.5643002491773441\n",
      "train loss:0.2971371332517387\n",
      "train loss:0.3606818553075604\n",
      "train loss:0.26914669094577737\n",
      "train loss:0.42970492554568324\n",
      "train loss:0.3513965493515944\n",
      "train loss:0.510984093468032\n",
      "train loss:0.33804294123751\n",
      "train loss:0.39589179650037076\n",
      "train loss:0.4811407395541786\n",
      "train loss:0.2818586916100115\n",
      "train loss:0.5694603069205689\n",
      "train loss:0.25657783143306107\n",
      "train loss:0.34517997219400004\n",
      "train loss:0.392575779110736\n",
      "train loss:0.2608545689268365\n",
      "train loss:0.4252805594341661\n",
      "train loss:0.3357689968063566\n",
      "train loss:0.32206522871303894\n",
      "train loss:0.2036654751527792\n",
      "train loss:0.24222756220608715\n",
      "train loss:0.3368251239502904\n",
      "train loss:0.32641332820505475\n",
      "train loss:0.299709070652044\n",
      "train loss:0.4411509016034595\n",
      "train loss:0.2470970384463307\n",
      "train loss:0.35969415007179767\n",
      "train loss:0.49484313957139003\n",
      "train loss:0.2411817342127771\n",
      "train loss:0.23950823293735787\n",
      "train loss:0.28484086638771783\n",
      "train loss:0.45287355502508325\n",
      "train loss:0.371158377607612\n",
      "train loss:0.16897724765918995\n",
      "train loss:0.4912002615173738\n",
      "train loss:0.29106199852108633\n",
      "train loss:0.23757116671777825\n",
      "train loss:0.2622157254991634\n",
      "train loss:0.3555736984349288\n",
      "train loss:0.34376801678112723\n",
      "train loss:0.35428669070225566\n",
      "train loss:0.4187388680344363\n",
      "train loss:0.2061862896705531\n",
      "train loss:0.5028916177614916\n",
      "train loss:0.25428498423105916\n",
      "train loss:0.45352978580314635\n",
      "train loss:0.47572021859409525\n",
      "train loss:0.27443487453640386\n",
      "train loss:0.3563533982466634\n",
      "train loss:0.3626685337423464\n",
      "train loss:0.4116248925390714\n",
      "train loss:0.34541230185179034\n",
      "train loss:0.23823637363748673\n",
      "train loss:0.39495637265843014\n",
      "train loss:0.4325446908693248\n",
      "train loss:0.32937656099710233\n",
      "train loss:0.6566107875351069\n",
      "train loss:0.3933119413112791\n",
      "train loss:0.2543101267962163\n",
      "train loss:0.24826333200419992\n",
      "train loss:0.42608233738123036\n",
      "train loss:0.3215678914262952\n",
      "train loss:0.4248678942863355\n",
      "train loss:0.42286370601865664\n",
      "train loss:0.3443345217509174\n",
      "train loss:0.348066143885276\n",
      "train loss:0.14156137445631023\n",
      "train loss:0.3508167640657872\n",
      "train loss:0.4118502979099096\n",
      "train loss:0.30163164895429995\n",
      "train loss:0.38835181779520966\n",
      "train loss:0.34561644386517676\n",
      "train loss:0.20478064321974002\n",
      "train loss:0.2568633093146866\n",
      "train loss:0.4009628173074391\n",
      "train loss:0.4607810393646541\n",
      "train loss:0.3001081062608605\n",
      "train loss:0.3105726303191288\n",
      "train loss:0.25751989976560835\n",
      "train loss:0.18544108167401202\n",
      "train loss:0.3876647071188243\n",
      "train loss:0.30814258279654205\n",
      "train loss:0.25648489983821526\n",
      "train loss:0.39796351801889684\n",
      "train loss:0.2389585281818821\n",
      "train loss:0.24436299318384302\n",
      "train loss:0.30432215891318776\n",
      "train loss:0.3181624050839561\n",
      "train loss:0.25522352343636834\n",
      "train loss:0.40248705854774697\n",
      "train loss:0.44571165142445957\n",
      "train loss:0.42742010709553435\n",
      "train loss:0.44985527787418983\n",
      "train loss:0.21103575023548185\n",
      "train loss:0.17367636943541126\n",
      "train loss:0.23892933424552715\n",
      "train loss:0.2985493620696559\n",
      "train loss:0.273990891016996\n",
      "train loss:0.256963731914893\n",
      "train loss:0.23299303996497805\n",
      "train loss:0.3073790672859926\n",
      "train loss:0.2419455715488316\n",
      "train loss:0.29250203624436294\n",
      "train loss:0.35726420059704384\n",
      "train loss:0.24095648172037032\n",
      "train loss:0.4399485383635123\n",
      "train loss:0.3101635428901763\n",
      "train loss:0.2656622435563772\n",
      "train loss:0.407547503774501\n",
      "train loss:0.3084265443270197\n",
      "train loss:0.27878683588224634\n",
      "train loss:0.16410763055862587\n",
      "train loss:0.16120674791685974\n",
      "train loss:0.19293248904627994\n",
      "train loss:0.1826916986350555\n",
      "train loss:0.2797384232481215\n",
      "train loss:0.15519663753768154\n",
      "train loss:0.2697099797166393\n",
      "train loss:0.32573252658556867\n",
      "train loss:0.396999127950308\n",
      "train loss:0.20588982736677086\n",
      "train loss:0.3281081785717792\n",
      "train loss:0.29763540283273543\n",
      "train loss:0.28605428239390274\n",
      "train loss:0.2676454447373132\n",
      "train loss:0.3747457265887576\n",
      "train loss:0.2554669939948525\n",
      "train loss:0.2117073851962136\n",
      "train loss:0.2296857888791331\n",
      "train loss:0.2106673147337342\n",
      "train loss:0.3146697752249731\n",
      "train loss:0.20860325638473529\n",
      "train loss:0.22831252565229923\n",
      "train loss:0.3479456632827824\n",
      "train loss:0.2313571775874755\n",
      "train loss:0.34782117549732094\n",
      "train loss:0.27349330807457334\n",
      "train loss:0.3058498896555918\n",
      "train loss:0.34531784691553646\n",
      "train loss:0.16385074418034962\n",
      "train loss:0.2964924382295292\n",
      "train loss:0.2029627633098291\n",
      "train loss:0.2189362836680618\n",
      "train loss:0.21191154422822175\n",
      "train loss:0.23317179876485597\n",
      "train loss:0.35068863714092197\n",
      "train loss:0.36170647509860193\n",
      "train loss:0.2153070777149214\n",
      "train loss:0.15382763701028918\n",
      "train loss:0.26897253640271573\n",
      "train loss:0.2342149121633274\n",
      "train loss:0.13959959839092892\n",
      "train loss:0.25805510537726606\n",
      "train loss:0.2659569858909537\n",
      "train loss:0.2397966675470666\n",
      "train loss:0.15181338979968367\n",
      "train loss:0.1818855624430088\n",
      "train loss:0.2438862832212541\n",
      "train loss:0.12853028606129452\n",
      "train loss:0.1710795155620325\n",
      "train loss:0.2309730452507366\n",
      "train loss:0.27385874283163286\n",
      "train loss:0.16755093523939435\n",
      "train loss:0.15060447540910032\n",
      "train loss:0.15380003520255944\n",
      "train loss:0.25572731245700786\n",
      "train loss:0.17492068662069293\n",
      "train loss:0.30993658397477114\n",
      "train loss:0.13846264400036498\n",
      "train loss:0.1280129168088215\n",
      "train loss:0.25502242676401365\n",
      "train loss:0.3514992215219608\n",
      "train loss:0.1777513225489802\n",
      "train loss:0.26953260979334037\n",
      "train loss:0.22703107028450592\n",
      "train loss:0.24715953632741022\n",
      "train loss:0.09849131873195223\n",
      "train loss:0.38242189418247574\n",
      "train loss:0.16808108460936502\n",
      "train loss:0.12241442148865588\n",
      "train loss:0.30485801139230906\n",
      "train loss:0.26653065692794525\n",
      "train loss:0.35678705846389297\n",
      "train loss:0.24935701884682335\n",
      "train loss:0.30273904104021104\n",
      "train loss:0.39315209328086953\n",
      "train loss:0.33270445057928255\n",
      "train loss:0.176990157768965\n",
      "train loss:0.150511158221366\n",
      "train loss:0.19606146717426298\n",
      "train loss:0.21395419600533686\n",
      "train loss:0.181376566261292\n",
      "train loss:0.10782554304111755\n",
      "train loss:0.18683163898727867\n",
      "train loss:0.12419836725879226\n",
      "train loss:0.23014647305998698\n",
      "train loss:0.14048853847907994\n",
      "train loss:0.18172623552219544\n",
      "train loss:0.08050253281398095\n",
      "train loss:0.14694483086747442\n",
      "train loss:0.21473680200273104\n",
      "train loss:0.3483687762318158\n",
      "train loss:0.3455630405836915\n",
      "train loss:0.16655491208777262\n",
      "train loss:0.16901560342982958\n",
      "train loss:0.26157134619413014\n",
      "train loss:0.11939062969140883\n",
      "train loss:0.30673767150371267\n",
      "train loss:0.22269849516250192\n",
      "train loss:0.14048944806618488\n",
      "train loss:0.26599043666715805\n",
      "train loss:0.20591505272748672\n",
      "train loss:0.26537589304709125\n",
      "train loss:0.10815139548547348\n",
      "train loss:0.1872477068994244\n",
      "train loss:0.15504083037239189\n",
      "train loss:0.21297462267803036\n",
      "train loss:0.17374754857155422\n",
      "train loss:0.2661241193532206\n",
      "train loss:0.15792787904574515\n",
      "train loss:0.17780988718825186\n",
      "train loss:0.18279369824528394\n",
      "train loss:0.37750407887688725\n",
      "train loss:0.23676635943726065\n",
      "train loss:0.2875220453977113\n",
      "train loss:0.3483360766823088\n",
      "train loss:0.1855235008641229\n",
      "train loss:0.1625770149048569\n",
      "train loss:0.10985354323681715\n",
      "train loss:0.14605534001344864\n",
      "train loss:0.23856636391805913\n",
      "train loss:0.25607573529898475\n",
      "train loss:0.13343541283115593\n",
      "train loss:0.08886218622014393\n",
      "train loss:0.13119642483272803\n",
      "train loss:0.1768725519429714\n",
      "train loss:0.27288723975438084\n",
      "train loss:0.2825508891640998\n",
      "train loss:0.2562576595771456\n",
      "train loss:0.13169824362734453\n",
      "train loss:0.07619339879602029\n",
      "train loss:0.23479604960081624\n",
      "train loss:0.16203318255919733\n",
      "train loss:0.21221724721851828\n",
      "train loss:0.15159484026998532\n",
      "train loss:0.11388048866453197\n",
      "train loss:0.15847419598491103\n",
      "train loss:0.12626964232511798\n",
      "train loss:0.25272938863841254\n",
      "train loss:0.1637685963803375\n",
      "train loss:0.15287528715180185\n",
      "train loss:0.186226374199734\n",
      "train loss:0.06371629960280625\n",
      "train loss:0.09820095759917503\n",
      "train loss:0.16505052081657648\n",
      "train loss:0.1713300523104067\n",
      "train loss:0.3139051128584411\n",
      "train loss:0.126501846937918\n",
      "train loss:0.15304358681324848\n",
      "train loss:0.10728530146377838\n",
      "train loss:0.1405045478696022\n",
      "train loss:0.17137723512848374\n",
      "train loss:0.1944313162016926\n",
      "train loss:0.23664772437094597\n",
      "train loss:0.13145689518902046\n",
      "train loss:0.23171524414631797\n",
      "train loss:0.13593683695926043\n",
      "train loss:0.08096072451836585\n",
      "train loss:0.16147955309284653\n",
      "train loss:0.19897601105494134\n",
      "train loss:0.1483857036331633\n",
      "train loss:0.16243340150757823\n",
      "train loss:0.23377706616696062\n",
      "train loss:0.14517987940739796\n",
      "train loss:0.10685234172828809\n",
      "train loss:0.06702827253451576\n",
      "train loss:0.18895704380640016\n",
      "train loss:0.14661075503711485\n",
      "train loss:0.2582162543140774\n",
      "train loss:0.21737277377088735\n",
      "train loss:0.2066808420554035\n",
      "train loss:0.27194554996746595\n",
      "train loss:0.11204829717418194\n",
      "train loss:0.14619055154725136\n",
      "train loss:0.18647918119451773\n",
      "train loss:0.08428302553582817\n",
      "train loss:0.1805600455586585\n",
      "train loss:0.11981599558773301\n",
      "train loss:0.14557945912707063\n",
      "train loss:0.22367929192725594\n",
      "train loss:0.21103001374107414\n",
      "train loss:0.07647382997119184\n",
      "train loss:0.11565903314752869\n",
      "train loss:0.07882261568659009\n",
      "train loss:0.13073298448040999\n",
      "train loss:0.10632128890148447\n",
      "train loss:0.09181933736840803\n",
      "train loss:0.09762018934987365\n",
      "train loss:0.23438164161397615\n",
      "train loss:0.08219718489395048\n",
      "train loss:0.25438751265019344\n",
      "train loss:0.13706712077697428\n",
      "train loss:0.17330061283013656\n",
      "train loss:0.08845604822493629\n",
      "train loss:0.17990237107350487\n",
      "train loss:0.17246144468852947\n",
      "train loss:0.13335784035717752\n",
      "train loss:0.23929781011958362\n",
      "train loss:0.07492170151725518\n",
      "train loss:0.08659852068111923\n",
      "train loss:0.1464667494876136\n",
      "train loss:0.05717096216240627\n",
      "train loss:0.1753791334838617\n",
      "train loss:0.12147832651430403\n",
      "train loss:0.12216947942798269\n",
      "train loss:0.07515637839531847\n",
      "train loss:0.21156053804994596\n",
      "train loss:0.1857884972920762\n",
      "train loss:0.1480046135995621\n",
      "train loss:0.15210550288143687\n",
      "train loss:0.17836380627970302\n",
      "train loss:0.18797221557659902\n",
      "train loss:0.11323334902236722\n",
      "train loss:0.13331835020178198\n",
      "train loss:0.334412556009722\n",
      "train loss:0.2072734498602059\n",
      "train loss:0.2010845119887615\n",
      "train loss:0.12094036880984975\n",
      "train loss:0.2360138910525163\n",
      "train loss:0.08529568849959908\n",
      "train loss:0.07009950497535797\n",
      "train loss:0.10058680877233811\n",
      "train loss:0.11963318620789394\n",
      "train loss:0.15820529334372319\n",
      "train loss:0.15681403074540012\n",
      "train loss:0.3563549185159497\n",
      "train loss:0.42440767056020046\n",
      "train loss:0.09952025516817121\n",
      "train loss:0.20187258014853082\n",
      "train loss:0.12169320222478307\n",
      "train loss:0.17487201340716635\n",
      "train loss:0.13842518809774612\n",
      "train loss:0.12155141837673225\n",
      "train loss:0.1886040377524505\n",
      "train loss:0.12208865129828349\n",
      "train loss:0.17441112770853834\n",
      "train loss:0.13998375390006154\n",
      "train loss:0.2115107191428084\n",
      "train loss:0.13596060153985548\n",
      "train loss:0.125232749667755\n",
      "train loss:0.2435694744796508\n",
      "train loss:0.04338119720846939\n",
      "train loss:0.09802104189471075\n",
      "train loss:0.11660084272971623\n",
      "train loss:0.13586707748925062\n",
      "train loss:0.09603616812610792\n",
      "train loss:0.16919347058178213\n",
      "train loss:0.2103587124003754\n",
      "train loss:0.10191073271483071\n",
      "train loss:0.19996308058359513\n",
      "train loss:0.07556324137085646\n",
      "train loss:0.07826246287088427\n",
      "train loss:0.2329059147738683\n",
      "train loss:0.19683852182614112\n",
      "train loss:0.06399187458687398\n",
      "train loss:0.10008965377648994\n",
      "train loss:0.10724308721214972\n",
      "train loss:0.12221257362878152\n",
      "train loss:0.12790402057918374\n",
      "train loss:0.14116663714033803\n",
      "train loss:0.15383216375282457\n",
      "train loss:0.09622313202420964\n",
      "train loss:0.318480107188609\n",
      "train loss:0.09009394728646981\n",
      "train loss:0.08759677485503449\n",
      "train loss:0.14632772816391937\n",
      "train loss:0.11797955812051723\n",
      "train loss:0.16976589706916864\n",
      "train loss:0.20553255624157923\n",
      "train loss:0.1532597831510972\n",
      "train loss:0.17198140721638397\n",
      "train loss:0.06320566591033208\n",
      "train loss:0.10230384623646774\n",
      "train loss:0.15272331576496437\n",
      "train loss:0.21693505111102826\n",
      "train loss:0.13413184830527822\n",
      "train loss:0.13105012997039703\n",
      "train loss:0.2519253416920919\n",
      "train loss:0.09377205558707626\n",
      "train loss:0.14639797668033677\n",
      "train loss:0.10651278884218472\n",
      "train loss:0.13408780567579495\n",
      "train loss:0.10584431509349625\n",
      "train loss:0.12141065770855919\n",
      "train loss:0.10223333146457986\n",
      "train loss:0.05838928750977344\n",
      "train loss:0.091192023301971\n",
      "train loss:0.11154235221560863\n",
      "train loss:0.2792573718815244\n",
      "train loss:0.08614944744389046\n",
      "train loss:0.11431436842445673\n",
      "train loss:0.07343605338782797\n",
      "train loss:0.19996546006299407\n",
      "train loss:0.12960328292608236\n",
      "train loss:0.1527360822322548\n",
      "train loss:0.11169116978764672\n",
      "train loss:0.11613130119721023\n",
      "train loss:0.1610249864420492\n",
      "train loss:0.11307114645407504\n",
      "train loss:0.058243462423267604\n",
      "train loss:0.09893036316968262\n",
      "train loss:0.12576525881841105\n",
      "train loss:0.08296532588333579\n",
      "train loss:0.14812329516043948\n",
      "train loss:0.12469779888939962\n",
      "train loss:0.10438875483430038\n",
      "train loss:0.2959109929517628\n",
      "train loss:0.23455236578042904\n",
      "train loss:0.09709605649157588\n",
      "train loss:0.24195812777434822\n",
      "train loss:0.1592428279629055\n",
      "train loss:0.1929919932026879\n",
      "train loss:0.209886073625162\n",
      "train loss:0.0900643046377804\n",
      "train loss:0.2552188559795413\n",
      "train loss:0.15592235928654113\n",
      "train loss:0.08733878226782597\n",
      "train loss:0.08227302627297618\n",
      "train loss:0.10543815425102863\n",
      "train loss:0.08449632194819984\n",
      "train loss:0.15911694102329973\n",
      "train loss:0.26900143936538784\n",
      "train loss:0.06137908514858374\n",
      "train loss:0.08954091499636235\n",
      "train loss:0.164504397386116\n",
      "train loss:0.09861265070478463\n",
      "train loss:0.05286979559176741\n",
      "train loss:0.11727978081545717\n",
      "train loss:0.2085230338105301\n",
      "train loss:0.07372465880927465\n",
      "train loss:0.21526307336111652\n",
      "train loss:0.04262582288146164\n",
      "train loss:0.07523941113047868\n",
      "train loss:0.1733357626152268\n",
      "train loss:0.12566048382212827\n",
      "train loss:0.16160037068364164\n",
      "train loss:0.10392062317637549\n",
      "train loss:0.11276440368347149\n",
      "train loss:0.05799229284582788\n",
      "train loss:0.08385275578968614\n",
      "train loss:0.11099511659053359\n",
      "train loss:0.07150491871462981\n",
      "train loss:0.23907428419792773\n",
      "train loss:0.06374633417305645\n",
      "train loss:0.06977140419495795\n",
      "train loss:0.16294363791014624\n",
      "train loss:0.08533459512835391\n",
      "train loss:0.09069490372071146\n",
      "train loss:0.07103902414978223\n",
      "train loss:0.11475576336664836\n",
      "train loss:0.13185797931287713\n",
      "train loss:0.14971325973082306\n",
      "train loss:0.06462236313596437\n",
      "train loss:0.11878450355228627\n",
      "train loss:0.11038545079211551\n",
      "train loss:0.10204472026054281\n",
      "train loss:0.18528374809814252\n",
      "train loss:0.08787540126738237\n",
      "train loss:0.10236122131845903\n",
      "train loss:0.1876216197360648\n",
      "train loss:0.10164516543794301\n",
      "train loss:0.056795007921322034\n",
      "train loss:0.11189171855356977\n",
      "train loss:0.10542719379219509\n",
      "train loss:0.06161073140636926\n",
      "train loss:0.12243597906555673\n",
      "train loss:0.11277691997754337\n",
      "train loss:0.14190453967945832\n",
      "train loss:0.10040749138027463\n",
      "train loss:0.23829109064727574\n",
      "train loss:0.1447659809569253\n",
      "train loss:0.19141404890553917\n",
      "train loss:0.09421304926366447\n",
      "train loss:0.08938658546803036\n",
      "train loss:0.16909590741680766\n",
      "train loss:0.06606737629707488\n",
      "train loss:0.11002607740750425\n",
      "train loss:0.09003738798153364\n",
      "train loss:0.04213189983918484\n",
      "train loss:0.17687333637085156\n",
      "train loss:0.07845581961488268\n",
      "train loss:0.12262742437378459\n",
      "train loss:0.10366067559858261\n",
      "train loss:0.10660725099802193\n",
      "train loss:0.14297694917397305\n",
      "train loss:0.06197340056327061\n",
      "train loss:0.09928181159658835\n",
      "train loss:0.0886355230081054\n",
      "train loss:0.23982435287593507\n",
      "train loss:0.10987288950897725\n",
      "train loss:0.10160706861407293\n",
      "train loss:0.07097473859052189\n",
      "train loss:0.2711798227627531\n",
      "train loss:0.0787003190099982\n",
      "train loss:0.0921351783966219\n",
      "train loss:0.11077143607000306\n",
      "train loss:0.08247473060895867\n",
      "train loss:0.08837594415101206\n",
      "train loss:0.15720823485673882\n",
      "train loss:0.08677323407140493\n",
      "train loss:0.10898563550293616\n",
      "train loss:0.16020159354412544\n",
      "current iter num:  600\n",
      "=== epoch:2, train acc:0.968, test acc:0.964 ===\n",
      "train loss:0.08047372389243412\n",
      "train loss:0.10487301380247396\n",
      "train loss:0.11793968887021652\n",
      "train loss:0.1553444401293658\n",
      "train loss:0.21019662909732595\n",
      "train loss:0.1770110522769734\n",
      "train loss:0.11282364011767615\n",
      "train loss:0.11784130314884679\n",
      "train loss:0.07876116362859036\n",
      "train loss:0.11383955569954879\n",
      "train loss:0.13826688965394987\n",
      "train loss:0.1305696993997108\n",
      "train loss:0.10085792818526251\n",
      "train loss:0.07516436016318379\n",
      "train loss:0.04131811681009429\n",
      "train loss:0.09684718794637696\n",
      "train loss:0.09377413561184154\n",
      "train loss:0.1305904940629169\n",
      "train loss:0.14047310468768306\n",
      "train loss:0.15050863075582252\n",
      "train loss:0.13594307166248354\n",
      "train loss:0.048467227543811574\n",
      "train loss:0.11055499367315656\n",
      "train loss:0.1156484209840099\n",
      "train loss:0.050552612022249865\n",
      "train loss:0.15795064269370496\n",
      "train loss:0.09240101455278976\n",
      "train loss:0.09926018578999533\n",
      "train loss:0.0986187647864436\n",
      "train loss:0.13843337253655502\n",
      "train loss:0.100887884537484\n",
      "train loss:0.0792907939387163\n",
      "train loss:0.11284976696740873\n",
      "train loss:0.04865576093935985\n",
      "train loss:0.10038912962654015\n",
      "train loss:0.10143164635752065\n",
      "train loss:0.046793534257767215\n",
      "train loss:0.11451282772589698\n",
      "train loss:0.2674958803185678\n",
      "train loss:0.11786695874967912\n",
      "train loss:0.169715510349434\n",
      "train loss:0.1339821161980203\n",
      "train loss:0.1588796455193275\n",
      "train loss:0.11831052954367026\n",
      "train loss:0.15065687929399108\n",
      "train loss:0.07894635744366668\n",
      "train loss:0.04206377644726042\n",
      "train loss:0.13539877550332907\n",
      "train loss:0.09826696226899649\n",
      "train loss:0.20371024422641143\n",
      "train loss:0.06453046953331414\n",
      "train loss:0.14023613326840784\n",
      "train loss:0.1614270599482097\n",
      "train loss:0.06977304383467063\n",
      "train loss:0.04766327832246062\n",
      "train loss:0.0959980922974637\n",
      "train loss:0.1639899674073499\n",
      "train loss:0.11527514833815733\n",
      "train loss:0.16666771193765084\n",
      "train loss:0.09352904183881998\n",
      "train loss:0.10921625938170919\n",
      "train loss:0.068759283414359\n",
      "train loss:0.25636760347484383\n",
      "train loss:0.0943424735903776\n",
      "train loss:0.09682512454495525\n",
      "train loss:0.12124066882277099\n",
      "train loss:0.15661134751274075\n",
      "train loss:0.10325676628879854\n",
      "train loss:0.10246771921758939\n",
      "train loss:0.1003643121034094\n",
      "train loss:0.1269384105225125\n",
      "train loss:0.0848034182533704\n",
      "train loss:0.07997936049742696\n",
      "train loss:0.04679247094627423\n",
      "train loss:0.15359163957013355\n",
      "train loss:0.09948381385341251\n",
      "train loss:0.07513088876053985\n",
      "train loss:0.11732793564616148\n",
      "train loss:0.16874118068937224\n",
      "train loss:0.19213886048253548\n",
      "train loss:0.11567778432917467\n",
      "train loss:0.0729745255187811\n",
      "train loss:0.11574062463885058\n",
      "train loss:0.10326417057744361\n",
      "train loss:0.12349796692464465\n",
      "train loss:0.06676152592972077\n",
      "train loss:0.20018662188947\n",
      "train loss:0.07815758858146135\n",
      "train loss:0.19297427656778315\n",
      "train loss:0.1110562790194219\n",
      "train loss:0.052112095672861375\n",
      "train loss:0.09109997012703606\n",
      "train loss:0.10215842542238253\n",
      "train loss:0.11965380515579628\n",
      "train loss:0.08296585039957625\n",
      "train loss:0.136876889024234\n",
      "train loss:0.16050124085190787\n",
      "train loss:0.10375178480424312\n",
      "train loss:0.11066548089123586\n",
      "train loss:0.0706737936471939\n",
      "train loss:0.032060344075305724\n",
      "train loss:0.09163140790408265\n",
      "train loss:0.11433594559437371\n",
      "train loss:0.13358321110595944\n",
      "train loss:0.05080941860978706\n",
      "train loss:0.18582230766310115\n",
      "train loss:0.08797296890233641\n",
      "train loss:0.04000117679420077\n",
      "train loss:0.10237689073009118\n",
      "train loss:0.02569456479859471\n",
      "train loss:0.03772236420119001\n",
      "train loss:0.029175437839520608\n",
      "train loss:0.07307494141461518\n",
      "train loss:0.10450833406875949\n",
      "train loss:0.14521680357299538\n",
      "train loss:0.11093537935669927\n",
      "train loss:0.0704332108217118\n",
      "train loss:0.12505263462480432\n",
      "train loss:0.11583714072508142\n",
      "train loss:0.08059471023928994\n",
      "train loss:0.05534656531752959\n",
      "train loss:0.14585996848053864\n",
      "train loss:0.0938018890638715\n",
      "train loss:0.027451589519595445\n",
      "train loss:0.03977125654457117\n",
      "train loss:0.12310235382243061\n",
      "train loss:0.1671691523429826\n",
      "train loss:0.060102574209739405\n",
      "train loss:0.061582827761141185\n",
      "train loss:0.08437128696780456\n",
      "train loss:0.08112598810370841\n",
      "train loss:0.08145965436758669\n",
      "train loss:0.1085293153444113\n",
      "train loss:0.098049601667268\n",
      "train loss:0.01673311101309083\n",
      "train loss:0.11602935697717348\n",
      "train loss:0.09525802128305326\n",
      "train loss:0.06205294376522528\n",
      "train loss:0.09742723699621139\n",
      "train loss:0.12514054124488175\n",
      "train loss:0.14423165575397656\n",
      "train loss:0.09874406570781132\n",
      "train loss:0.10036690312905547\n",
      "train loss:0.22785833229235727\n",
      "train loss:0.09853197965002983\n",
      "train loss:0.08632473871034181\n",
      "train loss:0.16559147231776886\n",
      "train loss:0.14875664475187672\n",
      "train loss:0.0401674201831393\n",
      "train loss:0.04531980925374419\n",
      "train loss:0.07242345878351011\n",
      "train loss:0.12071148243936829\n",
      "train loss:0.09587326684587015\n",
      "train loss:0.09759575030745307\n",
      "train loss:0.13372531908191346\n",
      "train loss:0.1528157281025721\n",
      "train loss:0.08803173905666178\n",
      "train loss:0.2154804815582954\n",
      "train loss:0.05037251668355961\n",
      "train loss:0.07349270955711869\n",
      "train loss:0.06855822189640354\n",
      "train loss:0.12307493337123931\n",
      "train loss:0.03408555984423342\n",
      "train loss:0.1036789309360188\n",
      "train loss:0.09193845153056986\n",
      "train loss:0.0653264541934406\n",
      "train loss:0.04865194031997767\n",
      "train loss:0.08297685133497659\n",
      "train loss:0.03571590915437611\n",
      "train loss:0.05068506991276918\n",
      "train loss:0.12663209819960794\n",
      "train loss:0.09865803815419256\n",
      "train loss:0.12116973353612805\n",
      "train loss:0.07190059132738004\n",
      "train loss:0.09040046664665756\n",
      "train loss:0.14732186713938514\n",
      "train loss:0.19664353980436264\n",
      "train loss:0.028326263989761406\n",
      "train loss:0.05299212380793383\n",
      "train loss:0.07492828327478605\n",
      "train loss:0.05848387320546542\n",
      "train loss:0.040662992341961086\n",
      "train loss:0.05875109118557016\n",
      "train loss:0.04548396214568555\n",
      "train loss:0.054332066098758826\n",
      "train loss:0.04690309887836339\n",
      "train loss:0.07634118217653406\n",
      "train loss:0.10104336286114293\n",
      "train loss:0.128617488937938\n",
      "train loss:0.08767549275917919\n",
      "train loss:0.056215448499863055\n",
      "train loss:0.047892355654675714\n",
      "train loss:0.08042039071259512\n",
      "train loss:0.04427747318018766\n",
      "train loss:0.05650168183616956\n",
      "train loss:0.0793619587494992\n",
      "train loss:0.07879528488023987\n",
      "train loss:0.10926345385623401\n",
      "train loss:0.04709753990028258\n",
      "train loss:0.060738942788607655\n",
      "train loss:0.039329376410486484\n",
      "train loss:0.08216026995340074\n",
      "train loss:0.1331071737651955\n",
      "train loss:0.13038558306314907\n",
      "train loss:0.17110716565709405\n",
      "train loss:0.04741238916531231\n",
      "train loss:0.12177096988031202\n",
      "train loss:0.06879626669543613\n",
      "train loss:0.0639949958044996\n",
      "train loss:0.06316770871442076\n",
      "train loss:0.1454839584283979\n",
      "train loss:0.07915722418266571\n",
      "train loss:0.0877968162069773\n",
      "train loss:0.07355534160902934\n",
      "train loss:0.04806492705193123\n",
      "train loss:0.07806649607437598\n",
      "train loss:0.07606444548851961\n",
      "train loss:0.061418747990177956\n",
      "train loss:0.06715044051641582\n",
      "train loss:0.07469419369610207\n",
      "train loss:0.05573123024455141\n",
      "train loss:0.14723224624256187\n",
      "train loss:0.05928435042048922\n",
      "train loss:0.05942754817043805\n",
      "train loss:0.10928219590745818\n",
      "train loss:0.04320534498712236\n",
      "train loss:0.050800800964266084\n",
      "train loss:0.03650913114952366\n",
      "train loss:0.07068820622337588\n",
      "train loss:0.014898432062275188\n",
      "train loss:0.127884799360251\n",
      "train loss:0.08395144511007482\n",
      "train loss:0.05097632347587971\n",
      "train loss:0.0562791241799588\n",
      "train loss:0.08298160345084211\n",
      "train loss:0.15991724829479567\n",
      "train loss:0.06935488041500058\n",
      "train loss:0.04037938529558332\n",
      "train loss:0.06789470539506187\n",
      "train loss:0.10039513206417278\n",
      "train loss:0.06971150502929538\n",
      "train loss:0.07127386961760215\n",
      "train loss:0.059727989605998434\n",
      "train loss:0.09849112994633959\n",
      "train loss:0.12465604511299666\n",
      "train loss:0.033900125971474855\n",
      "train loss:0.06352366138214477\n",
      "train loss:0.048826407514773525\n",
      "train loss:0.12588578737381115\n",
      "train loss:0.09642193631652024\n",
      "train loss:0.05130934703357655\n",
      "train loss:0.04379945041175634\n",
      "train loss:0.08151497017434839\n",
      "train loss:0.031754944198399054\n",
      "train loss:0.04292187796160785\n",
      "train loss:0.06932750444125746\n",
      "train loss:0.059678569113920325\n",
      "train loss:0.03603109763500425\n",
      "train loss:0.06779805488776794\n",
      "train loss:0.07671771778720161\n",
      "train loss:0.06863889880505612\n",
      "train loss:0.03465652369599991\n",
      "train loss:0.14994478590344074\n",
      "train loss:0.08649751134938574\n",
      "train loss:0.05278905146391251\n",
      "train loss:0.05208451300270642\n",
      "train loss:0.0998130960953156\n",
      "train loss:0.053970366706921034\n",
      "train loss:0.0858926432823393\n",
      "train loss:0.06607839323752487\n",
      "train loss:0.07594219906876759\n",
      "train loss:0.10139632133196218\n",
      "train loss:0.10541940240313466\n",
      "train loss:0.019303785779847024\n",
      "train loss:0.061272123402997695\n",
      "train loss:0.020652066303099702\n",
      "train loss:0.07121941079003143\n",
      "train loss:0.10990455498056559\n",
      "train loss:0.029843140143589485\n",
      "train loss:0.1577253440744964\n",
      "train loss:0.07552883105684843\n",
      "train loss:0.08969716582869722\n",
      "train loss:0.11859834939533996\n",
      "train loss:0.058369667744656996\n",
      "train loss:0.06961704139059029\n",
      "train loss:0.09278216819745126\n",
      "train loss:0.034708264811439224\n",
      "train loss:0.042755148838930605\n",
      "train loss:0.05034144463943942\n",
      "train loss:0.05222955518493556\n",
      "train loss:0.07489270369109709\n",
      "train loss:0.03832424028598482\n",
      "train loss:0.1090734336255751\n",
      "train loss:0.08746184944118092\n",
      "train loss:0.09597221168867931\n",
      "train loss:0.030287198859881837\n",
      "train loss:0.0778116311503648\n",
      "train loss:0.05452776825374316\n",
      "train loss:0.30528288092985423\n",
      "train loss:0.05914770922435928\n",
      "train loss:0.0468532897998359\n",
      "train loss:0.07058182824832097\n",
      "train loss:0.034636909730643727\n",
      "train loss:0.06974879913612834\n",
      "train loss:0.09722141475715121\n",
      "train loss:0.04118525705930094\n",
      "train loss:0.06594281289095762\n",
      "train loss:0.1177563912828607\n",
      "train loss:0.10622505155174565\n",
      "train loss:0.057915517863927714\n",
      "train loss:0.04467277453238802\n",
      "train loss:0.10310924528866547\n",
      "train loss:0.09932048984392047\n",
      "train loss:0.09592985569257458\n",
      "train loss:0.10790875585585555\n",
      "train loss:0.06546470222612535\n",
      "train loss:0.13551016587045972\n",
      "train loss:0.057031864063247346\n",
      "train loss:0.06059713483873222\n",
      "train loss:0.056879724220978076\n",
      "train loss:0.03984260087871636\n",
      "train loss:0.06884346761702251\n",
      "train loss:0.10170430102156619\n",
      "train loss:0.08259223733689933\n",
      "train loss:0.041720015950867495\n",
      "train loss:0.040759976198189586\n",
      "train loss:0.10074714013852196\n",
      "train loss:0.07969133451413772\n",
      "train loss:0.07592774248284959\n",
      "train loss:0.038330159305725725\n",
      "train loss:0.10052615676964427\n",
      "train loss:0.040400639320738715\n",
      "train loss:0.05297266362075967\n",
      "train loss:0.05693143727609465\n",
      "train loss:0.17832638012109917\n",
      "train loss:0.1806474859904091\n",
      "train loss:0.19004746807519002\n",
      "train loss:0.02047625185267636\n",
      "train loss:0.06841361890924426\n",
      "train loss:0.05846866864786866\n",
      "train loss:0.02475283860267812\n",
      "train loss:0.07233982936490015\n",
      "train loss:0.07133906319983567\n",
      "train loss:0.038037362510682245\n",
      "train loss:0.14641289412930292\n",
      "train loss:0.03155208429625634\n",
      "train loss:0.082837181489574\n",
      "train loss:0.051990403332649066\n",
      "train loss:0.043590502360916376\n",
      "train loss:0.04513664466621981\n",
      "train loss:0.04431931344115163\n",
      "train loss:0.13790068966772218\n",
      "train loss:0.09325476918990862\n",
      "train loss:0.0442940262172104\n",
      "train loss:0.09831549078701314\n",
      "train loss:0.06476250408144751\n",
      "train loss:0.046610984930203575\n",
      "train loss:0.02394894984957758\n",
      "train loss:0.13412450796410902\n",
      "train loss:0.1198231865861049\n",
      "train loss:0.0568869346312309\n",
      "train loss:0.09199576774528836\n",
      "train loss:0.12864768105114144\n",
      "train loss:0.035717045700950106\n",
      "train loss:0.0409872136562906\n",
      "train loss:0.06888612420682105\n",
      "train loss:0.09889960560215057\n",
      "train loss:0.026482278959269103\n",
      "train loss:0.0447390143771034\n",
      "train loss:0.09595353344448825\n",
      "train loss:0.0951697736055937\n",
      "train loss:0.03578213477067014\n",
      "train loss:0.02960664324520046\n",
      "train loss:0.06920662456764924\n",
      "train loss:0.07165329849578583\n",
      "train loss:0.02464799181032986\n",
      "train loss:0.06336065181912906\n",
      "train loss:0.038524428183531495\n",
      "train loss:0.12055394727769742\n",
      "train loss:0.03105388902374731\n",
      "train loss:0.05103358556024566\n",
      "train loss:0.06444014821695518\n",
      "train loss:0.06471020182552159\n",
      "train loss:0.10396916563124252\n",
      "train loss:0.08509777742280492\n",
      "train loss:0.019902132499360976\n",
      "train loss:0.03907480339585705\n",
      "train loss:0.11557738943602518\n",
      "train loss:0.027622048384105292\n",
      "train loss:0.023686785959349813\n",
      "train loss:0.14395570968518298\n",
      "train loss:0.13798672498833864\n",
      "train loss:0.03874437484482732\n",
      "train loss:0.04845928592838164\n",
      "train loss:0.020071217888415437\n",
      "train loss:0.09207883665678436\n",
      "train loss:0.09369368104298326\n",
      "train loss:0.05763192244216683\n",
      "train loss:0.12572152220467622\n",
      "train loss:0.094943545743836\n",
      "train loss:0.03607588550292158\n",
      "train loss:0.09533095810301857\n",
      "train loss:0.07392094093183953\n",
      "train loss:0.06263528442742938\n",
      "train loss:0.07307148141189804\n",
      "train loss:0.05113527236078668\n",
      "train loss:0.04763546787715635\n",
      "train loss:0.03269936594613745\n",
      "train loss:0.07025502340122565\n",
      "train loss:0.0408881260795365\n",
      "train loss:0.10781714234855148\n",
      "train loss:0.09117453078599994\n",
      "train loss:0.04937110521722391\n",
      "train loss:0.027551752848852074\n",
      "train loss:0.06328729133516978\n",
      "train loss:0.04899352093315144\n",
      "train loss:0.08377328256120153\n",
      "train loss:0.06666969064496579\n",
      "train loss:0.07671658405603438\n",
      "train loss:0.06623439038918427\n",
      "train loss:0.09592521111771989\n",
      "train loss:0.04150280820862046\n",
      "train loss:0.041060002585614555\n",
      "train loss:0.11674005353595013\n",
      "train loss:0.06660928230174519\n",
      "train loss:0.04222572377733418\n",
      "train loss:0.060319363110326395\n",
      "train loss:0.0833780319876312\n",
      "train loss:0.03699841149008032\n",
      "train loss:0.10641964872822454\n",
      "train loss:0.06367436233630534\n",
      "train loss:0.17138531727124878\n",
      "train loss:0.057312446380947894\n",
      "train loss:0.14213303402470234\n",
      "train loss:0.04022219818098336\n",
      "train loss:0.06256700476096616\n",
      "train loss:0.07372749912123563\n",
      "train loss:0.08926698665017778\n",
      "train loss:0.06843847804142072\n",
      "train loss:0.11431676972823547\n",
      "train loss:0.046188678860363534\n",
      "train loss:0.03570629015991249\n",
      "train loss:0.10436456046815795\n",
      "train loss:0.10979006184796647\n",
      "train loss:0.07894129539416812\n",
      "train loss:0.029678248399003158\n",
      "train loss:0.09359025042745472\n",
      "train loss:0.1709766440113365\n",
      "train loss:0.05117238711095542\n",
      "train loss:0.047496063279147294\n",
      "train loss:0.08587722575692883\n",
      "train loss:0.034222580173762146\n",
      "train loss:0.07246287150533115\n",
      "train loss:0.05789620259341351\n",
      "train loss:0.014283479868965571\n",
      "train loss:0.0644000865547612\n",
      "train loss:0.03461188525113155\n",
      "train loss:0.1293512662882963\n",
      "train loss:0.07917172341122602\n",
      "train loss:0.07264651268122\n",
      "train loss:0.21664710168548973\n",
      "train loss:0.05838761588313756\n",
      "train loss:0.07352509527894899\n",
      "train loss:0.041257590653705906\n",
      "train loss:0.055469063109896063\n",
      "train loss:0.09915379895848622\n",
      "train loss:0.058958912252977023\n",
      "train loss:0.10328529810640391\n",
      "train loss:0.09045898494364966\n",
      "train loss:0.03323802971993096\n",
      "train loss:0.056188657940048535\n",
      "train loss:0.06747690985408732\n",
      "train loss:0.0392322778373643\n",
      "train loss:0.18008596796531282\n",
      "train loss:0.072397090181566\n",
      "train loss:0.07989777677220367\n",
      "train loss:0.008678747497990023\n",
      "train loss:0.08087234178085428\n",
      "train loss:0.0418479082554393\n",
      "train loss:0.04257698947851897\n",
      "train loss:0.013504587253579694\n",
      "train loss:0.11140609526524343\n",
      "train loss:0.08664364426685184\n",
      "train loss:0.03557598486100924\n",
      "train loss:0.14216951298434644\n",
      "train loss:0.06398134764868085\n",
      "train loss:0.15478547246948868\n",
      "train loss:0.05470736743743704\n",
      "train loss:0.013352900291208331\n",
      "train loss:0.04137738206158582\n",
      "train loss:0.08412780336768719\n",
      "train loss:0.04636616070455677\n",
      "train loss:0.08760592704117641\n",
      "train loss:0.1016986680985482\n",
      "train loss:0.05242785125547017\n",
      "train loss:0.08212261219797132\n",
      "train loss:0.02093326125430241\n",
      "train loss:0.012084175671925834\n",
      "train loss:0.049529818477348456\n",
      "train loss:0.09039634325396712\n",
      "train loss:0.04824153395656736\n",
      "train loss:0.026539284241349676\n",
      "train loss:0.10746610612395718\n",
      "train loss:0.03606731647185507\n",
      "train loss:0.20624456035244848\n",
      "train loss:0.01622439312734401\n",
      "train loss:0.0838784601111452\n",
      "train loss:0.023634154097594555\n",
      "train loss:0.15364759838642386\n",
      "train loss:0.04258253633468361\n",
      "train loss:0.05767795336629801\n",
      "train loss:0.04882519473493405\n",
      "train loss:0.0615925820224389\n",
      "train loss:0.07792900727150467\n",
      "train loss:0.07289259046871614\n",
      "train loss:0.05721769632817371\n",
      "train loss:0.12635137848216865\n",
      "train loss:0.09364048514546881\n",
      "train loss:0.1245214036869602\n",
      "train loss:0.04638096361106551\n",
      "train loss:0.04092344094378145\n",
      "train loss:0.0263550174793874\n",
      "train loss:0.07718618576804634\n",
      "train loss:0.06437057330158522\n",
      "train loss:0.03244505651263091\n",
      "train loss:0.09401826623728873\n",
      "train loss:0.10262591412286687\n",
      "train loss:0.020867454785436085\n",
      "train loss:0.037050954344495325\n",
      "train loss:0.054057437001691966\n",
      "train loss:0.09467373612688172\n",
      "train loss:0.028850981589313443\n",
      "train loss:0.0701428386876876\n",
      "train loss:0.03220191254650273\n",
      "train loss:0.06815071379150131\n",
      "train loss:0.07243551766574813\n",
      "train loss:0.017647096156836357\n",
      "train loss:0.06526985326477225\n",
      "train loss:0.05461659399105243\n",
      "train loss:0.11645449832235957\n",
      "train loss:0.018041869494285597\n",
      "train loss:0.09591843994740958\n",
      "train loss:0.07470984208549478\n",
      "train loss:0.10971215218182412\n",
      "train loss:0.020177797946972432\n",
      "train loss:0.032691957431480914\n",
      "train loss:0.04543693949361316\n",
      "train loss:0.05222134262325629\n",
      "train loss:0.023814820945560783\n",
      "train loss:0.05549252283102501\n",
      "train loss:0.011719195375548249\n",
      "train loss:0.024407622264697393\n",
      "train loss:0.12215263780969489\n",
      "train loss:0.0561498833377021\n",
      "train loss:0.03173721632908452\n",
      "train loss:0.015998015719187308\n",
      "train loss:0.03387207364474484\n",
      "train loss:0.08400679005874474\n",
      "train loss:0.07021849086905918\n",
      "train loss:0.08667578180975904\n",
      "train loss:0.021075270510017965\n",
      "train loss:0.06723369403806291\n",
      "train loss:0.06737809961489082\n",
      "train loss:0.058867794712483865\n",
      "train loss:0.09841423662441237\n",
      "train loss:0.04751492256607281\n",
      "train loss:0.031095829183070734\n",
      "train loss:0.05882129191049084\n",
      "train loss:0.038389322554061266\n",
      "train loss:0.04928152916882823\n",
      "train loss:0.032002431814656104\n",
      "train loss:0.030255720760901873\n",
      "train loss:0.17482556457963538\n",
      "train loss:0.02992878556688166\n",
      "train loss:0.026706949397476358\n",
      "train loss:0.07815417107106133\n",
      "train loss:0.041243585799617816\n",
      "train loss:0.036784486398913185\n",
      "train loss:0.045238084795523735\n",
      "train loss:0.02473246743428934\n",
      "train loss:0.03678706222759447\n",
      "train loss:0.11538011742580573\n",
      "train loss:0.040212620688221776\n",
      "train loss:0.09251370022784176\n",
      "train loss:0.04362464595195328\n",
      "train loss:0.0422375123382907\n",
      "train loss:0.08807694464430064\n",
      "train loss:0.022562645514798172\n",
      "train loss:0.015699991029071315\n",
      "train loss:0.03530092801722757\n",
      "train loss:0.07156994548611735\n",
      "train loss:0.0438365877456409\n",
      "train loss:0.015660788491242514\n",
      "train loss:0.05905352227166904\n",
      "train loss:0.088398180288975\n",
      "train loss:0.031216386478503114\n",
      "train loss:0.03879629395364568\n",
      "train loss:0.02271321009166803\n",
      "train loss:0.0457600664021556\n",
      "train loss:0.08691654388790575\n",
      "current iter num:  1200\n",
      "=== epoch:3, train acc:0.976, test acc:0.973 ===\n",
      "train loss:0.05461950444363293\n",
      "train loss:0.05641273237616176\n",
      "train loss:0.09275538316289804\n",
      "train loss:0.029005182409818406\n",
      "train loss:0.010310417734837894\n",
      "train loss:0.06474969539482055\n",
      "train loss:0.08192481290264615\n",
      "train loss:0.06800620837748991\n",
      "train loss:0.04082945947561725\n",
      "train loss:0.10558783083442078\n",
      "train loss:0.0328547598753095\n",
      "train loss:0.08124354664518997\n",
      "train loss:0.07131272239474548\n",
      "train loss:0.16338908899997726\n",
      "train loss:0.019579193135319375\n",
      "train loss:0.13929354798682375\n",
      "train loss:0.03552482337770239\n",
      "train loss:0.024933711566427732\n",
      "train loss:0.11756205003498643\n",
      "train loss:0.13148970354937095\n",
      "train loss:0.04840859488456682\n",
      "train loss:0.02132569472081031\n",
      "train loss:0.04909500100788864\n",
      "train loss:0.13757818231810387\n",
      "train loss:0.0430505566470878\n",
      "train loss:0.0399910539645339\n",
      "train loss:0.05497018310327023\n",
      "train loss:0.10952084175111727\n",
      "train loss:0.06773500108924083\n",
      "train loss:0.057077567832363674\n",
      "train loss:0.02996493222662817\n",
      "train loss:0.11005276247403226\n",
      "train loss:0.05490384489734615\n",
      "train loss:0.04689386591821236\n",
      "train loss:0.019769937303393895\n",
      "train loss:0.0492293916182311\n",
      "train loss:0.07723251446141617\n",
      "train loss:0.05224763381300564\n",
      "train loss:0.044299987912520856\n",
      "train loss:0.05739643239203378\n",
      "train loss:0.024442671561250413\n",
      "train loss:0.022277070439770123\n",
      "train loss:0.03052000952664466\n",
      "train loss:0.1065620524724557\n",
      "train loss:0.06121997097589654\n",
      "train loss:0.05213188050445474\n",
      "train loss:0.0362806449206501\n",
      "train loss:0.09309240071353603\n",
      "train loss:0.045903148135884295\n",
      "train loss:0.035148340145347456\n",
      "train loss:0.10431189528865112\n",
      "train loss:0.09112066090662017\n",
      "train loss:0.0586097683990832\n",
      "train loss:0.05746277099390385\n",
      "train loss:0.049561168922077375\n",
      "train loss:0.024526954937645402\n",
      "train loss:0.022532002704135932\n",
      "train loss:0.03411051352393903\n",
      "train loss:0.022606649620173357\n",
      "train loss:0.07690280373765823\n",
      "train loss:0.037405849522917894\n",
      "train loss:0.02069830071656623\n",
      "train loss:0.0443643538200209\n",
      "train loss:0.05006327675500092\n",
      "train loss:0.07675728553751669\n",
      "train loss:0.055927524042468375\n",
      "train loss:0.06835174050089829\n",
      "train loss:0.053001637699429095\n",
      "train loss:0.05400265074714227\n",
      "train loss:0.0900651733462928\n",
      "train loss:0.05663240990700383\n",
      "train loss:0.11246134360799682\n",
      "train loss:0.0974623362842786\n",
      "train loss:0.06967966343876801\n",
      "train loss:0.09871090078989436\n",
      "train loss:0.04775572906431573\n",
      "train loss:0.05307133141505209\n",
      "train loss:0.05130384826786684\n",
      "train loss:0.10606461400824743\n",
      "train loss:0.05681427516586791\n",
      "train loss:0.042056075787347956\n",
      "train loss:0.04759019162738602\n",
      "train loss:0.062430480466151614\n",
      "train loss:0.06233838704869159\n",
      "train loss:0.05097166401710939\n",
      "train loss:0.1068365549385898\n",
      "train loss:0.03672020539165047\n",
      "train loss:0.036192543536894786\n",
      "train loss:0.06356251405789204\n",
      "train loss:0.04798839950081501\n",
      "train loss:0.22377015098065953\n",
      "train loss:0.022374413957171622\n",
      "train loss:0.08378572157372276\n",
      "train loss:0.048252823931726904\n",
      "train loss:0.12916362707364726\n",
      "train loss:0.06466174863265653\n",
      "train loss:0.09032181828136224\n",
      "train loss:0.1509111432865011\n",
      "train loss:0.04962621542064629\n",
      "train loss:0.11510296057414764\n",
      "train loss:0.04682837294486683\n",
      "train loss:0.04544463491030351\n",
      "train loss:0.047407155192062\n",
      "train loss:0.025237406484025268\n",
      "train loss:0.04255443201214316\n",
      "train loss:0.07295941383281992\n",
      "train loss:0.028497138488492245\n",
      "train loss:0.02107928731900556\n",
      "train loss:0.06593097975343967\n",
      "train loss:0.023234816883104782\n",
      "train loss:0.05909334508507925\n",
      "train loss:0.03411233713730245\n",
      "train loss:0.023978163610898072\n",
      "train loss:0.05933781604115317\n",
      "train loss:0.025212362832287342\n",
      "train loss:0.0435006192203639\n",
      "train loss:0.0359376286593805\n",
      "train loss:0.11195102488079188\n",
      "train loss:0.06107709868907915\n",
      "train loss:0.10516230542853629\n",
      "train loss:0.032521752824736655\n",
      "train loss:0.0653308917795339\n",
      "train loss:0.05298774939919296\n",
      "train loss:0.07363686771327663\n",
      "train loss:0.05720316646340209\n",
      "train loss:0.04036225888486989\n",
      "train loss:0.09850690423363545\n",
      "train loss:0.025238735400168234\n",
      "train loss:0.10079018470601019\n",
      "train loss:0.021405515893581386\n",
      "train loss:0.009552746285601934\n",
      "train loss:0.02934577219189051\n",
      "train loss:0.09613214320175906\n",
      "train loss:0.048163957641851746\n",
      "train loss:0.07131443795103794\n",
      "train loss:0.047264824074766884\n",
      "train loss:0.07064430511013882\n",
      "train loss:0.04586002685088488\n",
      "train loss:0.05249526849217868\n",
      "train loss:0.047972200585743446\n",
      "train loss:0.031669533915714705\n",
      "train loss:0.08061652176244996\n",
      "train loss:0.12126807268366738\n",
      "train loss:0.033254788432697205\n",
      "train loss:0.033680823345691197\n",
      "train loss:0.06447571738279016\n",
      "train loss:0.047540031226138035\n",
      "train loss:0.02349449222843869\n",
      "train loss:0.05549479949166531\n",
      "train loss:0.026667773292504107\n",
      "train loss:0.06588892003780028\n",
      "train loss:0.08692434165470629\n",
      "train loss:0.09605232272583963\n",
      "train loss:0.05225677956734618\n",
      "train loss:0.016055767790182533\n",
      "train loss:0.13511828715133004\n",
      "train loss:0.04669704841234708\n",
      "train loss:0.018602864619052244\n",
      "train loss:0.057161022384069386\n",
      "train loss:0.029047778889016867\n",
      "train loss:0.06085254688647668\n",
      "train loss:0.009865795139865302\n",
      "train loss:0.03117092909666868\n",
      "train loss:0.023464807143601208\n",
      "train loss:0.08228470504408686\n",
      "train loss:0.055783521015185265\n",
      "train loss:0.03984887757200842\n",
      "train loss:0.05393254707214346\n",
      "train loss:0.049704623919825545\n",
      "train loss:0.025711568404170267\n",
      "train loss:0.020747605742829824\n",
      "train loss:0.025915959450631788\n",
      "train loss:0.043439179795860774\n",
      "train loss:0.026331068628775826\n",
      "train loss:0.023555198152302427\n",
      "train loss:0.027820637326791974\n",
      "train loss:0.08351361272643346\n",
      "train loss:0.07761278758770632\n",
      "train loss:0.007605290369403002\n",
      "train loss:0.05530355094774506\n",
      "train loss:0.08267156730191305\n",
      "train loss:0.029211385860033908\n",
      "train loss:0.03348483399347552\n",
      "train loss:0.053397168977502775\n",
      "train loss:0.02201401506454439\n",
      "train loss:0.025551404810663217\n",
      "train loss:0.03931323528088338\n",
      "train loss:0.011736689124152742\n",
      "train loss:0.020337856105781423\n",
      "train loss:0.09659668242725243\n",
      "train loss:0.024509711789558075\n",
      "train loss:0.14561395557389803\n",
      "train loss:0.020259210514665373\n",
      "train loss:0.10921200443893875\n",
      "train loss:0.017767295615432267\n",
      "train loss:0.015142783749075225\n",
      "train loss:0.08072345038992257\n",
      "train loss:0.032626298691222126\n",
      "train loss:0.06954096019445985\n",
      "train loss:0.09345567685874168\n",
      "train loss:0.07134303478712653\n",
      "train loss:0.03119734373244115\n",
      "train loss:0.06783472261736744\n",
      "train loss:0.05536112449096121\n",
      "train loss:0.034415962001044416\n",
      "train loss:0.06298355190670912\n",
      "train loss:0.07761204229561541\n",
      "train loss:0.05687000900296898\n",
      "train loss:0.016613036148905264\n",
      "train loss:0.06117311364750822\n",
      "train loss:0.0868365040651479\n",
      "train loss:0.03457556620659927\n",
      "train loss:0.09129657018863942\n",
      "train loss:0.022968903773545383\n",
      "train loss:0.018671557589261752\n",
      "train loss:0.02063580419193899\n",
      "train loss:0.11206086064983783\n",
      "train loss:0.014030200633691042\n",
      "train loss:0.051061764582502764\n",
      "train loss:0.03009020051093944\n",
      "train loss:0.04911028954075688\n",
      "train loss:0.049989523115264445\n",
      "train loss:0.11726691556993389\n",
      "train loss:0.032386986616811136\n",
      "train loss:0.03381906310572671\n",
      "train loss:0.04274945109645008\n",
      "train loss:0.022462987885782423\n",
      "train loss:0.07149464155400605\n",
      "train loss:0.02278549848837175\n",
      "train loss:0.010744231032314078\n",
      "train loss:0.10130243697427903\n",
      "train loss:0.022036448857977124\n",
      "train loss:0.02883799006194907\n",
      "train loss:0.021059267432986493\n",
      "train loss:0.030240796031408886\n",
      "train loss:0.07271913294925536\n",
      "train loss:0.018120574049220236\n",
      "train loss:0.043805160297478635\n",
      "train loss:0.027892121405242484\n",
      "train loss:0.009430439155964001\n",
      "train loss:0.03619178933370987\n",
      "train loss:0.01830777726308928\n",
      "train loss:0.025716913979154352\n",
      "train loss:0.005367312924078453\n",
      "train loss:0.02390638097786976\n",
      "train loss:0.043826021453025395\n",
      "train loss:0.08400571191864499\n",
      "train loss:0.04230670593693105\n",
      "train loss:0.06001108802307642\n",
      "train loss:0.07874172565981058\n",
      "train loss:0.06643272226614388\n",
      "train loss:0.039049601385999756\n",
      "train loss:0.08480490909662612\n",
      "train loss:0.10441075147027509\n",
      "train loss:0.014515310143616844\n",
      "train loss:0.006247748167836598\n",
      "train loss:0.06473225169898009\n",
      "train loss:0.061150925073632\n",
      "train loss:0.06438692089448221\n",
      "train loss:0.06334131488188255\n",
      "train loss:0.10479498869004802\n",
      "train loss:0.020898124950547565\n",
      "train loss:0.038210857699078324\n",
      "train loss:0.11589960512334602\n",
      "train loss:0.04086334948063138\n",
      "train loss:0.11640622372537925\n",
      "train loss:0.03952512462161708\n",
      "train loss:0.08767562899920359\n",
      "train loss:0.004630430859012616\n",
      "train loss:0.03552863110790608\n",
      "train loss:0.03180546122410262\n",
      "train loss:0.029143767781633734\n",
      "train loss:0.010579298749504657\n",
      "train loss:0.07201741174294755\n",
      "train loss:0.10834997992179778\n",
      "train loss:0.04307224441238477\n",
      "train loss:0.04141274426999123\n",
      "train loss:0.0424406304929084\n",
      "train loss:0.0403563409803941\n",
      "train loss:0.081673741637027\n",
      "train loss:0.042571126440611395\n",
      "train loss:0.06584055197851452\n",
      "train loss:0.018571635298802135\n",
      "train loss:0.07275389043665259\n",
      "train loss:0.03149737397505426\n",
      "train loss:0.032187970593871636\n",
      "train loss:0.01799474724229893\n",
      "train loss:0.0713815600356832\n",
      "train loss:0.03692571164013887\n",
      "train loss:0.03052325678198634\n",
      "train loss:0.02911872781843919\n",
      "train loss:0.020329747403591624\n",
      "train loss:0.011205108094565668\n",
      "train loss:0.06763624844453427\n",
      "train loss:0.040763609159699676\n",
      "train loss:0.020496752766243267\n",
      "train loss:0.04478443182295484\n",
      "train loss:0.04290928809195041\n",
      "train loss:0.06561202540684442\n",
      "train loss:0.036183155622809625\n",
      "train loss:0.021260341391570514\n",
      "train loss:0.020220440071206346\n",
      "train loss:0.037352685005915466\n",
      "train loss:0.0357456071640066\n",
      "train loss:0.06943240403363142\n",
      "train loss:0.026796758128522\n",
      "train loss:0.05049691599000977\n",
      "train loss:0.029813550749215968\n",
      "train loss:0.025721059064475808\n",
      "train loss:0.16720171950390386\n",
      "train loss:0.05674490299756223\n",
      "train loss:0.04414096297276105\n",
      "train loss:0.045360154853137644\n",
      "train loss:0.05628486780593067\n",
      "train loss:0.08329609093413926\n",
      "train loss:0.034544589810006736\n",
      "train loss:0.04747692288879717\n",
      "train loss:0.044476367061459736\n",
      "train loss:0.037384629371037974\n",
      "train loss:0.0803902016590274\n",
      "train loss:0.039763998694961694\n",
      "train loss:0.05332916061489246\n",
      "train loss:0.020397301061650414\n",
      "train loss:0.10141075404891463\n",
      "train loss:0.06339113880074093\n",
      "train loss:0.0738357489654781\n",
      "train loss:0.02992572209231313\n",
      "train loss:0.02008843093617897\n",
      "train loss:0.037775929173183294\n",
      "train loss:0.025244671019581756\n",
      "train loss:0.07598190885993254\n",
      "train loss:0.11227255564701982\n",
      "train loss:0.06268998606513979\n",
      "train loss:0.09622648544677194\n",
      "train loss:0.13107633530815962\n",
      "train loss:0.03720250719343821\n",
      "train loss:0.0383632583185924\n",
      "train loss:0.04328413200889585\n",
      "train loss:0.05492054862174476\n",
      "train loss:0.009379353467499303\n",
      "train loss:0.1038590462064535\n",
      "train loss:0.06055293556078926\n",
      "train loss:0.013223372359189071\n",
      "train loss:0.05658955724417413\n",
      "train loss:0.014015016893646861\n",
      "train loss:0.01384649376919925\n",
      "train loss:0.01988442806452985\n",
      "train loss:0.02365157554934338\n",
      "train loss:0.03988656891137815\n",
      "train loss:0.07438694716253451\n",
      "train loss:0.12060519988044498\n",
      "train loss:0.01715897664286097\n",
      "train loss:0.18578099921258218\n",
      "train loss:0.045676850676584044\n",
      "train loss:0.03788065118557899\n",
      "train loss:0.014162980816250286\n",
      "train loss:0.052243376406221315\n",
      "train loss:0.03656274666158182\n",
      "train loss:0.037502119252206235\n",
      "train loss:0.016053794093878245\n",
      "train loss:0.031640928039525326\n",
      "train loss:0.07255937808751477\n",
      "train loss:0.04766636780433057\n",
      "train loss:0.045470595811747117\n",
      "train loss:0.029170169043848698\n",
      "train loss:0.04147342254366913\n",
      "train loss:0.0075157281672166\n",
      "train loss:0.0560734273711086\n",
      "train loss:0.03454929039272393\n",
      "train loss:0.04204568915580105\n",
      "train loss:0.04919492322419084\n",
      "train loss:0.04525107871740738\n",
      "train loss:0.023203977943299873\n",
      "train loss:0.04984809560937895\n",
      "train loss:0.11966318923469015\n",
      "train loss:0.044841296530450686\n",
      "train loss:0.033600405408874164\n",
      "train loss:0.06192758547765978\n",
      "train loss:0.03066737004821921\n",
      "train loss:0.05451882709513902\n",
      "train loss:0.01320982143536531\n",
      "train loss:0.027730056931949017\n",
      "train loss:0.020080344706527215\n",
      "train loss:0.05674464449134608\n",
      "train loss:0.030113644630371087\n",
      "train loss:0.013439883339250771\n",
      "train loss:0.056273371952596615\n",
      "train loss:0.03487593805563276\n",
      "train loss:0.02945581562890078\n",
      "train loss:0.02730071712227715\n",
      "train loss:0.056116739401202126\n",
      "train loss:0.10636678750075086\n",
      "train loss:0.021159217674133056\n",
      "train loss:0.07516052739572392\n",
      "train loss:0.014481323100493009\n",
      "train loss:0.07322966995327246\n",
      "train loss:0.04926705750549791\n",
      "train loss:0.1020980520227295\n",
      "train loss:0.07061622235195492\n",
      "train loss:0.05288847166576652\n",
      "train loss:0.04800279492446054\n",
      "train loss:0.05192798726955781\n",
      "train loss:0.07339706707767844\n",
      "train loss:0.04164145149238386\n",
      "train loss:0.04961027987342168\n",
      "train loss:0.051027796500763584\n",
      "train loss:0.03235629754023684\n",
      "train loss:0.04398053708440331\n",
      "train loss:0.027038149262238843\n",
      "train loss:0.019846722254994204\n",
      "train loss:0.1335908281077196\n",
      "train loss:0.04258796062354547\n",
      "train loss:0.023566085218247993\n",
      "train loss:0.03096479431317228\n",
      "train loss:0.08532469989540015\n",
      "train loss:0.03297147537804047\n",
      "train loss:0.019854491133782955\n",
      "train loss:0.022466751045334486\n",
      "train loss:0.014423351124607584\n",
      "train loss:0.09871489089917493\n",
      "train loss:0.02260246932901909\n",
      "train loss:0.009538099712389721\n",
      "train loss:0.03706569503157116\n",
      "train loss:0.036820852588327894\n",
      "train loss:0.03503683480978715\n",
      "train loss:0.012951392014900953\n",
      "train loss:0.019194034012527894\n",
      "train loss:0.039164939805077854\n",
      "train loss:0.01320006535292662\n",
      "train loss:0.02068269148919817\n",
      "train loss:0.07679688518294368\n",
      "train loss:0.015699639297834828\n",
      "train loss:0.09683919312867262\n",
      "train loss:0.020521601737939293\n",
      "train loss:0.04261403812951823\n",
      "train loss:0.050947691530559745\n",
      "train loss:0.025334285299672857\n",
      "train loss:0.05411597503428953\n",
      "train loss:0.058698760459446914\n",
      "train loss:0.011395032835381975\n",
      "train loss:0.029262088663641703\n",
      "train loss:0.006892806760934279\n",
      "train loss:0.026666935802945144\n",
      "train loss:0.021876220302516552\n",
      "train loss:0.01602800563318888\n",
      "train loss:0.08460243420654295\n",
      "train loss:0.018480603686315263\n",
      "train loss:0.06334743469833042\n",
      "train loss:0.11268425903065993\n",
      "train loss:0.017392761483651552\n",
      "train loss:0.05206077262948936\n",
      "train loss:0.07411453790733752\n",
      "train loss:0.03497495929572204\n",
      "train loss:0.043539390754437586\n",
      "train loss:0.0331416097925514\n",
      "train loss:0.03439633560386926\n",
      "train loss:0.03360456153622391\n",
      "train loss:0.014763904725534538\n",
      "train loss:0.05409135390602892\n",
      "train loss:0.04275525024124278\n",
      "train loss:0.08315931986265722\n",
      "train loss:0.04293903886926226\n",
      "train loss:0.0575540247098639\n",
      "train loss:0.06581473171887094\n",
      "train loss:0.022024890376931793\n",
      "train loss:0.01948040486804793\n",
      "train loss:0.021274661789295216\n",
      "train loss:0.145942114041112\n",
      "train loss:0.14304305461466915\n",
      "train loss:0.029958103102738515\n",
      "train loss:0.06574112544001448\n",
      "train loss:0.030195882280594163\n",
      "train loss:0.03575113090667384\n",
      "train loss:0.030948290680396437\n",
      "train loss:0.03324518038754424\n",
      "train loss:0.02958123491302535\n",
      "train loss:0.020065957305333636\n",
      "train loss:0.04386661697717642\n",
      "train loss:0.014435354353705838\n",
      "train loss:0.02552314998934002\n",
      "train loss:0.022599196942420553\n",
      "train loss:0.04364255788860345\n",
      "train loss:0.07413884965727481\n",
      "train loss:0.03067805320299818\n",
      "train loss:0.009459031836356064\n",
      "train loss:0.015913520078864676\n",
      "train loss:0.028674326032326593\n",
      "train loss:0.02200237615727382\n",
      "train loss:0.013826498656457152\n",
      "train loss:0.0959291236789025\n",
      "train loss:0.02270891361780796\n",
      "train loss:0.017837663153026394\n",
      "train loss:0.08125567899950414\n",
      "train loss:0.016994876450658777\n",
      "train loss:0.03058676789059859\n",
      "train loss:0.09380665816101878\n",
      "train loss:0.02670424990253228\n",
      "train loss:0.05674739116698666\n",
      "train loss:0.023493351935231496\n",
      "train loss:0.11443413558354075\n",
      "train loss:0.032807901522897864\n",
      "train loss:0.08873975058613758\n",
      "train loss:0.012511969967788545\n",
      "train loss:0.07861512260974622\n",
      "train loss:0.14586893479980356\n",
      "train loss:0.02835930423973752\n",
      "train loss:0.04247393916532329\n",
      "train loss:0.28878384973718474\n",
      "train loss:0.021428070431705665\n",
      "train loss:0.034658923292816425\n",
      "train loss:0.05782147782439282\n",
      "train loss:0.007627506026349741\n",
      "train loss:0.01738122301226827\n",
      "train loss:0.030263738945948658\n",
      "train loss:0.031752817246548576\n",
      "train loss:0.032498286277461756\n",
      "train loss:0.009698993992837851\n",
      "train loss:0.07986515772988838\n",
      "train loss:0.11561268856546071\n",
      "train loss:0.021037985234411673\n",
      "train loss:0.03772003583035494\n",
      "train loss:0.06769305960309353\n",
      "train loss:0.08116901012547265\n",
      "train loss:0.027363019882774186\n",
      "train loss:0.021412080864136554\n",
      "train loss:0.14874063533150614\n",
      "train loss:0.07165023188533222\n",
      "train loss:0.05375593451042916\n",
      "train loss:0.02638292649399774\n",
      "train loss:0.07999342457607563\n",
      "train loss:0.05760631819198667\n",
      "train loss:0.0504641405369875\n",
      "train loss:0.03265457220713068\n",
      "train loss:0.04855508501408023\n",
      "train loss:0.050900218488335695\n",
      "train loss:0.06442038124282541\n",
      "train loss:0.012170755964587908\n",
      "train loss:0.036901107558201834\n",
      "train loss:0.015393965765173318\n",
      "train loss:0.06166385421212153\n",
      "train loss:0.037802377368769235\n",
      "train loss:0.05738237617042294\n",
      "train loss:0.07901267516001938\n",
      "train loss:0.1201765349398117\n",
      "train loss:0.037004210181949555\n",
      "train loss:0.06765857520800372\n",
      "train loss:0.014963704003089235\n",
      "train loss:0.018752978568876276\n",
      "train loss:0.06495013149002915\n",
      "train loss:0.02329708427125753\n",
      "train loss:0.030379401339048703\n",
      "train loss:0.06909107562903387\n",
      "train loss:0.03585964495228418\n",
      "train loss:0.026964556025580295\n",
      "train loss:0.03835755783253303\n",
      "train loss:0.07039989926940925\n",
      "train loss:0.01573428258302761\n",
      "train loss:0.025083262701485195\n",
      "train loss:0.08294692940436578\n",
      "train loss:0.08377797893003523\n",
      "train loss:0.025509090914661345\n",
      "train loss:0.03735229784213447\n",
      "train loss:0.018596144484678313\n",
      "train loss:0.053586470697690576\n",
      "train loss:0.012063871234604688\n",
      "train loss:0.06331183445567738\n",
      "train loss:0.037333585622778616\n",
      "train loss:0.07575998798212019\n",
      "train loss:0.0968503936924244\n",
      "train loss:0.020500736636209813\n",
      "train loss:0.020340198352085222\n",
      "train loss:0.09323236002599057\n",
      "train loss:0.026487666026976546\n",
      "train loss:0.14224546311171093\n",
      "train loss:0.04953339656535588\n",
      "train loss:0.040846889620687304\n",
      "train loss:0.05256504664412859\n",
      "train loss:0.029422385174396153\n",
      "train loss:0.01997771928956704\n",
      "train loss:0.02933956676331435\n",
      "train loss:0.02664250208161183\n",
      "train loss:0.04471319286431039\n",
      "train loss:0.019936475156062443\n",
      "train loss:0.09576477249974077\n",
      "train loss:0.0363379619480286\n",
      "train loss:0.060851960247054394\n",
      "train loss:0.09349719004182201\n",
      "train loss:0.054428679465035615\n",
      "train loss:0.033429913715271355\n",
      "train loss:0.02060246004685261\n",
      "train loss:0.03774129398452929\n",
      "train loss:0.048028331351989924\n",
      "train loss:0.055861674869061934\n",
      "train loss:0.04480977419824161\n",
      "train loss:0.03087523697158287\n",
      "train loss:0.01591778062183578\n",
      "train loss:0.02512425180001231\n",
      "train loss:0.029946391043388223\n",
      "train loss:0.08712455940126769\n",
      "train loss:0.0990162276491848\n",
      "current iter num:  1800\n",
      "=== epoch:4, train acc:0.982, test acc:0.977 ===\n",
      "train loss:0.043416080082924795\n",
      "train loss:0.018168951960983188\n",
      "train loss:0.02951925901683365\n",
      "train loss:0.04836065264492561\n",
      "train loss:0.014431839372332094\n",
      "train loss:0.014617221209220134\n",
      "train loss:0.017758459006336918\n",
      "train loss:0.07954994414909129\n",
      "train loss:0.03344060279467681\n",
      "train loss:0.10795598620068078\n",
      "train loss:0.04819743561271299\n",
      "train loss:0.07513226960347767\n",
      "train loss:0.05804135660453655\n",
      "train loss:0.06947509135887069\n",
      "train loss:0.14623515385424304\n",
      "train loss:0.028825560979968183\n",
      "train loss:0.025831078980021138\n",
      "train loss:0.04444715756111587\n",
      "train loss:0.01613188024815314\n",
      "train loss:0.0375301532691832\n",
      "train loss:0.0421396824114434\n",
      "train loss:0.01867620693487597\n",
      "train loss:0.05355863243787261\n",
      "train loss:0.06932661858372159\n",
      "train loss:0.029982241560671107\n",
      "train loss:0.03703915348296061\n",
      "train loss:0.08721775248525467\n",
      "train loss:0.0151571290309662\n",
      "train loss:0.01482054345005837\n",
      "train loss:0.02619996619367063\n",
      "train loss:0.035686967131340216\n",
      "train loss:0.0688517429683694\n",
      "train loss:0.06804069951319142\n",
      "train loss:0.17453325643092296\n",
      "train loss:0.013374639405762046\n",
      "train loss:0.023217353300562387\n",
      "train loss:0.014416599263260515\n",
      "train loss:0.05496573771871627\n",
      "train loss:0.1114639251643109\n",
      "train loss:0.030587048880029468\n",
      "train loss:0.024485863270443567\n",
      "train loss:0.07079279655273034\n",
      "train loss:0.04287647006734053\n",
      "train loss:0.02890431439463227\n",
      "train loss:0.04846605162466863\n",
      "train loss:0.030453014466578582\n",
      "train loss:0.02157049776237657\n",
      "train loss:0.02416315013205008\n",
      "train loss:0.08225975977951513\n",
      "train loss:0.042908046757188636\n",
      "train loss:0.019119787462735134\n",
      "train loss:0.07139202499994675\n",
      "train loss:0.008143491909486399\n",
      "train loss:0.02867135797063074\n",
      "train loss:0.016784863985274612\n",
      "train loss:0.050569660027243425\n",
      "train loss:0.029038193910942426\n",
      "train loss:0.0511910713060067\n",
      "train loss:0.007811002747736138\n",
      "train loss:0.009309913983012192\n",
      "train loss:0.05073000384628909\n",
      "train loss:0.09124091397298224\n",
      "train loss:0.043005139739933224\n",
      "train loss:0.04443751760236877\n",
      "train loss:0.10814728399948949\n",
      "train loss:0.011651329490880465\n",
      "train loss:0.029791275995418257\n",
      "train loss:0.01236024022096136\n",
      "train loss:0.018204046761843805\n",
      "train loss:0.009675090586404714\n",
      "train loss:0.005254274845106301\n",
      "train loss:0.0026240764361520424\n",
      "train loss:0.02877081928130008\n",
      "train loss:0.11369913760089607\n",
      "train loss:0.06999961792753424\n",
      "train loss:0.01608627011977516\n",
      "train loss:0.05918290570007012\n",
      "train loss:0.02886252037617979\n",
      "train loss:0.08268152470875081\n",
      "train loss:0.06877094297251501\n",
      "train loss:0.03222441733193535\n",
      "train loss:0.017289200485780035\n",
      "train loss:0.08417255685831418\n",
      "train loss:0.0686399189302593\n",
      "train loss:0.019740328256871612\n",
      "train loss:0.04462215582443931\n",
      "train loss:0.015943615451267646\n",
      "train loss:0.03879897507591293\n",
      "train loss:0.08039217757402356\n",
      "train loss:0.08651673826943725\n",
      "train loss:0.06830876946977346\n",
      "train loss:0.02597931626519983\n",
      "train loss:0.03643936294654163\n",
      "train loss:0.0342550554334004\n",
      "train loss:0.04298212141640374\n",
      "train loss:0.009968996074811725\n",
      "train loss:0.013541314350358243\n",
      "train loss:0.011420147801862884\n",
      "train loss:0.019328150969531225\n",
      "train loss:0.03310252692478894\n",
      "train loss:0.023083497611803058\n",
      "train loss:0.07558693013739129\n",
      "train loss:0.03319318697269837\n",
      "train loss:0.011325564115362225\n",
      "train loss:0.011192781362954636\n",
      "train loss:0.07697670434026022\n",
      "train loss:0.04239383733058702\n",
      "train loss:0.042532329556440526\n",
      "train loss:0.10187774844223739\n",
      "train loss:0.03897808828447957\n",
      "train loss:0.043742574461265164\n",
      "train loss:0.020682126655313802\n",
      "train loss:0.014551988217740055\n",
      "train loss:0.060752682994395665\n",
      "train loss:0.05047620293681403\n",
      "train loss:0.03895626987254185\n",
      "train loss:0.02103097222743438\n",
      "train loss:0.042958090693728475\n",
      "train loss:0.09819421271726812\n",
      "train loss:0.020191648005616233\n",
      "train loss:0.01718174801141842\n",
      "train loss:0.0377467468964868\n",
      "train loss:0.03588269361143324\n",
      "train loss:0.009232015554447965\n",
      "train loss:0.04337930173564747\n",
      "train loss:0.05216695956326782\n",
      "train loss:0.03276563243322109\n",
      "train loss:0.025624060525190777\n",
      "train loss:0.028294874026479577\n",
      "train loss:0.08637397910920509\n",
      "train loss:0.005313889561263152\n",
      "train loss:0.0215047055735397\n",
      "train loss:0.03657358293190044\n",
      "train loss:0.030568966389011507\n",
      "train loss:0.05300578685304771\n",
      "train loss:0.012202360381834419\n",
      "train loss:0.03794635068986829\n",
      "train loss:0.025806640049024485\n",
      "train loss:0.054302986096839714\n",
      "train loss:0.1065567038301706\n",
      "train loss:0.047936969509125564\n",
      "train loss:0.04061727488945252\n",
      "train loss:0.06060933167064344\n",
      "train loss:0.027781499435309876\n",
      "train loss:0.02306967644975503\n",
      "train loss:0.030372338752611958\n",
      "train loss:0.0309556871765079\n",
      "train loss:0.00439496132103933\n",
      "train loss:0.058754692556652416\n",
      "train loss:0.05725580934871195\n",
      "train loss:0.024686003253792877\n",
      "train loss:0.013005077476525479\n",
      "train loss:0.09676513976585122\n",
      "train loss:0.03929883893190762\n",
      "train loss:0.11025405029439601\n",
      "train loss:0.025635269224653586\n",
      "train loss:0.009736009188919358\n",
      "train loss:0.03162475974777027\n",
      "train loss:0.1224223114959742\n",
      "train loss:0.01725657938521816\n",
      "train loss:0.04184747105661847\n",
      "train loss:0.013315301689865132\n",
      "train loss:0.049919243678827885\n",
      "train loss:0.03145967224802092\n",
      "train loss:0.06800790808978217\n",
      "train loss:0.02612140235744342\n",
      "train loss:0.06754273616834885\n",
      "train loss:0.03879923479954736\n",
      "train loss:0.019355936817791262\n",
      "train loss:0.3201239267641231\n",
      "train loss:0.09140222273216397\n",
      "train loss:0.017040651426848165\n",
      "train loss:0.010097968965375872\n",
      "train loss:0.057845474924376354\n",
      "train loss:0.04079386347648137\n",
      "train loss:0.033068827121296135\n",
      "train loss:0.011375377930455234\n",
      "train loss:0.019638725966408924\n",
      "train loss:0.014344825360606519\n",
      "train loss:0.04728518884904357\n",
      "train loss:0.012911971281009555\n",
      "train loss:0.09127061311537117\n",
      "train loss:0.03678035084583284\n",
      "train loss:0.02533001204776067\n",
      "train loss:0.032976333819077344\n",
      "train loss:0.015858942991248327\n",
      "train loss:0.019243051965665857\n",
      "train loss:0.022623548806049644\n",
      "train loss:0.0265818758110423\n",
      "train loss:0.0901379598830938\n",
      "train loss:0.012349045181368899\n",
      "train loss:0.06012050151561023\n",
      "train loss:0.016129124187134285\n",
      "train loss:0.06237736530253036\n",
      "train loss:0.013380316058366228\n",
      "train loss:0.07732252050855026\n",
      "train loss:0.1085949476352671\n",
      "train loss:0.012274724629697172\n",
      "train loss:0.030764911840795904\n",
      "train loss:0.03552691185668166\n",
      "train loss:0.07911618914166048\n",
      "train loss:0.02125737468165742\n",
      "train loss:0.09388971755080125\n",
      "train loss:0.022692899232202503\n",
      "train loss:0.02477788686329244\n",
      "train loss:0.042189753770496934\n",
      "train loss:0.016332840537186\n",
      "train loss:0.019092297093299734\n",
      "train loss:0.025023316431389366\n",
      "train loss:0.03394843753191721\n",
      "train loss:0.03627859752997289\n",
      "train loss:0.01429456052136566\n",
      "train loss:0.050156970165105966\n",
      "train loss:0.02742117181212721\n",
      "train loss:0.023274307506843492\n",
      "train loss:0.10936273473284441\n",
      "train loss:0.04522335838569294\n",
      "train loss:0.013708790319131097\n",
      "train loss:0.024342327431306057\n",
      "train loss:0.030971293734466422\n",
      "train loss:0.03931157186856926\n",
      "train loss:0.1724844354048314\n",
      "train loss:0.03774053827052882\n",
      "train loss:0.04629231294957815\n",
      "train loss:0.021614708547391163\n",
      "train loss:0.014454689042224198\n",
      "train loss:0.011395058085453179\n",
      "train loss:0.041791232256336484\n",
      "train loss:0.02496579801220616\n",
      "train loss:0.03174218243798381\n",
      "train loss:0.028434654954309652\n",
      "train loss:0.060702177948251945\n",
      "train loss:0.015266444457331954\n",
      "train loss:0.008291692848409575\n",
      "train loss:0.046076172928424766\n",
      "train loss:0.03813532195313491\n",
      "train loss:0.082709897786619\n",
      "train loss:0.03396587796298772\n",
      "train loss:0.023387092366596755\n",
      "train loss:0.007424692639379639\n",
      "train loss:0.0408341858447286\n",
      "train loss:0.015743556374870418\n",
      "train loss:0.09777827702694682\n",
      "train loss:0.019784165307531153\n",
      "train loss:0.12260076528791945\n",
      "train loss:0.045099913542392664\n",
      "train loss:0.03808847537271364\n",
      "train loss:0.02141800326088433\n",
      "train loss:0.04157758889246593\n",
      "train loss:0.02133058042359375\n",
      "train loss:0.02771104456313823\n",
      "train loss:0.09587649212334332\n",
      "train loss:0.03812610755918793\n",
      "train loss:0.018177421998153295\n",
      "train loss:0.04241232667338912\n",
      "train loss:0.021818993848756164\n",
      "train loss:0.01716469034657414\n",
      "train loss:0.02462629360554942\n",
      "train loss:0.03760333117640598\n",
      "train loss:0.048969005575321266\n",
      "train loss:0.026410202518958616\n",
      "train loss:0.04439465825894513\n",
      "train loss:0.0597742573010135\n",
      "train loss:0.025370412962246357\n",
      "train loss:0.023358837416446985\n",
      "train loss:0.0667656726869776\n",
      "train loss:0.01801450125185608\n",
      "train loss:0.01763061157842061\n",
      "train loss:0.023421379982089013\n",
      "train loss:0.01905869487576334\n",
      "train loss:0.020903074898114903\n",
      "train loss:0.03333934325144633\n",
      "train loss:0.05291975255553357\n",
      "train loss:0.020198274063032026\n",
      "train loss:0.022556417160053154\n",
      "train loss:0.1399966567180004\n",
      "train loss:0.017361645859582133\n",
      "train loss:0.061537407652149385\n",
      "train loss:0.012127890717275526\n",
      "train loss:0.0171894639521019\n",
      "train loss:0.0529898185264603\n",
      "train loss:0.0287583854796763\n",
      "train loss:0.020829656359560605\n",
      "train loss:0.04525290295002846\n",
      "train loss:0.03590346990101131\n",
      "train loss:0.05722642931260512\n",
      "train loss:0.033009850533963714\n",
      "train loss:0.06052417926816033\n",
      "train loss:0.09749412401213833\n",
      "train loss:0.012371748168952262\n",
      "train loss:0.04883510644157212\n",
      "train loss:0.013234211539048952\n",
      "train loss:0.04716514966095448\n",
      "train loss:0.029436110353255055\n",
      "train loss:0.011615354019404334\n",
      "train loss:0.03547487060957491\n",
      "train loss:0.04358516004791557\n",
      "train loss:0.014610673150062717\n",
      "train loss:0.06829531536057043\n",
      "train loss:0.03957983425187346\n",
      "train loss:0.05947238375945724\n",
      "train loss:0.02679631748216157\n",
      "train loss:0.08788411664257981\n",
      "train loss:0.027260650489125085\n",
      "train loss:0.016294179616379575\n",
      "train loss:0.03783869664582551\n",
      "train loss:0.030143474137528194\n",
      "train loss:0.01667864340863361\n",
      "train loss:0.008818377038876068\n",
      "train loss:0.07634704425132101\n",
      "train loss:0.032673577526143804\n",
      "train loss:0.0158144807440894\n",
      "train loss:0.08667823234621444\n",
      "train loss:0.025098017892086766\n",
      "train loss:0.04146000344788832\n",
      "train loss:0.04798681859851681\n",
      "train loss:0.10549191292048904\n",
      "train loss:0.03963980845297597\n",
      "train loss:0.07903775745050963\n",
      "train loss:0.011119718723763302\n",
      "train loss:0.04791093637258972\n",
      "train loss:0.03635158317573273\n",
      "train loss:0.025693500787461455\n",
      "train loss:0.07776479320083661\n",
      "train loss:0.016774291739698705\n",
      "train loss:0.0210357478603343\n",
      "train loss:0.09889457170568842\n",
      "train loss:0.025016285670198715\n",
      "train loss:0.11253587411102642\n",
      "train loss:0.012332416076098139\n",
      "train loss:0.014670195723363769\n",
      "train loss:0.047423763020347724\n",
      "train loss:0.11077246921323532\n",
      "train loss:0.05093585235948697\n",
      "train loss:0.016635307754660454\n",
      "train loss:0.01165659115352003\n",
      "train loss:0.08486541919515592\n",
      "train loss:0.04675403391636454\n",
      "train loss:0.04203176924097376\n",
      "train loss:0.08597534925388815\n",
      "train loss:0.031862948340169124\n",
      "train loss:0.045579656446180276\n",
      "train loss:0.00598419723321611\n",
      "train loss:0.013318220472278946\n",
      "train loss:0.04318780424301389\n",
      "train loss:0.03300755194142568\n",
      "train loss:0.008049658216096557\n",
      "train loss:0.06708351498866115\n",
      "train loss:0.019193331369120385\n",
      "train loss:0.013626091778718595\n",
      "train loss:0.015796354894297496\n",
      "train loss:0.028307485969078092\n",
      "train loss:0.008973161221558107\n",
      "train loss:0.020854266347605965\n",
      "train loss:0.0351830431904932\n",
      "train loss:0.007974820155615646\n",
      "train loss:0.06893552735104891\n",
      "train loss:0.0474464067268124\n",
      "train loss:0.013339790367764842\n",
      "train loss:0.030818038693201433\n",
      "train loss:0.03652535556737225\n",
      "train loss:0.06157375596985029\n",
      "train loss:0.030604739879643815\n",
      "train loss:0.041238066647072705\n",
      "train loss:0.008394780288196742\n",
      "train loss:0.00745822567962643\n",
      "train loss:0.03418297213649382\n",
      "train loss:0.035380552496754585\n",
      "train loss:0.05226916841928419\n",
      "train loss:0.013163189248259823\n",
      "train loss:0.02751926804713261\n",
      "train loss:0.0766641041600387\n",
      "train loss:0.01862275928449236\n",
      "train loss:0.05627195320409064\n",
      "train loss:0.012548249340577472\n",
      "train loss:0.04702380150262265\n",
      "train loss:0.028284857234550213\n",
      "train loss:0.04495420596659717\n",
      "train loss:0.04021502481001415\n",
      "train loss:0.028388641325341166\n",
      "train loss:0.03017534313235716\n",
      "train loss:0.027784674694571257\n",
      "train loss:0.13504608945261795\n",
      "train loss:0.028901610369108148\n",
      "train loss:0.019025523974758218\n",
      "train loss:0.019547554677888254\n",
      "train loss:0.022796084309945855\n",
      "train loss:0.06329694810069732\n",
      "train loss:0.03370731674775499\n",
      "train loss:0.027035647750560807\n",
      "train loss:0.052258100996955575\n",
      "train loss:0.01211827785368638\n",
      "train loss:0.0434896366520137\n",
      "train loss:0.02888213451384501\n",
      "train loss:0.012505896211838874\n",
      "train loss:0.011794222611176552\n",
      "train loss:0.13025356646129707\n",
      "train loss:0.014263869227718207\n",
      "train loss:0.03287556375922749\n",
      "train loss:0.013859553917218404\n",
      "train loss:0.06209592642492091\n",
      "train loss:0.019624882150601607\n",
      "train loss:0.010320949411303314\n",
      "train loss:0.019930239456613746\n",
      "train loss:0.013723685695959532\n",
      "train loss:0.09390364828739836\n",
      "train loss:0.04078739187268826\n",
      "train loss:0.016320298438559474\n",
      "train loss:0.034908987176327726\n",
      "train loss:0.05482248279267639\n",
      "train loss:0.016884741915187303\n",
      "train loss:0.05662835744738051\n",
      "train loss:0.029612083220469713\n",
      "train loss:0.020082539466768766\n",
      "train loss:0.019582629878703225\n",
      "train loss:0.014125324034932052\n",
      "train loss:0.017226427415253806\n",
      "train loss:0.03430782047994297\n",
      "train loss:0.06423172564260654\n",
      "train loss:0.05863815836843957\n",
      "train loss:0.011883893702134374\n",
      "train loss:0.01021643039616561\n",
      "train loss:0.01746472812311769\n",
      "train loss:0.005514850231256675\n",
      "train loss:0.018028685610779815\n",
      "train loss:0.021510701081480402\n",
      "train loss:0.0173916557835156\n",
      "train loss:0.02300421616113586\n",
      "train loss:0.10947047136622472\n",
      "train loss:0.010818754911415316\n",
      "train loss:0.011117391794814857\n",
      "train loss:0.07666513598382224\n",
      "train loss:0.010705072773418747\n",
      "train loss:0.02567761272778514\n",
      "train loss:0.0152170358261759\n",
      "train loss:0.030747296846596037\n",
      "train loss:0.03227747711262515\n",
      "train loss:0.05590421605969035\n",
      "train loss:0.017825607630696867\n",
      "train loss:0.05445761084401334\n",
      "train loss:0.007586238927483515\n",
      "train loss:0.03272659385593601\n",
      "train loss:0.03867886902673869\n",
      "train loss:0.02935870238694197\n",
      "train loss:0.04938884559643771\n",
      "train loss:0.010168954546635635\n",
      "train loss:0.02964966179125648\n",
      "train loss:0.028343231254622726\n",
      "train loss:0.02314515014257732\n",
      "train loss:0.02207155800085527\n",
      "train loss:0.018097089417590556\n",
      "train loss:0.022998584341683167\n",
      "train loss:0.014618175185569127\n",
      "train loss:0.0456352056012157\n",
      "train loss:0.007364186022337715\n",
      "train loss:0.006764963834703783\n",
      "train loss:0.008992872210612181\n",
      "train loss:0.03742402616336747\n",
      "train loss:0.03302766637128406\n",
      "train loss:0.035574836848324715\n",
      "train loss:0.003675942571798045\n",
      "train loss:0.036526480890223284\n",
      "train loss:0.0526641977439097\n",
      "train loss:0.019996306793031195\n",
      "train loss:0.021216493239082156\n",
      "train loss:0.0618772335110693\n",
      "train loss:0.022844082908765356\n",
      "train loss:0.011641611878791988\n",
      "train loss:0.0498761459418417\n",
      "train loss:0.030814610790425932\n",
      "train loss:0.06485121086037354\n",
      "train loss:0.021610529140755702\n",
      "train loss:0.07219675558644863\n",
      "train loss:0.027714942526084436\n",
      "train loss:0.03523459031137192\n",
      "train loss:0.02988024365246159\n",
      "train loss:0.03162673315647033\n",
      "train loss:0.04899462514490481\n",
      "train loss:0.008569460541328018\n",
      "train loss:0.061667754005230925\n",
      "train loss:0.08104882977909485\n",
      "train loss:0.020514600355728566\n",
      "train loss:0.034542518171592096\n",
      "train loss:0.02545767579119116\n",
      "train loss:0.037856173096600755\n",
      "train loss:0.015025380742717505\n",
      "train loss:0.016499300476983857\n",
      "train loss:0.028479990076838393\n",
      "train loss:0.015162091748965907\n",
      "train loss:0.006840585518618786\n",
      "train loss:0.01489049327906671\n",
      "train loss:0.01845212024034971\n",
      "train loss:0.009111717462360195\n",
      "train loss:0.005998639468882797\n",
      "train loss:0.010397321132090378\n",
      "train loss:0.028478179313768198\n",
      "train loss:0.03741280658984872\n",
      "train loss:0.043107340111384375\n",
      "train loss:0.03173954939180889\n",
      "train loss:0.033056828933510224\n",
      "train loss:0.016045941428871766\n",
      "train loss:0.030062782231222177\n",
      "train loss:0.07857945051878071\n",
      "train loss:0.038820958089635996\n",
      "train loss:0.01823697814727704\n",
      "train loss:0.05792651251360741\n",
      "train loss:0.0523327068800934\n",
      "train loss:0.04355239443896274\n",
      "train loss:0.05869122227541892\n",
      "train loss:0.003293752930507374\n",
      "train loss:0.012476418104685538\n",
      "train loss:0.011235403301833804\n",
      "train loss:0.00861952495854138\n",
      "train loss:0.02880652942705585\n",
      "train loss:0.017723757208103326\n",
      "train loss:0.01164678437843034\n",
      "train loss:0.06180298780129394\n",
      "train loss:0.028859837129770794\n",
      "train loss:0.004581035511229942\n",
      "train loss:0.021496559796359126\n",
      "train loss:0.051517301829555356\n",
      "train loss:0.013374972433196894\n",
      "train loss:0.03421346306313256\n",
      "train loss:0.07793182899761007\n",
      "train loss:0.02465177551005379\n",
      "train loss:0.01521435207981666\n",
      "train loss:0.016907412917090598\n",
      "train loss:0.03932338376484867\n",
      "train loss:0.020629146695367792\n",
      "train loss:0.02521678757650188\n",
      "train loss:0.015975984996940473\n",
      "train loss:0.026943495325201235\n",
      "train loss:0.013602052630076869\n",
      "train loss:0.01063713213032153\n",
      "train loss:0.009623122868320387\n",
      "train loss:0.03781197122629762\n",
      "train loss:0.05163042768095975\n",
      "train loss:0.07250239064082632\n",
      "train loss:0.01723736893619252\n",
      "train loss:0.030015401881105144\n",
      "train loss:0.02866530954564801\n",
      "train loss:0.006805923514859452\n",
      "train loss:0.024009453956539285\n",
      "train loss:0.04046502832014412\n",
      "train loss:0.054271552308301725\n",
      "train loss:0.009263127787440241\n",
      "train loss:0.0461505198531258\n",
      "train loss:0.0577920365441049\n",
      "train loss:0.05505254041703079\n",
      "train loss:0.011870287470956024\n",
      "train loss:0.017452510562032588\n",
      "train loss:0.013261864758743767\n",
      "train loss:0.03799558749293011\n",
      "train loss:0.02463825132884478\n",
      "train loss:0.021745052174342014\n",
      "train loss:0.010388422461053919\n",
      "train loss:0.03136881872980537\n",
      "train loss:0.025461175035236473\n",
      "train loss:0.02160825234503823\n",
      "train loss:0.0187666409412297\n",
      "train loss:0.021243264564788863\n",
      "train loss:0.00956364946993981\n",
      "train loss:0.024364310520211468\n",
      "train loss:0.019723478921266167\n",
      "train loss:0.03095033055394747\n",
      "train loss:0.0177200505464567\n",
      "train loss:0.016895484251065615\n",
      "train loss:0.005204059738862402\n",
      "train loss:0.029873924083007538\n",
      "train loss:0.05250967666467771\n",
      "train loss:0.028519414126634956\n",
      "train loss:0.004218648277661206\n",
      "train loss:0.05626871411153513\n",
      "train loss:0.05666909679041959\n",
      "train loss:0.011752015160328071\n",
      "train loss:0.0174369890055288\n",
      "train loss:0.08238497597606731\n",
      "train loss:0.09559681214371915\n",
      "train loss:0.13408880700624154\n",
      "train loss:0.05019090972884232\n",
      "train loss:0.07772003977422087\n",
      "train loss:0.01066100504383913\n",
      "train loss:0.06469094597475575\n",
      "train loss:0.10620783832617146\n",
      "train loss:0.026724178975116256\n",
      "train loss:0.022533643835265228\n",
      "train loss:0.0050315432080869\n",
      "train loss:0.007561657631142582\n",
      "train loss:0.07393514208320938\n",
      "train loss:0.031934143454243655\n",
      "train loss:0.04944725740188097\n",
      "train loss:0.008746325806492096\n",
      "train loss:0.0730204142232061\n",
      "train loss:0.01300001093856878\n",
      "train loss:0.07004643529157771\n",
      "train loss:0.03140271445696757\n",
      "train loss:0.07035891164240354\n",
      "train loss:0.01239889650393641\n",
      "train loss:0.02339853103849594\n",
      "train loss:0.01786699870438372\n",
      "current iter num:  2400\n",
      "=== epoch:5, train acc:0.986, test acc:0.984 ===\n",
      "train loss:0.03398765198533768\n",
      "train loss:0.08656082235218511\n",
      "train loss:0.029389760033282905\n",
      "train loss:0.014629913768122163\n",
      "train loss:0.013321651003736583\n",
      "train loss:0.035400636682426656\n",
      "train loss:0.037435038358942595\n",
      "train loss:0.031884501397180603\n",
      "train loss:0.08048951746276037\n",
      "train loss:0.00899743795217387\n",
      "train loss:0.01732729709806509\n",
      "train loss:0.021275138353127915\n",
      "train loss:0.02927446729280895\n",
      "train loss:0.05565953012329983\n",
      "train loss:0.02439075613148806\n",
      "train loss:0.013690083502601655\n",
      "train loss:0.05734495624182525\n",
      "train loss:0.035149272181897624\n",
      "train loss:0.03758989300875078\n",
      "train loss:0.0474032708862763\n",
      "train loss:0.01589989439172383\n",
      "train loss:0.0059289264721315614\n",
      "train loss:0.011097671565010873\n",
      "train loss:0.011886012099585985\n",
      "train loss:0.02856209908144717\n",
      "train loss:0.06167877869429545\n",
      "train loss:0.016539293946079864\n",
      "train loss:0.02332563977836179\n",
      "train loss:0.061117767830638305\n",
      "train loss:0.011730654012354999\n",
      "train loss:0.02475044388056964\n",
      "train loss:0.01978639455575443\n",
      "train loss:0.015728384333995843\n",
      "train loss:0.0231120360878582\n",
      "train loss:0.05278722422713268\n",
      "train loss:0.03386966870499378\n",
      "train loss:0.04848524810864872\n",
      "train loss:0.006520147560830651\n",
      "train loss:0.02809470211341867\n",
      "train loss:0.029218130377030808\n",
      "train loss:0.007240911486321101\n",
      "train loss:0.045584308859289005\n",
      "train loss:0.012554137742474965\n",
      "train loss:0.016295875944295775\n",
      "train loss:0.04416004121364713\n",
      "train loss:0.05529024566059324\n",
      "train loss:0.048448071332155306\n",
      "train loss:0.01511944673058548\n",
      "train loss:0.017670246731839586\n",
      "train loss:0.017764772887231075\n",
      "train loss:0.017398388105248398\n",
      "train loss:0.03299218900985224\n",
      "train loss:0.044536714603961684\n",
      "train loss:0.013047461558911413\n",
      "train loss:0.011105700029267444\n",
      "train loss:0.00538041955642718\n",
      "train loss:0.039709660773199686\n",
      "train loss:0.0184921147595574\n",
      "train loss:0.018017579870063992\n",
      "train loss:0.03206299401857927\n",
      "train loss:0.012779655608351653\n",
      "train loss:0.014841346640117308\n",
      "train loss:0.02011549833984867\n",
      "train loss:0.016179539109236998\n",
      "train loss:0.08191785925136409\n",
      "train loss:0.04326932548481877\n",
      "train loss:0.1847346602645539\n",
      "train loss:0.03292123602171205\n",
      "train loss:0.00403528774568403\n",
      "train loss:0.012449620744814795\n",
      "train loss:0.015104206725878597\n",
      "train loss:0.015779478355326355\n",
      "train loss:0.04117838588078912\n",
      "train loss:0.1294813414848498\n",
      "train loss:0.02058615469897575\n",
      "train loss:0.03163997111451344\n",
      "train loss:0.04015901919863658\n",
      "train loss:0.023415395451256673\n",
      "train loss:0.011157673279653533\n",
      "train loss:0.04694544358179296\n",
      "train loss:0.015067099417465113\n",
      "train loss:0.004072625370338824\n",
      "train loss:0.03427864834309388\n",
      "train loss:0.069094731319823\n",
      "train loss:0.008256510703231737\n",
      "train loss:0.011307784040956961\n",
      "train loss:0.025131792195633097\n",
      "train loss:0.00555509757768402\n",
      "train loss:0.04508207326528932\n",
      "train loss:0.030107330436987708\n",
      "train loss:0.007970998259378505\n",
      "train loss:0.01776051419069718\n",
      "train loss:0.006307346519710794\n",
      "train loss:0.01289942808942406\n",
      "train loss:0.01669367489490432\n",
      "train loss:0.06110031916312694\n",
      "train loss:0.010514672746808541\n",
      "train loss:0.015440451392671084\n",
      "train loss:0.029997676424952248\n",
      "train loss:0.02322783341412891\n",
      "train loss:0.016280558496484566\n",
      "train loss:0.08098671329682952\n",
      "train loss:0.014844468338651572\n",
      "train loss:0.004354944943139336\n",
      "train loss:0.01954961559189059\n",
      "train loss:0.03316485138666467\n",
      "train loss:0.020654469949125634\n",
      "train loss:0.029750143563990295\n",
      "train loss:0.029576691365485765\n",
      "train loss:0.011625171331226399\n",
      "train loss:0.08936741659981692\n",
      "train loss:0.017985931353635073\n",
      "train loss:0.010145213848593643\n",
      "train loss:0.05659387748469869\n",
      "train loss:0.0506224574691944\n",
      "train loss:0.0032485980419134546\n",
      "train loss:0.02806121230372225\n",
      "train loss:0.03047071836840272\n",
      "train loss:0.02712772575824807\n",
      "train loss:0.03180121105831052\n",
      "train loss:0.078504572094116\n",
      "train loss:0.03593403287849032\n",
      "train loss:0.02981505072910866\n",
      "train loss:0.020258302465395173\n",
      "train loss:0.008859521353748607\n",
      "train loss:0.047935058568773066\n",
      "train loss:0.09827411871194532\n",
      "train loss:0.033483743420870224\n",
      "train loss:0.07174170764327827\n",
      "train loss:0.008116417681127738\n",
      "train loss:0.006308540124855944\n",
      "train loss:0.042757073042097554\n",
      "train loss:0.02360122982816342\n",
      "train loss:0.04033246086488257\n",
      "train loss:0.01254548980090719\n",
      "train loss:0.015901430747838185\n",
      "train loss:0.052868727149764254\n",
      "train loss:0.03257433552991556\n",
      "train loss:0.020340465849471176\n",
      "train loss:0.020724989714921607\n",
      "train loss:0.009573125311877527\n",
      "train loss:0.014688341662329008\n",
      "train loss:0.08608377814966639\n",
      "train loss:0.02057201088540596\n",
      "train loss:0.03692646483072548\n",
      "train loss:0.011287909520877269\n",
      "train loss:0.06718308778647983\n",
      "train loss:0.010266939616306694\n",
      "train loss:0.014743915938146761\n",
      "train loss:0.015853311042565578\n",
      "train loss:0.018297201493548382\n",
      "train loss:0.012937203584402389\n",
      "train loss:0.02347791909235881\n",
      "train loss:0.03211246670334658\n",
      "train loss:0.04275291569861074\n",
      "train loss:0.03268551038463709\n",
      "train loss:0.007042394698946802\n",
      "train loss:0.011006252667585188\n",
      "train loss:0.019855976028930605\n",
      "train loss:0.05872339897454364\n",
      "train loss:0.02181092741211048\n",
      "train loss:0.0058843355457489895\n",
      "train loss:0.018781092781150734\n",
      "train loss:0.04129742573731887\n",
      "train loss:0.049689042506861174\n",
      "train loss:0.10687910571898801\n",
      "train loss:0.025934928509166743\n",
      "train loss:0.007910048202892726\n",
      "train loss:0.03688084384552948\n",
      "train loss:0.05854755440566881\n",
      "train loss:0.03640264323663582\n",
      "train loss:0.009689966990971273\n",
      "train loss:0.008255159652654909\n",
      "train loss:0.017790431393532747\n",
      "train loss:0.010522619419407993\n",
      "train loss:0.00597640580708466\n",
      "train loss:0.04173274827232029\n",
      "train loss:0.013283396201913728\n",
      "train loss:0.013777456149812607\n",
      "train loss:0.014594326215722184\n",
      "train loss:0.006802422871685802\n",
      "train loss:0.03470581427474096\n",
      "train loss:0.008313208552668596\n",
      "train loss:0.08643163958232228\n",
      "train loss:0.054507346227151185\n",
      "train loss:0.00856430699996274\n",
      "train loss:0.014940516192631986\n",
      "train loss:0.022071584260210376\n",
      "train loss:0.018039166102489915\n",
      "train loss:0.07897846622929917\n",
      "train loss:0.012320253751452388\n",
      "train loss:0.07234468138364365\n",
      "train loss:0.01967892385398257\n",
      "train loss:0.007392944535084621\n",
      "train loss:0.016244758104023132\n",
      "train loss:0.02793375281601938\n",
      "train loss:0.023163586295609488\n",
      "train loss:0.021383073286832945\n",
      "train loss:0.009328920774735084\n",
      "train loss:0.02865330571809988\n",
      "train loss:0.04232983859208151\n",
      "train loss:0.023754682302289623\n",
      "train loss:0.05751659791192588\n",
      "train loss:0.0730833075534852\n",
      "train loss:0.033725934473825346\n",
      "train loss:0.020754667526153594\n",
      "train loss:0.008123693367915671\n",
      "train loss:0.022159507448419314\n",
      "train loss:0.004220732723541917\n",
      "train loss:0.0030227553080686537\n",
      "train loss:0.019703024304732503\n",
      "train loss:0.007376478576371664\n",
      "train loss:0.008464687914711347\n",
      "train loss:0.019659898429027035\n",
      "train loss:0.01672472651989395\n",
      "train loss:0.03921982103154807\n",
      "train loss:0.04241363084288344\n",
      "train loss:0.02366918414123517\n",
      "train loss:0.017564823966131914\n",
      "train loss:0.009219268173816826\n",
      "train loss:0.015766663408483796\n",
      "train loss:0.0054528791940193925\n",
      "train loss:0.02162551547156893\n",
      "train loss:0.046787689699347684\n",
      "train loss:0.05211428687095878\n",
      "train loss:0.03437239350241589\n",
      "train loss:0.008966783791439933\n",
      "train loss:0.019139586317762216\n",
      "train loss:0.01534617590112854\n",
      "train loss:0.002980678745695396\n",
      "train loss:0.006984788038235856\n",
      "train loss:0.017353106719491414\n",
      "train loss:0.01501923216552103\n",
      "train loss:0.006165291889151504\n",
      "train loss:0.04699779221019198\n",
      "train loss:0.03338402128345659\n",
      "train loss:0.011019007307420807\n",
      "train loss:0.032780393949971696\n",
      "train loss:0.07956170533349934\n",
      "train loss:0.016923164803668816\n",
      "train loss:0.020089166065876154\n",
      "train loss:0.056151171389217255\n",
      "train loss:0.015704423922926317\n",
      "train loss:0.0651931187013371\n",
      "train loss:0.028031824029497075\n",
      "train loss:0.0022422853764447587\n",
      "train loss:0.04457070778810945\n",
      "train loss:0.012441483546921222\n",
      "train loss:0.05444556159586002\n",
      "train loss:0.07014473781832054\n",
      "train loss:0.026851373558453743\n",
      "train loss:0.016774669058032238\n",
      "train loss:0.02209466274007855\n",
      "train loss:0.03165850088877969\n",
      "train loss:0.016713182146786562\n",
      "train loss:0.04095326468821689\n",
      "train loss:0.0047566593821481455\n",
      "train loss:0.03686321997878142\n",
      "train loss:0.009242470435315491\n",
      "train loss:0.00717666476241256\n",
      "train loss:0.04007693791787525\n",
      "train loss:0.01188195507764218\n",
      "train loss:0.04901721643026739\n",
      "train loss:0.04290714472959793\n",
      "train loss:0.043658404912277404\n",
      "train loss:0.05170252808121328\n",
      "train loss:0.0710427199640143\n",
      "train loss:0.028296088455002675\n",
      "train loss:0.008656343679148013\n",
      "train loss:0.029787243179077074\n",
      "train loss:0.10928819071874905\n",
      "train loss:0.024297541861903915\n",
      "train loss:0.07667562731279666\n",
      "train loss:0.038176573194653514\n",
      "train loss:0.016856176816241936\n",
      "train loss:0.02610247465206035\n",
      "train loss:0.005452092555211246\n",
      "train loss:0.008379519516981149\n",
      "train loss:0.01353104283262433\n",
      "train loss:0.00260945228500199\n",
      "train loss:0.027398214509662955\n",
      "train loss:0.04363823742750552\n",
      "train loss:0.05767732807330157\n",
      "train loss:0.04878725418825825\n",
      "train loss:0.03010336234322514\n",
      "train loss:0.018157966105336192\n",
      "train loss:0.0633120738175494\n",
      "train loss:0.008539940901931713\n",
      "train loss:0.012803991845955731\n",
      "train loss:0.020963637288221034\n",
      "train loss:0.047614324970639714\n",
      "train loss:0.004776047663880633\n",
      "train loss:0.03982586722496137\n",
      "train loss:0.07439399786540409\n",
      "train loss:0.016131779903186018\n",
      "train loss:0.13954775470009492\n",
      "train loss:0.007294295025639088\n",
      "train loss:0.006938697801770691\n",
      "train loss:0.04796695608124784\n",
      "train loss:0.013174653527129569\n",
      "train loss:0.0043970788809815755\n",
      "train loss:0.038139080646832565\n",
      "train loss:0.033003029807689065\n",
      "train loss:0.016550152174041825\n",
      "train loss:0.011890589439341046\n",
      "train loss:0.016628678042291754\n",
      "train loss:0.042629689823242306\n",
      "train loss:0.09541164527603263\n",
      "train loss:0.005490709616494881\n",
      "train loss:0.031100043759837204\n",
      "train loss:0.020043770313376893\n",
      "train loss:0.03860121458157451\n",
      "train loss:0.156545525810074\n",
      "train loss:0.024188481520360197\n",
      "train loss:0.023062999865291558\n",
      "train loss:0.007153356644538639\n",
      "train loss:0.010462874450849919\n",
      "train loss:0.09112735161842356\n",
      "train loss:0.02630891297349567\n",
      "train loss:0.10470490746427043\n",
      "train loss:0.02152010663675097\n",
      "train loss:0.014033321398850279\n",
      "train loss:0.061849461318595804\n",
      "train loss:0.0229216104677228\n",
      "train loss:0.006343022178313974\n",
      "train loss:0.01676078649109152\n",
      "train loss:0.008547673746055107\n",
      "train loss:0.04400895764447491\n",
      "train loss:0.016810260947373347\n",
      "train loss:0.07776794714833524\n",
      "train loss:0.013089299016754805\n",
      "train loss:0.030683220180872182\n",
      "train loss:0.02288436490411816\n",
      "train loss:0.07275384283203093\n",
      "train loss:0.016600905775229685\n",
      "train loss:0.031926788418427\n",
      "train loss:0.03706182377709743\n",
      "train loss:0.009583759654716842\n",
      "train loss:0.012814864369440071\n",
      "train loss:0.016422411017226916\n",
      "train loss:0.042410998504521354\n",
      "train loss:0.02597970518998915\n",
      "train loss:0.026529276933297305\n",
      "train loss:0.007302438090670839\n",
      "train loss:0.010654825396089166\n",
      "train loss:0.022067849584211804\n",
      "train loss:0.0069415548074511\n",
      "train loss:0.08757595642885681\n",
      "train loss:0.05564181016236829\n",
      "train loss:0.009241072309201827\n",
      "train loss:0.04258623000931891\n",
      "train loss:0.032806939786067045\n",
      "train loss:0.015654877493686194\n",
      "train loss:0.0407825170771699\n",
      "train loss:0.04787127560254323\n",
      "train loss:0.07305222485930181\n",
      "train loss:0.16988956265858615\n",
      "train loss:0.01702578854597989\n",
      "train loss:0.06224211727557944\n",
      "train loss:0.03463706753427143\n",
      "train loss:0.08048635934739591\n",
      "train loss:0.08340448696217242\n",
      "train loss:0.01729405608967236\n",
      "train loss:0.061658800168867246\n",
      "train loss:0.02961567025577359\n",
      "train loss:0.029524981076838482\n",
      "train loss:0.01509810149878408\n",
      "train loss:0.033873209452677466\n",
      "train loss:0.03775670544596224\n",
      "train loss:0.0210987830831209\n",
      "train loss:0.03480583195738615\n",
      "train loss:0.03437495823105448\n",
      "train loss:0.012381926311207679\n",
      "train loss:0.009173279043982723\n",
      "train loss:0.029041090966757954\n",
      "train loss:0.012409876801190513\n",
      "train loss:0.058390673022219515\n",
      "train loss:0.01306850902116854\n",
      "train loss:0.009329412633853862\n",
      "train loss:0.029051159282342694\n",
      "train loss:0.01811355926771779\n",
      "train loss:0.029797721932447258\n",
      "train loss:0.01557186732425963\n",
      "train loss:0.013187568725025849\n",
      "train loss:0.0539017921639863\n",
      "train loss:0.08104669669736858\n",
      "train loss:0.030714247344450604\n",
      "train loss:0.015512078754111923\n",
      "train loss:0.005978640013477889\n",
      "train loss:0.00959680398637862\n",
      "train loss:0.010458913897922544\n",
      "train loss:0.018725184000484073\n",
      "train loss:0.041517272160446685\n",
      "train loss:0.08161264029703355\n",
      "train loss:0.027671063513142743\n",
      "train loss:0.025946439581042608\n",
      "train loss:0.039657697877015986\n",
      "train loss:0.020492156855610198\n",
      "train loss:0.031058389516093063\n",
      "train loss:0.033039542417246555\n",
      "train loss:0.006706551515169254\n",
      "train loss:0.03437949307329729\n",
      "train loss:0.020437220581944467\n",
      "train loss:0.03432233311645507\n",
      "train loss:0.013379740501407143\n",
      "train loss:0.018895713365640002\n",
      "train loss:0.013328923948358882\n",
      "train loss:0.009418645040482553\n",
      "train loss:0.03489273602335161\n",
      "train loss:0.003494636185835352\n",
      "train loss:0.1906962846169692\n",
      "train loss:0.017121234582109075\n",
      "train loss:0.0320423413614244\n",
      "train loss:0.024037718123158663\n",
      "train loss:0.010071791306422571\n",
      "train loss:0.07271660998025885\n",
      "train loss:0.02486414047399561\n",
      "train loss:0.008435149286109861\n",
      "train loss:0.009441390235980097\n",
      "train loss:0.022152390915824507\n",
      "train loss:0.10024295622359133\n",
      "train loss:0.014949891953495848\n",
      "train loss:0.04413348572654634\n",
      "train loss:0.02650044208490602\n",
      "train loss:0.1257982351873034\n",
      "train loss:0.009904897351344859\n",
      "train loss:0.039211562653910736\n",
      "train loss:0.019063630476477453\n",
      "train loss:0.02400700103834281\n",
      "train loss:0.00840342683613508\n",
      "train loss:0.007539223907189283\n",
      "train loss:0.07236787812839054\n",
      "train loss:0.00512182926531723\n",
      "train loss:0.03605202611048308\n",
      "train loss:0.011689081437189934\n",
      "train loss:0.05027110092603244\n",
      "train loss:0.03072769264201017\n",
      "train loss:0.01800455124977166\n",
      "train loss:0.01729511742164572\n",
      "train loss:0.03217812179323166\n",
      "train loss:0.035907615932166424\n",
      "train loss:0.007372745824567769\n",
      "train loss:0.014982905317212645\n",
      "train loss:0.006094293713416113\n",
      "train loss:0.008229036624654289\n",
      "train loss:0.06509963170554309\n",
      "train loss:0.016032736571793483\n",
      "train loss:0.02160403920237031\n",
      "train loss:0.0028831160463445\n",
      "train loss:0.011096774306938641\n",
      "train loss:0.006013914086269239\n",
      "train loss:0.02929565789585263\n",
      "train loss:0.06320915583233594\n",
      "train loss:0.011002748256763555\n",
      "train loss:0.047292525319866385\n",
      "train loss:0.009659346267679838\n",
      "train loss:0.01607175155998298\n",
      "train loss:0.0140676071252219\n",
      "train loss:0.01494934439020809\n",
      "train loss:0.029040922352515156\n",
      "train loss:0.009375424252506965\n",
      "train loss:0.009420044429266949\n",
      "train loss:0.02558232456424833\n",
      "train loss:0.01340962763948486\n",
      "train loss:0.01890041420042422\n",
      "train loss:0.008866277973284629\n",
      "train loss:0.01455908014838506\n",
      "train loss:0.007520723583735005\n",
      "train loss:0.042779629011621925\n",
      "train loss:0.032356182768018736\n",
      "train loss:0.007855906415384303\n",
      "train loss:0.02370628886520628\n",
      "train loss:0.03154180787367132\n",
      "train loss:0.014490407455556398\n",
      "train loss:0.004446452486047546\n",
      "train loss:0.01844807514736131\n",
      "train loss:0.015048310851924724\n",
      "train loss:0.03622968726706073\n",
      "train loss:0.07065238910052912\n",
      "train loss:0.016702927132280262\n",
      "train loss:0.009190045486035178\n",
      "train loss:0.0030807246417729247\n",
      "train loss:0.04000923619650985\n",
      "train loss:0.016775523528130255\n",
      "train loss:0.014401350152082557\n",
      "train loss:0.01974510801959199\n",
      "train loss:0.01394092009541804\n",
      "train loss:0.017634528521135368\n",
      "train loss:0.05845122926963559\n",
      "train loss:0.04599929022889215\n",
      "train loss:0.00867584789197043\n",
      "train loss:0.007915904891867024\n",
      "train loss:0.015475615724633338\n",
      "train loss:0.03738286591928864\n",
      "train loss:0.006676738194295808\n",
      "train loss:0.014157088132590992\n",
      "train loss:0.010453254135370937\n",
      "train loss:0.005391762281361844\n",
      "train loss:0.015882220825250867\n",
      "train loss:0.01716135700188516\n",
      "train loss:0.012737205148377884\n",
      "train loss:0.06348906122237968\n",
      "train loss:0.005737094938797069\n",
      "train loss:0.011713704840507797\n",
      "train loss:0.01711689283983758\n",
      "train loss:0.011847644223613065\n",
      "train loss:0.015161348893979965\n",
      "train loss:0.03528521827889333\n",
      "train loss:0.011702108910226803\n",
      "train loss:0.014416698345270818\n",
      "train loss:0.0033178805001398896\n",
      "train loss:0.021808388668771966\n",
      "train loss:0.03520215544503926\n",
      "train loss:0.07190408996913175\n",
      "train loss:0.02574472956442889\n",
      "train loss:0.018308965982043346\n",
      "train loss:0.013349908307455567\n",
      "train loss:0.039771624535638136\n",
      "train loss:0.010176043935149669\n",
      "train loss:0.04751631656917184\n",
      "train loss:0.05486247214113296\n",
      "train loss:0.01276421879840376\n",
      "train loss:0.035543978888999725\n",
      "train loss:0.010248184732410723\n",
      "train loss:0.033893873871032675\n",
      "train loss:0.01604800824572753\n",
      "train loss:0.03535469211867095\n",
      "train loss:0.0027503541858171915\n",
      "train loss:0.005950913676951791\n",
      "train loss:0.020512657782614286\n",
      "train loss:0.014040222691317297\n",
      "train loss:0.017474661335607152\n",
      "train loss:0.020344064152481654\n",
      "train loss:0.003535090835757492\n",
      "train loss:0.025108914424348493\n",
      "train loss:0.015437287935145448\n",
      "train loss:0.026704959142922972\n",
      "train loss:0.029060067420496383\n",
      "train loss:0.011448438732765914\n",
      "train loss:0.008258067875208545\n",
      "train loss:0.028847675720198834\n",
      "train loss:0.008179297249219853\n",
      "train loss:0.051299128444762206\n",
      "train loss:0.02037571545419719\n",
      "train loss:0.04352793253547241\n",
      "train loss:0.03581849202783223\n",
      "train loss:0.0029523745571158667\n",
      "train loss:0.009309736727128055\n",
      "train loss:0.03846088770156589\n",
      "train loss:0.04302134374340052\n",
      "train loss:0.01399791087282417\n",
      "train loss:0.012548699893418305\n",
      "train loss:0.021257032860608628\n",
      "train loss:0.051712808800348925\n",
      "train loss:0.0194480993539217\n",
      "train loss:0.023225941379791584\n",
      "train loss:0.03732485800366781\n",
      "train loss:0.054433783869218495\n",
      "train loss:0.01648068552797273\n",
      "train loss:0.01738122706244428\n",
      "train loss:0.01903896794031445\n",
      "train loss:0.02696714172923944\n",
      "train loss:0.007750000766501511\n",
      "train loss:0.007469342622778016\n",
      "train loss:0.011490840621417163\n",
      "train loss:0.01188781883518979\n",
      "train loss:0.008957775583732352\n",
      "train loss:0.03907653759564415\n",
      "train loss:0.00953687594622203\n",
      "train loss:0.006893129657118668\n",
      "train loss:0.03637072044737494\n",
      "train loss:0.03207460926071314\n",
      "train loss:0.05288220871452291\n",
      "train loss:0.025059600623217628\n",
      "train loss:0.018641290002854048\n",
      "train loss:0.027761741641723514\n",
      "train loss:0.011518292961130732\n",
      "train loss:0.02274659140742593\n",
      "train loss:0.05708739540622978\n",
      "train loss:0.02633656855762184\n",
      "train loss:0.010008102990526761\n",
      "train loss:0.006831905746675793\n",
      "train loss:0.007699771200120124\n",
      "train loss:0.005521936112835495\n",
      "train loss:0.003513377727759481\n",
      "train loss:0.019276192755690275\n",
      "train loss:0.03303249868365102\n",
      "train loss:0.036933681310543795\n",
      "train loss:0.002858862443564902\n",
      "train loss:0.011227170251074224\n",
      "train loss:0.11129334728025934\n",
      "train loss:0.02415236139768203\n",
      "train loss:0.03685246072755993\n",
      "train loss:0.024439156610956917\n",
      "train loss:0.02370737209843055\n",
      "train loss:0.05955931948028395\n",
      "train loss:0.016501437873643537\n",
      "train loss:0.04552012751927086\n",
      "train loss:0.012097369347641405\n",
      "train loss:0.015347915456541366\n",
      "current iter num:  3000\n",
      "=== epoch:6, train acc:0.988, test acc:0.981 ===\n",
      "train loss:0.021908384413938827\n",
      "train loss:0.014253549852524982\n",
      "train loss:0.01519619431234242\n",
      "train loss:0.018847273228629124\n",
      "train loss:0.07652342389028723\n",
      "train loss:0.01652903207208516\n",
      "train loss:0.11018133972200012\n",
      "train loss:0.015774641378659703\n",
      "train loss:0.026269403674403113\n",
      "train loss:0.019425343115176352\n",
      "train loss:0.03211431028506196\n",
      "train loss:0.029409177937870526\n",
      "train loss:0.011235329755896776\n",
      "train loss:0.005436516405692364\n",
      "train loss:0.03225932802209345\n",
      "train loss:0.14056612065204485\n",
      "train loss:0.011262722477146577\n",
      "train loss:0.02092432543264346\n",
      "train loss:0.01147624809094555\n",
      "train loss:0.009863248160710976\n",
      "train loss:0.006840224881290625\n",
      "train loss:0.02669835532863303\n",
      "train loss:0.032212915914053744\n",
      "train loss:0.03678765693544285\n",
      "train loss:0.0153726187089516\n",
      "train loss:0.10162017569031022\n",
      "train loss:0.017828947510105053\n",
      "train loss:0.028446568692712358\n",
      "train loss:0.009948653101268263\n",
      "train loss:0.014776511386134571\n",
      "train loss:0.050452633933414086\n",
      "train loss:0.04666582679349755\n",
      "train loss:0.039568998393002326\n",
      "train loss:0.006548595362518388\n",
      "train loss:0.018105333571634238\n",
      "train loss:0.02043766613820748\n",
      "train loss:0.023365996330101436\n",
      "train loss:0.030357095309732417\n",
      "train loss:0.023886812436017382\n",
      "train loss:0.10056250086004674\n",
      "train loss:0.040689208616146694\n",
      "train loss:0.01686294228931418\n",
      "train loss:0.013542227295229142\n",
      "train loss:0.02322801505649328\n",
      "train loss:0.0055497060632910735\n",
      "train loss:0.007925273942474524\n",
      "train loss:0.007781271197979069\n",
      "train loss:0.033212619552785594\n",
      "train loss:0.024067011157506205\n",
      "train loss:0.06645066034701345\n",
      "train loss:0.015218605182622871\n",
      "train loss:0.05413890755099732\n",
      "train loss:0.0266132114267201\n",
      "train loss:0.01021630615659485\n",
      "train loss:0.013769933988136116\n",
      "train loss:0.044611885602883115\n",
      "train loss:0.020909508396568736\n",
      "train loss:0.013904335364532384\n",
      "train loss:0.016081593047776625\n",
      "train loss:0.03576205065889189\n",
      "train loss:0.0036993457040376677\n",
      "train loss:0.017257354030447907\n",
      "train loss:0.01958522428566579\n",
      "train loss:0.0033501917886967036\n",
      "train loss:0.10878905896444534\n",
      "train loss:0.01587855355330624\n",
      "train loss:0.027855546711370294\n",
      "train loss:0.019005444739644394\n",
      "train loss:0.01625140055453498\n",
      "train loss:0.01354201777576063\n",
      "train loss:0.011839969970518725\n",
      "train loss:0.009104184839526802\n",
      "train loss:0.0362898754514123\n",
      "train loss:0.021233556403647172\n",
      "train loss:0.012598907816904013\n",
      "train loss:0.02745153975513319\n",
      "train loss:0.01667162978218761\n",
      "train loss:0.009597806743635681\n",
      "train loss:0.017938247203432317\n",
      "train loss:0.012527527966188666\n",
      "train loss:0.022281219162005126\n",
      "train loss:0.08016167682959051\n",
      "train loss:0.046610469798551726\n",
      "train loss:0.030470824767174248\n",
      "train loss:0.04479807903754514\n",
      "train loss:0.030939471156026684\n",
      "train loss:0.02531127379237178\n",
      "train loss:0.014893483285381514\n",
      "train loss:0.030546964798978286\n",
      "train loss:0.011770246402886238\n",
      "train loss:0.025178965139260254\n",
      "train loss:0.015407083020272726\n",
      "train loss:0.015830434201824676\n",
      "train loss:0.00858763287842632\n",
      "train loss:0.06312487036657533\n",
      "train loss:0.007716913939866438\n",
      "train loss:0.005156095063390527\n",
      "train loss:0.01069392871560731\n",
      "train loss:0.012127871917181612\n",
      "train loss:0.009850962490057673\n",
      "train loss:0.032793249105750606\n",
      "train loss:0.011397849172215632\n",
      "train loss:0.01749249738089668\n",
      "train loss:0.006112577389303443\n",
      "train loss:0.012157548353911453\n",
      "train loss:0.007454349425503128\n",
      "train loss:0.02898145173830401\n",
      "train loss:0.011988212108127883\n",
      "train loss:0.016084734108482043\n",
      "train loss:0.013783637121050135\n",
      "train loss:0.01902803552830802\n",
      "train loss:0.004645015968991147\n",
      "train loss:0.02096250490283079\n",
      "train loss:0.01384941640010362\n",
      "train loss:0.00880154390800737\n",
      "train loss:0.03375643848727293\n",
      "train loss:0.0319227969182839\n",
      "train loss:0.009146665782928089\n",
      "train loss:0.002007084952011768\n",
      "train loss:0.01940291278787575\n",
      "train loss:0.04796684163417174\n",
      "train loss:0.0087303695367751\n",
      "train loss:0.015887933803789122\n",
      "train loss:0.01842002802559472\n",
      "train loss:0.009491312305710337\n",
      "train loss:0.004133488792735986\n",
      "train loss:0.023082826565064635\n",
      "train loss:0.012506667596566778\n",
      "train loss:0.032457602943576094\n",
      "train loss:0.03989480223372233\n",
      "train loss:0.04862973420894257\n",
      "train loss:0.04359619980052579\n",
      "train loss:0.04632075078981605\n",
      "train loss:0.018238010251651157\n",
      "train loss:0.012608905212988512\n",
      "train loss:0.0077182859928387445\n",
      "train loss:0.010362350886887526\n",
      "train loss:0.024045882474999454\n",
      "train loss:0.0130029653887459\n",
      "train loss:0.00493271199577602\n",
      "train loss:0.011553541576301098\n",
      "train loss:0.032053940744095476\n",
      "train loss:0.029096925328206246\n",
      "train loss:0.010316802824982476\n",
      "train loss:0.016908093917050136\n",
      "train loss:0.015862287231296252\n",
      "train loss:0.06864759728187994\n",
      "train loss:0.01778187844317987\n",
      "train loss:0.031251839912837774\n",
      "train loss:0.00280217096319772\n",
      "train loss:0.016932026855324956\n",
      "train loss:0.05144428858754682\n",
      "train loss:0.006002824578942773\n",
      "train loss:0.0842881042951574\n",
      "train loss:0.0339117399774648\n",
      "train loss:0.007136277537844003\n",
      "train loss:0.003979231255140181\n",
      "train loss:0.008808228280997962\n",
      "train loss:0.006608967592725414\n",
      "train loss:0.012080563456398274\n",
      "train loss:0.023197616642789017\n",
      "train loss:0.010911278161271035\n",
      "train loss:0.01454840419767736\n",
      "train loss:0.006151592194651144\n",
      "train loss:0.00989528374800505\n",
      "train loss:0.025270108203094884\n",
      "train loss:0.06239654835802852\n",
      "train loss:0.009670177040512812\n",
      "train loss:0.029024770281792057\n",
      "train loss:0.01714194950694541\n",
      "train loss:0.018789539530472202\n",
      "train loss:0.05162640471967224\n",
      "train loss:0.008395929527038014\n",
      "train loss:0.0020873054427769594\n",
      "train loss:0.013249207272932228\n",
      "train loss:0.0178919798597805\n",
      "train loss:0.011768754663344323\n",
      "train loss:0.015666612164208366\n",
      "train loss:0.023333699755403985\n",
      "train loss:0.005781291621578373\n",
      "train loss:0.028077592351548622\n",
      "train loss:0.012568492906387747\n",
      "train loss:0.06084375024435731\n",
      "train loss:0.01117441657384447\n",
      "train loss:0.031575772366433615\n",
      "train loss:0.011296419617254883\n",
      "train loss:0.026073760481349274\n",
      "train loss:0.01444650865453923\n",
      "train loss:0.049901491325376864\n",
      "train loss:0.01107321943824209\n",
      "train loss:0.055762263473196017\n",
      "train loss:0.0041444684084629875\n",
      "train loss:0.017341963099309698\n",
      "train loss:0.008954248419913364\n",
      "train loss:0.019949557360920304\n",
      "train loss:0.01683248808845291\n",
      "train loss:0.12844530754926098\n",
      "train loss:0.015896808243144638\n",
      "train loss:0.006030511879839039\n",
      "train loss:0.02141244145576381\n",
      "train loss:0.0158878777345131\n",
      "train loss:0.00973708441391253\n",
      "train loss:0.056308227478457186\n",
      "train loss:0.03619239884834338\n",
      "train loss:0.028155842030816634\n",
      "train loss:0.031561852743709505\n",
      "train loss:0.015516418871421044\n",
      "train loss:0.008160607563437415\n",
      "train loss:0.006219973712014101\n",
      "train loss:0.004837738445681998\n",
      "train loss:0.012474105159281872\n",
      "train loss:0.02708484039032325\n",
      "train loss:0.03489152080833632\n",
      "train loss:0.01587119548892938\n",
      "train loss:0.004126140329754637\n",
      "train loss:0.00963503482275512\n",
      "train loss:0.008050271603428378\n",
      "train loss:0.011024804462037059\n",
      "train loss:0.027856455824310632\n",
      "train loss:0.010051940358035538\n",
      "train loss:0.05619880628969974\n",
      "train loss:0.0074484265253764274\n",
      "train loss:0.06820728200917196\n",
      "train loss:0.01671583801000512\n",
      "train loss:0.008186895481975438\n",
      "train loss:0.0051282376526200655\n",
      "train loss:0.001433753935629437\n",
      "train loss:0.009179272978773339\n",
      "train loss:0.016323539275316067\n",
      "train loss:0.011283786353332303\n",
      "train loss:0.0033226047979781925\n",
      "train loss:0.00978704047938115\n",
      "train loss:0.03268066708643618\n",
      "train loss:0.032475294514165205\n",
      "train loss:0.020535603508565696\n",
      "train loss:0.006974147920149985\n",
      "train loss:0.005924595345945074\n",
      "train loss:0.03348997951532946\n",
      "train loss:0.013734448746220373\n",
      "train loss:0.11441526883365194\n",
      "train loss:0.006676330778792389\n",
      "train loss:0.03843779229835143\n",
      "train loss:0.0193540976097205\n",
      "train loss:0.0083967352597221\n",
      "train loss:0.012604769731741426\n",
      "train loss:0.028136035351730856\n",
      "train loss:0.03674947620948184\n",
      "train loss:0.009668330950765629\n",
      "train loss:0.10257642374080814\n",
      "train loss:0.08355927724513959\n",
      "train loss:0.0040275337131764725\n",
      "train loss:0.011367990447235665\n",
      "train loss:0.017908832354552005\n",
      "train loss:0.019804051664700756\n",
      "train loss:0.053336838193185916\n",
      "train loss:0.0152495356260659\n",
      "train loss:0.003799434298325516\n",
      "train loss:0.10927738042634738\n",
      "train loss:0.054656358610107515\n",
      "train loss:0.030126623426685355\n",
      "train loss:0.033426050087346476\n",
      "train loss:0.006438579929921246\n",
      "train loss:0.038708610676974266\n",
      "train loss:0.03175174644380327\n",
      "train loss:0.01194315266512188\n",
      "train loss:0.0026906121623221234\n",
      "train loss:0.017798007893423156\n",
      "train loss:0.0028650335284451395\n",
      "train loss:0.018903919964943963\n",
      "train loss:0.0378401573261427\n",
      "train loss:0.04609920819395964\n",
      "train loss:0.014722825255977352\n",
      "train loss:0.012039355138376048\n",
      "train loss:0.0031410605293953793\n",
      "train loss:0.05519412648110916\n",
      "train loss:0.004850965657796383\n",
      "train loss:0.023770969359532792\n",
      "train loss:0.00492141088602768\n",
      "train loss:0.02524122337133612\n",
      "train loss:0.0414158184245776\n",
      "train loss:0.05540861328998413\n",
      "train loss:0.007511475282300731\n",
      "train loss:0.008874422749232131\n",
      "train loss:0.005270838168613678\n",
      "train loss:0.014464507788517529\n",
      "train loss:0.010156684636678926\n",
      "train loss:0.020642874637863885\n",
      "train loss:0.041582281875791466\n",
      "train loss:0.036376816198691764\n",
      "train loss:0.013678195935146707\n",
      "train loss:0.021265231991531866\n",
      "train loss:0.011859225685064539\n",
      "train loss:0.02058857531559707\n",
      "train loss:0.026159882179677876\n",
      "train loss:0.017227119857660466\n",
      "train loss:0.04676244072412987\n",
      "train loss:0.008158522337765815\n",
      "train loss:0.006568289245483045\n",
      "train loss:0.05664633459785804\n",
      "train loss:0.009747617909198182\n",
      "train loss:0.007327203212222851\n",
      "train loss:0.025685138640741356\n",
      "train loss:0.006897887815382988\n",
      "train loss:0.00448568571198721\n",
      "train loss:0.05678369396883626\n",
      "train loss:0.03443113290869724\n",
      "train loss:0.00415557957514274\n",
      "train loss:0.005995033443663164\n",
      "train loss:0.023806225550247424\n",
      "train loss:0.009019129960806345\n",
      "train loss:0.014400827051220515\n",
      "train loss:0.014246370767815972\n",
      "train loss:0.008216655340187883\n",
      "train loss:0.017459752002196592\n",
      "train loss:0.04478601100481004\n",
      "train loss:0.01139027387961963\n",
      "train loss:0.01873883639824018\n",
      "train loss:0.015964578929055628\n",
      "train loss:0.011680319470942778\n",
      "train loss:0.005542554421513896\n",
      "train loss:0.011576521670005103\n",
      "train loss:0.015733577284630512\n",
      "train loss:0.04002392702191246\n",
      "train loss:0.012905665927961378\n",
      "train loss:0.02339830448468376\n",
      "train loss:0.01836486003101108\n",
      "train loss:0.02416238262518728\n",
      "train loss:0.05823821018677297\n",
      "train loss:0.008125065665521175\n",
      "train loss:0.03667325778198022\n",
      "train loss:0.040927763665674605\n",
      "train loss:0.018613351646845872\n",
      "train loss:0.01857557905277376\n",
      "train loss:0.0028093892192288983\n",
      "train loss:0.04871249550991516\n",
      "train loss:0.013372412576322663\n",
      "train loss:0.04038716654365306\n",
      "train loss:0.013362840658543661\n",
      "train loss:0.012688273134666866\n",
      "train loss:0.10118301129152864\n",
      "train loss:0.0909085585906158\n",
      "train loss:0.015153443679038578\n",
      "train loss:0.0029659699250617864\n",
      "train loss:0.08927939280419041\n",
      "train loss:0.00909967842651473\n",
      "train loss:0.024020119804115864\n",
      "train loss:0.021712444977481118\n",
      "train loss:0.020391093444466987\n",
      "train loss:0.020126604113358905\n",
      "train loss:0.012832092481430473\n",
      "train loss:0.04573069312616474\n",
      "train loss:0.03542647315842998\n",
      "train loss:0.023117312708493626\n",
      "train loss:0.014836419462220871\n",
      "train loss:0.01229409349767423\n",
      "train loss:0.006517558758724955\n",
      "train loss:0.009931659618105541\n",
      "train loss:0.019135987891205998\n",
      "train loss:0.05937369862507504\n",
      "train loss:0.020435317267626627\n",
      "train loss:0.03444342629373865\n",
      "train loss:0.022766222387436225\n",
      "train loss:0.008467029299094608\n",
      "train loss:0.03189443929273771\n",
      "train loss:0.00956743812394974\n",
      "train loss:0.006047087405411545\n",
      "train loss:0.016907943602957224\n",
      "train loss:0.040008067714128986\n",
      "train loss:0.02497268450804296\n",
      "train loss:0.014921466472825738\n",
      "train loss:0.02172887299196172\n",
      "train loss:0.01843347707858127\n",
      "train loss:0.015506065191281641\n",
      "train loss:0.013819164430603962\n",
      "train loss:0.007913878845472844\n",
      "train loss:0.012246253462161117\n",
      "train loss:0.012396136089216237\n",
      "train loss:0.031133866862804966\n",
      "train loss:0.02417693573954648\n",
      "train loss:0.017399390261187943\n",
      "train loss:0.013321609503635723\n",
      "train loss:0.025022233129082593\n",
      "train loss:0.011424002679884368\n",
      "train loss:0.018970024073650487\n",
      "train loss:0.027725677434621305\n",
      "train loss:0.055131045068920005\n",
      "train loss:0.005032628288985144\n",
      "train loss:0.006518351770587083\n",
      "train loss:0.02411143271709383\n",
      "train loss:0.025912395340264554\n",
      "train loss:0.005709229355561889\n",
      "train loss:0.04236452433477236\n",
      "train loss:0.021650922723029695\n",
      "train loss:0.07160435724532029\n",
      "train loss:0.0258083034386231\n",
      "train loss:0.0031465078672382324\n",
      "train loss:0.019063202593963994\n",
      "train loss:0.004157302456229454\n",
      "train loss:0.04211575938458921\n",
      "train loss:0.00324812270672483\n",
      "train loss:0.012577566853328141\n",
      "train loss:0.006847273975969407\n",
      "train loss:0.005202555569338937\n",
      "train loss:0.00345111561219657\n",
      "train loss:0.001039600006979449\n",
      "train loss:0.015627858762221898\n",
      "train loss:0.010519734409971013\n",
      "train loss:0.02782760067975267\n",
      "train loss:0.0037355438085016303\n",
      "train loss:0.004221434685835095\n",
      "train loss:0.01073052535684352\n",
      "train loss:0.007494712235812555\n",
      "train loss:0.014459697125585565\n",
      "train loss:0.040701234041117244\n",
      "train loss:0.04654480288397123\n",
      "train loss:0.012195679583453465\n",
      "train loss:0.0949474516524089\n",
      "train loss:0.037924216167111675\n",
      "train loss:0.00791831026533314\n",
      "train loss:0.07145519553431785\n",
      "train loss:0.010025047691991392\n",
      "train loss:0.015333024455361676\n",
      "train loss:0.05024125367307398\n",
      "train loss:0.00601266047642365\n",
      "train loss:0.0029307985028366513\n",
      "train loss:0.004510462201574875\n",
      "train loss:0.017988118243486794\n",
      "train loss:0.03473454293390985\n",
      "train loss:0.005494647231928332\n",
      "train loss:0.0180182751349439\n",
      "train loss:0.028623772740440083\n",
      "train loss:0.06930903998620716\n",
      "train loss:0.027474875033387202\n",
      "train loss:0.12522516645737014\n",
      "train loss:0.0031733276395660786\n",
      "train loss:0.005330075008937457\n",
      "train loss:0.026380812990065977\n",
      "train loss:0.010679124705469712\n",
      "train loss:0.014403156794562418\n",
      "train loss:0.007485067010150009\n",
      "train loss:0.017645130828302557\n",
      "train loss:0.04793670790519469\n",
      "train loss:0.0059486260453017435\n",
      "train loss:0.0033409422352009897\n",
      "train loss:0.009238216357031887\n",
      "train loss:0.00852207990098964\n",
      "train loss:0.013562653191871023\n",
      "train loss:0.011505070120494254\n",
      "train loss:0.009504405190721127\n",
      "train loss:0.03214100529500251\n",
      "train loss:0.007932893142716292\n",
      "train loss:0.022058693639380997\n",
      "train loss:0.022139782544633536\n",
      "train loss:0.08387874847555297\n",
      "train loss:0.011820451493652075\n",
      "train loss:0.004799953124018748\n",
      "train loss:0.017656366799971412\n",
      "train loss:0.007064059547929039\n",
      "train loss:0.012249854635429736\n",
      "train loss:0.0027056313860324686\n",
      "train loss:0.021057354762026727\n",
      "train loss:0.023452656030202867\n",
      "train loss:0.023458023121809585\n",
      "train loss:0.042192548660427646\n",
      "train loss:0.002239192576257541\n",
      "train loss:0.012501326472213036\n",
      "train loss:0.015143902231638313\n",
      "train loss:0.0033177900366792145\n",
      "train loss:0.008515208988511222\n",
      "train loss:0.022919132090438627\n",
      "train loss:0.018770162759882875\n",
      "train loss:0.016540244049579076\n",
      "train loss:0.014140411626501396\n",
      "train loss:0.020988546054883993\n",
      "train loss:0.005308095014286673\n",
      "train loss:0.032048341887410126\n",
      "train loss:0.009325395998526734\n",
      "train loss:0.09639076147768937\n",
      "train loss:0.005151815558870848\n",
      "train loss:0.006432397288278194\n",
      "train loss:0.010659318117629466\n",
      "train loss:0.014087587022671655\n",
      "train loss:0.035628191969002986\n",
      "train loss:0.021064806695814516\n",
      "train loss:0.028960758377463734\n",
      "train loss:0.007813444172310812\n",
      "train loss:0.009317236658201661\n",
      "train loss:0.00506514699752729\n",
      "train loss:0.010681652606608649\n",
      "train loss:0.03593554808228046\n",
      "train loss:0.007089680690459333\n",
      "train loss:0.003202089416146085\n",
      "train loss:0.03098840754102573\n",
      "train loss:0.01354635436115494\n",
      "train loss:0.003853671197374944\n",
      "train loss:0.023140716378060987\n",
      "train loss:0.012934772257248766\n",
      "train loss:0.00914983537143284\n",
      "train loss:0.0049883281632782\n",
      "train loss:0.014075535412861813\n",
      "train loss:0.018796075959114426\n",
      "train loss:0.004715662874338517\n",
      "train loss:0.004432375196595043\n",
      "train loss:0.01287304807225719\n",
      "train loss:0.006055861048412689\n",
      "train loss:0.01104078888939071\n",
      "train loss:0.006362552498012423\n",
      "train loss:0.026681951176396598\n",
      "train loss:0.07973091972532545\n",
      "train loss:0.049719484016235625\n",
      "train loss:0.0285014459952368\n",
      "train loss:0.004626301040593472\n",
      "train loss:0.002712279397490414\n",
      "train loss:0.043890230669513315\n",
      "train loss:0.0185042750261116\n",
      "train loss:0.009983433610604147\n",
      "train loss:0.0020665662283328714\n",
      "train loss:0.02793682891568361\n",
      "train loss:0.0096358930434157\n",
      "train loss:0.02694631762759051\n",
      "train loss:0.019274357404183432\n",
      "train loss:0.005250816431833666\n",
      "train loss:0.01053937575639529\n",
      "train loss:0.013530137852449749\n",
      "train loss:0.01252451786072089\n",
      "train loss:0.008648374156497466\n",
      "train loss:0.03153146378957055\n",
      "train loss:0.0854578947803089\n",
      "train loss:0.02648508768773734\n",
      "train loss:0.020704220424265337\n",
      "train loss:0.0118315801179813\n",
      "train loss:0.010500125391772381\n",
      "train loss:0.015906385023582728\n",
      "train loss:0.002740620675998042\n",
      "train loss:0.006208643746113704\n",
      "train loss:0.03654211509361918\n",
      "train loss:0.007763247906588976\n",
      "train loss:0.007237140022240277\n",
      "train loss:0.014410600337188589\n",
      "train loss:0.03255859509770982\n",
      "train loss:0.06042031502909683\n",
      "train loss:0.010761234549034432\n",
      "train loss:0.018571569275680915\n",
      "train loss:0.02083528529086287\n",
      "train loss:0.03408567777610993\n",
      "train loss:0.021414482839622617\n",
      "train loss:0.01917302437380636\n",
      "train loss:0.011989327202542845\n",
      "train loss:0.005492015070352753\n",
      "train loss:0.012132503162156343\n",
      "train loss:0.01632709229972012\n",
      "train loss:0.00215477462722845\n",
      "train loss:0.003200255533115626\n",
      "train loss:0.009567755920754374\n",
      "train loss:0.030554192461121014\n",
      "train loss:0.027082300430235268\n",
      "train loss:0.013429662420828287\n",
      "train loss:0.014770592837949893\n",
      "train loss:0.05768813239816689\n",
      "train loss:0.00362368131611826\n",
      "train loss:0.08144695015133911\n",
      "train loss:0.014457086423540304\n",
      "train loss:0.019042062590821945\n",
      "train loss:0.009214786839363246\n",
      "train loss:0.011951710023889379\n",
      "train loss:0.01612578458519265\n",
      "train loss:0.006437549573653879\n",
      "train loss:0.006685248070543931\n",
      "train loss:0.005462967544819586\n",
      "train loss:0.051272265459410146\n",
      "train loss:0.033076627597783674\n",
      "train loss:0.009986730090312427\n",
      "train loss:0.005535726177146292\n",
      "train loss:0.030819128872739906\n",
      "train loss:0.041032238096393545\n",
      "train loss:0.01634598105586971\n",
      "train loss:0.024299528152179062\n",
      "train loss:0.013498125054678744\n",
      "train loss:0.013060904711902832\n",
      "train loss:0.01608750337490703\n",
      "train loss:0.003950973670504554\n",
      "train loss:0.005783035674435615\n",
      "train loss:0.004559697383796253\n",
      "train loss:0.01084701112754582\n",
      "train loss:0.041745365299509285\n",
      "train loss:0.02523880174674114\n",
      "train loss:0.01760565314474557\n",
      "train loss:0.0070229457367159175\n",
      "train loss:0.024058813726363294\n",
      "train loss:0.010687958729622644\n",
      "train loss:0.012198270705061465\n",
      "train loss:0.010309630833304839\n",
      "train loss:0.007060272552567537\n",
      "train loss:0.013741829880676602\n",
      "train loss:0.006093105960565226\n",
      "train loss:0.018911570342182774\n",
      "train loss:0.006375238542525989\n",
      "train loss:0.015316151606976776\n",
      "train loss:0.015608756728340067\n",
      "train loss:0.003777633610588435\n",
      "current iter num:  3600\n",
      "=== epoch:7, train acc:0.99, test acc:0.978 ===\n",
      "train loss:0.012336002890519625\n",
      "train loss:0.007577250268504586\n",
      "train loss:0.02044619107489938\n",
      "train loss:0.010534232212388764\n",
      "train loss:0.014280442159728629\n",
      "train loss:0.007413157565410524\n",
      "train loss:0.04797342489038216\n",
      "train loss:0.015997541640943528\n",
      "train loss:0.006926780397229897\n",
      "train loss:0.007267264427307613\n",
      "train loss:0.011363992037250432\n",
      "train loss:0.023731136691716094\n",
      "train loss:0.040173509546312214\n",
      "train loss:0.012671328158360211\n",
      "train loss:0.0092066367151753\n",
      "train loss:0.009126808962130289\n",
      "train loss:0.003759407745052084\n",
      "train loss:0.020206599150358762\n",
      "train loss:0.019842089579100324\n",
      "train loss:0.023065402042704134\n",
      "train loss:0.02558401596331865\n",
      "train loss:0.023497267085740882\n",
      "train loss:0.022925690665345718\n",
      "train loss:0.020867145388792575\n",
      "train loss:0.02102356651311152\n",
      "train loss:0.004016398341989493\n",
      "train loss:0.04407204963830219\n",
      "train loss:0.001498204361185147\n",
      "train loss:0.06198298721689892\n",
      "train loss:0.02601897320951969\n",
      "train loss:0.0055783074726580275\n",
      "train loss:0.019345616930918894\n",
      "train loss:0.007120976514599368\n",
      "train loss:0.045754084610839234\n",
      "train loss:0.006788751037543958\n",
      "train loss:0.01622617801593335\n",
      "train loss:0.019907217800300036\n",
      "train loss:0.022174546066265538\n",
      "train loss:0.0028666417829331698\n",
      "train loss:0.010314573787092291\n",
      "train loss:0.01559510373764684\n",
      "train loss:0.036044198565587264\n",
      "train loss:0.013473480419062482\n",
      "train loss:0.01413874847092764\n",
      "train loss:0.009791306584231975\n",
      "train loss:0.024320279862671795\n",
      "train loss:0.0034101655074158885\n",
      "train loss:0.01502180876559105\n",
      "train loss:0.005563470233922439\n",
      "train loss:0.010166670083940505\n",
      "train loss:0.03871064125133395\n",
      "train loss:0.008127496207788769\n",
      "train loss:0.019951903375651673\n",
      "train loss:0.01701832987909794\n",
      "train loss:0.030184138018262013\n",
      "train loss:0.015283636164456266\n",
      "train loss:0.018191867966580778\n",
      "train loss:0.009656802234725962\n",
      "train loss:0.01730775908366298\n",
      "train loss:0.03168974415606812\n",
      "train loss:0.011964419739252087\n",
      "train loss:0.01033531852282564\n",
      "train loss:0.018019236516074778\n",
      "train loss:0.007314427561089942\n",
      "train loss:0.013028207040514701\n",
      "train loss:0.01889398695205807\n",
      "train loss:0.01741924453489182\n",
      "train loss:0.025126819658611427\n",
      "train loss:0.03789459654522016\n",
      "train loss:0.0015658119972458628\n",
      "train loss:0.00294560357883264\n",
      "train loss:0.026804525106564555\n",
      "train loss:0.010733202002867339\n",
      "train loss:0.0029735081604297807\n",
      "train loss:0.014377662490586368\n",
      "train loss:0.01203418957847539\n",
      "train loss:0.002944434324208376\n",
      "train loss:0.00924598410564406\n",
      "train loss:0.07153622903055122\n",
      "train loss:0.019058495939747188\n",
      "train loss:0.004504293849305526\n",
      "train loss:0.011612196107934191\n",
      "train loss:0.023794709625213212\n",
      "train loss:0.008094024550184785\n",
      "train loss:0.016439118863071103\n",
      "train loss:0.01071770216660252\n",
      "train loss:0.00983536016621042\n",
      "train loss:0.01076529870497956\n",
      "train loss:0.028488178728187544\n",
      "train loss:0.0036266537494942745\n",
      "train loss:0.04725734749179864\n",
      "train loss:0.03286727651003483\n",
      "train loss:0.03238783002202322\n",
      "train loss:0.029550391718150538\n",
      "train loss:0.0264406487267497\n",
      "train loss:0.004429044993327919\n",
      "train loss:0.009181689348891605\n",
      "train loss:0.01713602013706171\n",
      "train loss:0.012853176388738542\n",
      "train loss:0.01024383604989692\n",
      "train loss:0.0036554082694299117\n",
      "train loss:0.0072775036276430645\n",
      "train loss:0.019200782879315315\n",
      "train loss:0.18963018453525182\n",
      "train loss:0.013855406838881338\n",
      "train loss:0.0021145429806006637\n",
      "train loss:0.008244752249491031\n",
      "train loss:0.053231751603019226\n",
      "train loss:0.024266714367087727\n",
      "train loss:0.010595830257501245\n",
      "train loss:0.008245339204910952\n",
      "train loss:0.010565873404254925\n",
      "train loss:0.017506376487228456\n",
      "train loss:0.009591126773593866\n",
      "train loss:0.01373455166860269\n",
      "train loss:0.025692019586869744\n",
      "train loss:0.01301898526390117\n",
      "train loss:0.005547989954682936\n",
      "train loss:0.0678022982677304\n",
      "train loss:0.008089895890680382\n",
      "train loss:0.008254829427790055\n",
      "train loss:0.017961309762587554\n",
      "train loss:0.09539765846226617\n",
      "train loss:0.0028726888190572127\n",
      "train loss:0.014086843679237862\n",
      "train loss:0.010722175573146787\n",
      "train loss:0.013281286141875816\n",
      "train loss:0.011574323112671016\n",
      "train loss:0.005355256136452534\n",
      "train loss:0.06616319561175404\n",
      "train loss:0.022019390244827353\n",
      "train loss:0.007881351561347647\n",
      "train loss:0.04356654647397706\n",
      "train loss:0.011624276191107048\n",
      "train loss:0.05274640406660296\n",
      "train loss:0.017958767244763182\n",
      "train loss:0.013959904104954029\n",
      "train loss:0.01488462992454636\n",
      "train loss:0.003195307680071264\n",
      "train loss:0.017946379879652002\n",
      "train loss:0.0903673142380341\n",
      "train loss:0.06409131395968742\n",
      "train loss:0.005136131740160759\n",
      "train loss:0.08193245982908551\n",
      "train loss:0.0012329908318965026\n",
      "train loss:0.016214091669124357\n",
      "train loss:0.03303030072383528\n",
      "train loss:0.048815628571138435\n",
      "train loss:0.022675703296404684\n",
      "train loss:0.017218052996892913\n",
      "train loss:0.017039863610927474\n",
      "train loss:0.014124485316812686\n",
      "train loss:0.015551340412420423\n",
      "train loss:0.004753203881847308\n",
      "train loss:0.00398271385294343\n",
      "train loss:0.005790572336381205\n",
      "train loss:0.0570843460589562\n",
      "train loss:0.011733029518215778\n",
      "train loss:0.004199409814186319\n",
      "train loss:0.004459972637943433\n",
      "train loss:0.055127882524297546\n",
      "train loss:0.009249116183104308\n",
      "train loss:0.019053245232692632\n",
      "train loss:0.009283249101802398\n",
      "train loss:0.003338221013545063\n",
      "train loss:0.016850874470548522\n",
      "train loss:0.0469663471100778\n",
      "train loss:0.02658900945857724\n",
      "train loss:0.0038732163665535627\n",
      "train loss:0.03840045180786192\n",
      "train loss:0.012690708857045777\n",
      "train loss:0.013007269018177704\n",
      "train loss:0.0078679189048874\n",
      "train loss:0.02396329872277287\n",
      "train loss:0.014065721925022206\n",
      "train loss:0.0020646578705747314\n",
      "train loss:0.021176535174271045\n",
      "train loss:0.007245320133717204\n",
      "train loss:0.039616942560672615\n",
      "train loss:0.025006851666486248\n",
      "train loss:0.0074390319690185855\n",
      "train loss:0.00275861911260688\n",
      "train loss:0.025009631032447396\n",
      "train loss:0.020049319454223835\n",
      "train loss:0.03772320874670901\n",
      "train loss:0.005582218890171594\n",
      "train loss:0.01059830760562013\n",
      "train loss:0.032435403527270076\n",
      "train loss:0.019547560583945984\n",
      "train loss:0.0032219315104863437\n",
      "train loss:0.009987266891788691\n",
      "train loss:0.024883693513757184\n",
      "train loss:0.03378368861529498\n",
      "train loss:0.0013161928320879006\n",
      "train loss:0.008434416399268098\n",
      "train loss:0.009258315772100345\n",
      "train loss:0.010244632645571286\n",
      "train loss:0.008210431659270965\n",
      "train loss:0.05404299290984796\n",
      "train loss:0.003735978779935066\n",
      "train loss:0.00911600606250616\n",
      "train loss:0.02418633174240521\n",
      "train loss:0.03698485065016392\n",
      "train loss:0.0050170741458329835\n",
      "train loss:0.009312040133085148\n",
      "train loss:0.011617497726741975\n",
      "train loss:0.0672392343850114\n",
      "train loss:0.014141722823197957\n",
      "train loss:0.019749693976756334\n",
      "train loss:0.06529528672311968\n",
      "train loss:0.01591338041940331\n",
      "train loss:0.04458479170600747\n",
      "train loss:0.008654987576589852\n",
      "train loss:0.024051354219144536\n",
      "train loss:0.004498279764287504\n",
      "train loss:0.0038387490582219024\n",
      "train loss:0.014816280961788391\n",
      "train loss:0.028594299391173385\n",
      "train loss:0.015826846357778063\n",
      "train loss:0.030557703441804772\n",
      "train loss:0.004067389181896731\n",
      "train loss:0.018837969325997038\n",
      "train loss:0.019851030987331503\n",
      "train loss:0.011741623433943504\n",
      "train loss:0.007272812952958588\n",
      "train loss:0.00974298220759751\n",
      "train loss:0.02195652108654746\n",
      "train loss:0.009464410989967725\n",
      "train loss:0.006951828256745811\n",
      "train loss:0.060359396267397926\n",
      "train loss:0.012807770991708909\n",
      "train loss:0.010210022157948457\n",
      "train loss:0.010427130240942144\n",
      "train loss:0.011479359372115782\n",
      "train loss:0.019813545229367225\n",
      "train loss:0.03286167227789209\n",
      "train loss:0.004770451069880201\n",
      "train loss:0.012480676192738594\n",
      "train loss:0.021451296302539474\n",
      "train loss:0.005618422022309555\n",
      "train loss:0.003547226261967643\n",
      "train loss:0.007954456149576718\n",
      "train loss:0.05626418111898158\n",
      "train loss:0.04150638231588645\n",
      "train loss:0.06343125531064159\n",
      "train loss:0.007373589089414568\n",
      "train loss:0.0022220397738958145\n",
      "train loss:0.008163422826614706\n",
      "train loss:0.013490357777047997\n",
      "train loss:0.007673925862760142\n",
      "train loss:0.02759733094719679\n",
      "train loss:0.014323701551311163\n",
      "train loss:0.02915932680154435\n",
      "train loss:0.009730140645370427\n",
      "train loss:0.0013384388489499884\n",
      "train loss:0.014606043193923801\n",
      "train loss:0.008342610827440247\n",
      "train loss:0.005024181175338467\n",
      "train loss:0.004267603963826302\n",
      "train loss:0.004782790422384987\n",
      "train loss:0.08081493409615689\n",
      "train loss:0.0027751030537619945\n",
      "train loss:0.025636852218972404\n",
      "train loss:0.0279755675906461\n",
      "train loss:0.02026524813361414\n",
      "train loss:0.009477237884092314\n",
      "train loss:0.006230393067457004\n",
      "train loss:0.011195421973667416\n",
      "train loss:0.013984899376691846\n",
      "train loss:0.01564805455975612\n",
      "train loss:0.0064513634000608455\n",
      "train loss:0.0036397198952715904\n",
      "train loss:0.03002188020909036\n",
      "train loss:0.0024651972588204405\n",
      "train loss:0.013171472255247878\n",
      "train loss:0.05588627650744051\n",
      "train loss:0.00732357547763084\n",
      "train loss:0.015213184157123598\n",
      "train loss:0.09851185322094531\n",
      "train loss:0.013810700552353037\n",
      "train loss:0.021402278324936637\n",
      "train loss:0.01324983651287973\n",
      "train loss:0.006454823352246386\n",
      "train loss:0.022680423995039733\n",
      "train loss:0.02412430839986504\n",
      "train loss:0.014791459121902986\n",
      "train loss:0.004631846954562278\n",
      "train loss:0.006576668839016018\n",
      "train loss:0.014432157461262592\n",
      "train loss:0.011751516244322645\n",
      "train loss:0.03804508958908061\n",
      "train loss:0.013008107680076294\n",
      "train loss:0.014285390282711741\n",
      "train loss:0.01856415222268093\n",
      "train loss:0.004861605734265611\n",
      "train loss:0.011055806850817971\n",
      "train loss:0.008765433580868918\n",
      "train loss:0.09457531861326678\n",
      "train loss:0.010054023463115326\n",
      "train loss:0.011545012842269433\n",
      "train loss:0.003750818859628814\n",
      "train loss:0.004860120748096157\n",
      "train loss:0.0414276070611117\n",
      "train loss:0.009177979177558488\n",
      "train loss:0.012732522465436508\n",
      "train loss:0.11686311138589467\n",
      "train loss:0.014511765587714049\n",
      "train loss:0.04548893727438775\n",
      "train loss:0.007613300579236245\n",
      "train loss:0.0055319101989753814\n",
      "train loss:0.004818117836510711\n",
      "train loss:0.007255806495269145\n",
      "train loss:0.024310000303692764\n",
      "train loss:0.0323001508715534\n",
      "train loss:0.009264835982172516\n",
      "train loss:0.0068417297314477986\n",
      "train loss:0.09703009995189114\n",
      "train loss:0.025187011779711613\n",
      "train loss:0.026325030957953754\n",
      "train loss:0.019845420519768186\n",
      "train loss:0.023615017278752582\n",
      "train loss:0.004103598326652306\n",
      "train loss:0.04093953097266343\n",
      "train loss:0.02202773427303417\n",
      "train loss:0.0025103438681646716\n",
      "train loss:0.0076693217840515145\n",
      "train loss:0.009440399702420865\n",
      "train loss:0.017477564283409172\n",
      "train loss:0.009900558634377022\n",
      "train loss:0.011108399513620377\n",
      "train loss:0.030194182306079634\n",
      "train loss:0.01918867874718965\n",
      "train loss:0.0036416640509351838\n",
      "train loss:0.0074001097286430385\n",
      "train loss:0.017126722912122912\n",
      "train loss:0.0038335739752798225\n",
      "train loss:0.016656774055139198\n",
      "train loss:0.011267189535887458\n",
      "train loss:0.004021435946458435\n",
      "train loss:0.06594066251309626\n",
      "train loss:0.011825341565166108\n",
      "train loss:0.021515976773656846\n",
      "train loss:0.011218372271253208\n",
      "train loss:0.003863215833735203\n",
      "train loss:0.004704858036266856\n",
      "train loss:0.002703904443323452\n",
      "train loss:0.00736064731363625\n",
      "train loss:0.05989114768010897\n",
      "train loss:0.0057707768194328116\n",
      "train loss:0.05160463425613337\n",
      "train loss:0.002566140210037073\n",
      "train loss:0.032307894091979435\n",
      "train loss:0.015255201336363426\n",
      "train loss:0.06795019293397833\n",
      "train loss:0.018980041498602516\n",
      "train loss:0.013353433745358858\n",
      "train loss:0.09662773488702184\n",
      "train loss:0.0028901062094774522\n",
      "train loss:0.01910494747006166\n",
      "train loss:0.004134468376535504\n",
      "train loss:0.004387725318197336\n",
      "train loss:0.01258932221763388\n",
      "train loss:0.005119885539932753\n",
      "train loss:0.029058246522365237\n",
      "train loss:0.01853085868570449\n",
      "train loss:0.022083361562802096\n",
      "train loss:0.006995866377568925\n",
      "train loss:0.02135821074019451\n",
      "train loss:0.0032681785445114997\n",
      "train loss:0.004670512327570029\n",
      "train loss:0.014876066331242046\n",
      "train loss:0.009868032456436079\n",
      "train loss:0.025484413743257\n",
      "train loss:0.01161916507286696\n",
      "train loss:0.03328087065558282\n",
      "train loss:0.006801755577997646\n",
      "train loss:0.005557961403143418\n",
      "train loss:0.0063579651430585695\n",
      "train loss:0.009491415015051423\n",
      "train loss:0.01535856405260968\n",
      "train loss:0.006945223567189079\n",
      "train loss:0.012452490728688344\n",
      "train loss:0.012733161082382838\n",
      "train loss:0.01964264808929728\n",
      "train loss:0.022482047578949015\n",
      "train loss:0.005594742235709383\n",
      "train loss:0.00653024186793266\n",
      "train loss:0.007468387633428279\n",
      "train loss:0.0165892163525698\n",
      "train loss:0.00740546896891642\n",
      "train loss:0.016464547471019397\n",
      "train loss:0.018547218555671402\n",
      "train loss:0.01247724410955247\n",
      "train loss:0.015400068985499692\n",
      "train loss:0.013419119784216692\n",
      "train loss:0.01621169700759887\n",
      "train loss:0.02583435423720297\n",
      "train loss:0.016254921709757123\n",
      "train loss:0.0033728474446783124\n",
      "train loss:0.01325129758402354\n",
      "train loss:0.0026757908676044574\n",
      "train loss:0.01290281423061085\n",
      "train loss:0.020606004988294773\n",
      "train loss:0.01471314567139957\n",
      "train loss:0.009282784582283822\n",
      "train loss:0.02582950497136334\n",
      "train loss:0.020059136175907745\n",
      "train loss:0.02745890036137756\n",
      "train loss:0.023277001901840687\n",
      "train loss:0.032632072436533155\n",
      "train loss:0.013317980687815634\n",
      "train loss:0.026496231278016088\n",
      "train loss:0.008548331493402775\n",
      "train loss:0.012062883953352612\n",
      "train loss:0.006829001579113442\n",
      "train loss:0.00984859834289996\n",
      "train loss:0.0035477703903063745\n",
      "train loss:0.046298720266394706\n",
      "train loss:0.00446208055623161\n",
      "train loss:0.016728041487910236\n",
      "train loss:0.008356437950581625\n",
      "train loss:0.02665262922819714\n",
      "train loss:0.007149690276761504\n",
      "train loss:0.002034099667841066\n",
      "train loss:0.008194814843332178\n",
      "train loss:0.007028242299294091\n",
      "train loss:0.0038616477992955436\n",
      "train loss:0.037586949212095905\n",
      "train loss:0.008702648996875548\n",
      "train loss:0.08387557133087041\n",
      "train loss:0.011929168038307864\n",
      "train loss:0.013642744182261326\n",
      "train loss:0.030802305621876344\n",
      "train loss:0.0040161019450213695\n",
      "train loss:0.04412162204693165\n",
      "train loss:0.006360945052277661\n",
      "train loss:0.002427931422767099\n",
      "train loss:0.0018627287225302532\n",
      "train loss:0.016316722337721353\n",
      "train loss:0.053535219253965406\n",
      "train loss:0.002854036965762941\n",
      "train loss:0.05584932640371637\n",
      "train loss:0.013613544504392344\n",
      "train loss:0.005027087266251718\n",
      "train loss:0.00937487995219425\n",
      "train loss:0.004157852332439413\n",
      "train loss:0.009819095202587256\n",
      "train loss:0.015463144225331522\n",
      "train loss:0.017239471975485435\n",
      "train loss:0.03565243822432818\n",
      "train loss:0.0009615603246526543\n",
      "train loss:0.016754412416078574\n",
      "train loss:0.005258317838128159\n",
      "train loss:0.017944495379832698\n",
      "train loss:0.002599972384690086\n",
      "train loss:0.011364974473926587\n",
      "train loss:0.019894442520788445\n",
      "train loss:0.03902663679038089\n",
      "train loss:0.007061056922967235\n",
      "train loss:0.0023242410827499648\n",
      "train loss:0.0023914393517104576\n",
      "train loss:0.00397579225052431\n",
      "train loss:0.0011121069104778963\n",
      "train loss:0.030359800988076856\n",
      "train loss:0.019531875519511178\n",
      "train loss:0.006360712836278659\n",
      "train loss:0.012842553731646717\n",
      "train loss:0.04542137975349931\n",
      "train loss:0.02399404220521359\n",
      "train loss:0.04963981446803998\n",
      "train loss:0.013568891555154505\n",
      "train loss:0.01013091548431656\n",
      "train loss:0.01611531065281481\n",
      "train loss:0.020768061468935124\n",
      "train loss:0.02833977443031019\n",
      "train loss:0.00799468277917691\n",
      "train loss:0.005013216110702532\n",
      "train loss:0.03598082795724854\n",
      "train loss:0.009129955546147865\n",
      "train loss:0.05649090133218383\n",
      "train loss:0.006727309083046809\n",
      "train loss:0.01336829064200513\n",
      "train loss:0.006998959036440378\n",
      "train loss:0.017031134606036658\n",
      "train loss:0.009255028115357554\n",
      "train loss:0.0007070236059551206\n",
      "train loss:0.0015021328303886582\n",
      "train loss:0.0051739068510808115\n",
      "train loss:0.01473144732976351\n",
      "train loss:0.017879060583607952\n",
      "train loss:0.014848662144592769\n",
      "train loss:0.01723446603770693\n",
      "train loss:0.0012058280820699064\n",
      "train loss:0.019608723995969638\n",
      "train loss:0.008975443003442586\n",
      "train loss:0.006147442113780206\n",
      "train loss:0.005075015127878815\n",
      "train loss:0.008557590731879986\n",
      "train loss:0.020534617151641458\n",
      "train loss:0.06014006649486683\n",
      "train loss:0.004446333776851769\n",
      "train loss:0.008562882594019808\n",
      "train loss:0.004177656890940011\n",
      "train loss:0.004030702734833329\n",
      "train loss:0.008545470392356725\n",
      "train loss:0.005561119478653403\n",
      "train loss:0.005625538972921718\n",
      "train loss:0.013386157838761101\n",
      "train loss:0.02988621502724441\n",
      "train loss:0.014618412133726442\n",
      "train loss:0.06622453290966505\n",
      "train loss:0.006544539345740651\n",
      "train loss:0.007042310311892703\n",
      "train loss:0.006245937000578456\n",
      "train loss:0.015894092703745793\n",
      "train loss:0.0044292524656902275\n",
      "train loss:0.029212147958755433\n",
      "train loss:0.007172261331093531\n",
      "train loss:0.0014205099689906598\n",
      "train loss:0.014524120479207097\n",
      "train loss:0.014713279828268288\n",
      "train loss:0.0008846299066749894\n",
      "train loss:0.05413906671030721\n",
      "train loss:0.00812276900014018\n",
      "train loss:0.010743389483428718\n",
      "train loss:0.0014937069275054234\n",
      "train loss:0.0017695616632700676\n",
      "train loss:0.007918948777217024\n",
      "train loss:0.007414268607894129\n",
      "train loss:0.009924503589269305\n",
      "train loss:0.005479511721172271\n",
      "train loss:0.03131741541377688\n",
      "train loss:0.012939339825632326\n",
      "train loss:0.005048697260680773\n",
      "train loss:0.01936559625789786\n",
      "train loss:0.0038349416965109784\n",
      "train loss:0.06106730384348353\n",
      "train loss:0.019799310471014873\n",
      "train loss:0.05935835389432313\n",
      "train loss:0.007476767390736963\n",
      "train loss:0.0013406562591219827\n",
      "train loss:0.006612694344719144\n",
      "train loss:0.06562172860412954\n",
      "train loss:0.030054294257429417\n",
      "train loss:0.0034772771337851166\n",
      "train loss:0.004211684447077398\n",
      "train loss:0.01462532476774232\n",
      "train loss:0.004683949346410519\n",
      "train loss:0.0036010403640128865\n",
      "train loss:0.15518255643194626\n",
      "train loss:0.010454713077916703\n",
      "train loss:0.001727414001872151\n",
      "train loss:0.01380014876294034\n",
      "train loss:0.014900138305607068\n",
      "train loss:0.033697938320315035\n",
      "train loss:0.010338126811425234\n",
      "train loss:0.013257726634428122\n",
      "train loss:0.007934486731911376\n",
      "train loss:0.047059337552254804\n",
      "train loss:0.006852394610867895\n",
      "train loss:0.055731123305676464\n",
      "train loss:0.016698576017113973\n",
      "train loss:0.020855864596626885\n",
      "train loss:0.004348296316419671\n",
      "train loss:0.010046928754616016\n",
      "train loss:0.005294557377390912\n",
      "train loss:0.0142288132168404\n",
      "train loss:0.012359093772034209\n",
      "train loss:0.017903744526806625\n",
      "train loss:0.01508036646987212\n",
      "train loss:0.013194250397383698\n",
      "train loss:0.010525831442762677\n",
      "train loss:0.027838962660420085\n",
      "train loss:0.0036745486233410347\n",
      "train loss:0.005855701117307373\n",
      "train loss:0.008358682283187388\n",
      "train loss:0.005880017519070736\n",
      "train loss:0.02868547241142491\n",
      "train loss:0.015273043075866109\n",
      "train loss:0.01022441171678002\n",
      "train loss:0.0018704780257812897\n",
      "train loss:0.011740344401957457\n",
      "train loss:0.03335924145651842\n",
      "train loss:0.03751003375049425\n",
      "train loss:0.013146339914469935\n",
      "train loss:0.008612890709487704\n",
      "train loss:0.004644227854429181\n",
      "train loss:0.009227365460967696\n",
      "train loss:0.006836823162121637\n",
      "train loss:0.021664540211576634\n",
      "train loss:0.023619053452841998\n",
      "train loss:0.007645718650845896\n",
      "train loss:0.001969217858413659\n",
      "train loss:0.012648651097346031\n",
      "train loss:0.0469299242503958\n",
      "train loss:0.024663504054597055\n",
      "train loss:0.0019306692069384263\n",
      "train loss:0.0023970429189110716\n",
      "train loss:0.004760741001401832\n",
      "train loss:0.012832798212669782\n",
      "current iter num:  4200\n",
      "=== epoch:8, train acc:0.992, test acc:0.985 ===\n",
      "train loss:0.027830831495410373\n",
      "train loss:0.013482892767746833\n",
      "train loss:0.010555474053858024\n",
      "train loss:0.012547838498044582\n",
      "train loss:0.01932676109977299\n",
      "train loss:0.006467486131213044\n",
      "train loss:0.02126246455284161\n",
      "train loss:0.025752731705927227\n",
      "train loss:0.041413509265962656\n",
      "train loss:0.009736208105982197\n",
      "train loss:0.003415421293850396\n",
      "train loss:0.008299099761385688\n",
      "train loss:0.0032988964821165628\n",
      "train loss:0.009143400772428496\n",
      "train loss:0.008514187473924293\n",
      "train loss:0.032201791778130734\n",
      "train loss:0.05494990013466236\n",
      "train loss:0.003240790755230467\n",
      "train loss:0.03945512216898689\n",
      "train loss:0.0037586679541660927\n",
      "train loss:0.011017362964456214\n",
      "train loss:0.009177520847353458\n",
      "train loss:0.016507543247178187\n",
      "train loss:0.010980472990175343\n",
      "train loss:0.005583022764885659\n",
      "train loss:0.0038521328028917044\n",
      "train loss:0.00723057366303109\n",
      "train loss:0.0004967148368559171\n",
      "train loss:0.002617445878774962\n",
      "train loss:0.0193271203974514\n",
      "train loss:0.015304884444330489\n",
      "train loss:0.05912018964475409\n",
      "train loss:0.01008284324723199\n",
      "train loss:0.0014382162012042596\n",
      "train loss:0.002902160615209475\n",
      "train loss:0.0008554137484892985\n",
      "train loss:0.017432699886627077\n",
      "train loss:0.004746897076100276\n",
      "train loss:0.033051521814317725\n",
      "train loss:0.01913181060457758\n",
      "train loss:0.004519506855358226\n",
      "train loss:0.022332078056277255\n",
      "train loss:0.01258711892737632\n",
      "train loss:0.002047831891933213\n",
      "train loss:0.004332322259986339\n",
      "train loss:0.017938801090195668\n",
      "train loss:0.0075904913172688115\n",
      "train loss:0.007366447709627636\n",
      "train loss:0.07524370646747465\n",
      "train loss:0.0948895833434031\n",
      "train loss:0.012292960850668722\n",
      "train loss:0.027812218582540415\n",
      "train loss:0.0687551484075572\n",
      "train loss:0.01344059035369103\n",
      "train loss:0.01347390448865211\n",
      "train loss:0.022731329138582345\n",
      "train loss:0.007189317356570357\n",
      "train loss:0.013520772685633613\n",
      "train loss:0.005499757802790232\n",
      "train loss:0.029931402574958684\n",
      "train loss:0.005471704421104601\n",
      "train loss:0.005059884087516944\n",
      "train loss:0.05591762202098963\n",
      "train loss:0.01857776113460242\n",
      "train loss:0.0036024306892270003\n",
      "train loss:0.002222968947306005\n",
      "train loss:0.0036165633461610084\n",
      "train loss:0.01116357386110794\n",
      "train loss:0.007093705142793427\n",
      "train loss:0.004375341201176447\n",
      "train loss:0.005705064399616711\n",
      "train loss:0.0026357716404128493\n",
      "train loss:0.006995115385715029\n",
      "train loss:0.06974722169145359\n",
      "train loss:0.007700747326533908\n",
      "train loss:0.010178308429069603\n",
      "train loss:0.004896832153021821\n",
      "train loss:0.01695080271571557\n",
      "train loss:0.002734973269835224\n",
      "train loss:0.002019935007690924\n",
      "train loss:0.015318442951599407\n",
      "train loss:0.012317662912155174\n",
      "train loss:0.020787158955516305\n",
      "train loss:0.023571746125426726\n",
      "train loss:0.0017042348290475654\n",
      "train loss:0.028850519534955144\n",
      "train loss:0.007988284425563055\n",
      "train loss:0.016048748529406996\n",
      "train loss:0.005795613301182142\n",
      "train loss:0.016405283928047163\n",
      "train loss:0.018352929160072452\n",
      "train loss:0.0019032904572971324\n",
      "train loss:0.010195864482806391\n",
      "train loss:0.01473115989638605\n",
      "train loss:0.012313877057332268\n",
      "train loss:0.017276141612290765\n",
      "train loss:0.004421513574867346\n",
      "train loss:0.02160943901444699\n",
      "train loss:0.01047458927476982\n",
      "train loss:0.007291175347776242\n",
      "train loss:0.0256608859005415\n",
      "train loss:0.0076393146024949145\n",
      "train loss:0.005449246637095743\n",
      "train loss:0.0009427422897234331\n",
      "train loss:0.011393754117116305\n",
      "train loss:0.009509943412008925\n",
      "train loss:0.004600964403834976\n",
      "train loss:0.009195983243770318\n",
      "train loss:0.005091125095185124\n",
      "train loss:0.01305510666440584\n",
      "train loss:0.011374368671249437\n",
      "train loss:0.011406021147325371\n",
      "train loss:0.055913587659616015\n",
      "train loss:0.04803766174205073\n",
      "train loss:0.004560433744730635\n",
      "train loss:0.007438108538675325\n",
      "train loss:0.003075480290374147\n",
      "train loss:0.05374265897100227\n",
      "train loss:0.027715084765164523\n",
      "train loss:0.0011031679278090576\n",
      "train loss:0.007781984181382944\n",
      "train loss:0.001984127637558\n",
      "train loss:0.0028119584831295522\n",
      "train loss:0.005984909297989762\n",
      "train loss:0.005003253242139023\n",
      "train loss:0.0015595697381303832\n",
      "train loss:0.005234195347368833\n",
      "train loss:0.002140244395591972\n",
      "train loss:0.04386761451664215\n",
      "train loss:0.013612386943800117\n",
      "train loss:0.005058387760982209\n",
      "train loss:0.002597360935665026\n",
      "train loss:0.004148634659990701\n",
      "train loss:0.0012670819092505856\n",
      "train loss:0.0072777747156939025\n",
      "train loss:0.008008995997767554\n",
      "train loss:0.007777696387217374\n",
      "train loss:0.009098892012787999\n",
      "train loss:0.0014536864916286033\n",
      "train loss:0.00082568933758456\n",
      "train loss:0.003994154270061906\n",
      "train loss:0.006356417251140989\n",
      "train loss:0.005638028259712555\n",
      "train loss:0.0016659167349753123\n",
      "train loss:0.014935053383544773\n",
      "train loss:0.007118731585138775\n",
      "train loss:0.0034893336504228712\n",
      "train loss:0.04605385164309986\n",
      "train loss:0.020360612816782492\n",
      "train loss:0.011595502889693092\n",
      "train loss:0.006761716042305669\n",
      "train loss:0.0030214647297328325\n",
      "train loss:0.0017086496612207167\n",
      "train loss:0.006076666681156679\n",
      "train loss:0.03597231764353303\n",
      "train loss:0.0159635529078558\n",
      "train loss:0.006378519619042284\n",
      "train loss:0.0581618311644991\n",
      "train loss:0.007953997685595885\n",
      "train loss:0.010623858183343715\n",
      "train loss:0.008242638096764905\n",
      "train loss:0.003602046739730188\n",
      "train loss:0.013213003328410744\n",
      "train loss:0.014986830786300222\n",
      "train loss:0.006441841155045213\n",
      "train loss:0.014184654507547164\n",
      "train loss:0.00935122046079107\n",
      "train loss:0.049519787662634256\n",
      "train loss:0.007832871344041387\n",
      "train loss:0.005734097788006165\n",
      "train loss:0.008489198758247158\n",
      "train loss:0.0028658673180545885\n",
      "train loss:0.0034878623458531165\n",
      "train loss:0.016914158632788966\n",
      "train loss:0.0073630352163686\n",
      "train loss:0.0021445351519485353\n",
      "train loss:0.020210513427720768\n",
      "train loss:0.007370584032945956\n",
      "train loss:0.005091309716055752\n",
      "train loss:0.041053613820031074\n",
      "train loss:0.004507509566792494\n",
      "train loss:0.017459098259915297\n",
      "train loss:0.02039410032217439\n",
      "train loss:0.027895739559638463\n",
      "train loss:0.0016599338096000196\n",
      "train loss:0.02742326445011086\n",
      "train loss:0.004570541521918098\n",
      "train loss:0.007526960792141495\n",
      "train loss:0.08867611761375352\n",
      "train loss:0.014240909762689736\n",
      "train loss:0.0022329266717296993\n",
      "train loss:0.01474087616172528\n",
      "train loss:0.010125888153148634\n",
      "train loss:0.003959703808185635\n",
      "train loss:0.005607832040550454\n",
      "train loss:0.02056668345652278\n",
      "train loss:0.06343487714752773\n",
      "train loss:0.0007609131479738197\n",
      "train loss:0.005472785246484909\n",
      "train loss:0.009594125543991631\n",
      "train loss:0.01924948441705846\n",
      "train loss:0.003397458418162877\n",
      "train loss:0.008764972183283046\n",
      "train loss:0.012824327255679885\n",
      "train loss:0.020655444618002287\n",
      "train loss:0.008854619735044532\n",
      "train loss:0.042105418279524756\n",
      "train loss:0.01722664178249644\n",
      "train loss:0.009554503923952145\n",
      "train loss:0.015747624178971166\n",
      "train loss:0.01631507606227472\n",
      "train loss:0.008714347264220627\n",
      "train loss:0.005948745575821003\n",
      "train loss:0.05316537044798536\n",
      "train loss:0.012439798977394669\n",
      "train loss:0.0825937581066026\n",
      "train loss:0.05008294077314075\n",
      "train loss:0.038551939139262095\n",
      "train loss:0.03246031694614584\n",
      "train loss:0.010593614874852386\n",
      "train loss:0.006789447340544016\n",
      "train loss:0.005681949725174766\n",
      "train loss:0.0103366388017018\n",
      "train loss:0.026910815520689677\n",
      "train loss:0.004002641807466949\n",
      "train loss:0.01758604953623808\n",
      "train loss:0.025268349055520268\n",
      "train loss:0.010293201024063415\n",
      "train loss:0.011698082585026885\n",
      "train loss:0.032058945298687656\n",
      "train loss:0.01994067881965637\n",
      "train loss:0.027096525692722712\n",
      "train loss:0.006200687387703384\n",
      "train loss:0.0025351257054914617\n",
      "train loss:0.004147957157774701\n",
      "train loss:0.019384214079614815\n",
      "train loss:0.010016436687662278\n",
      "train loss:0.004967674718412924\n",
      "train loss:0.007412337560564816\n",
      "train loss:0.016910426415007274\n",
      "train loss:0.043626766245199074\n",
      "train loss:0.0027104438623177184\n",
      "train loss:0.017756309288515692\n",
      "train loss:0.07411163997793103\n",
      "train loss:0.01872742181069434\n",
      "train loss:0.006472832315860438\n",
      "train loss:0.002003390782471419\n",
      "train loss:0.005358662908133091\n",
      "train loss:0.007801191909250736\n",
      "train loss:0.010409828614565826\n",
      "train loss:0.005314376852481576\n",
      "train loss:0.022493843193698834\n",
      "train loss:0.007172688330214861\n",
      "train loss:0.055733180759573966\n",
      "train loss:0.008838577759572865\n",
      "train loss:0.0042008200652674235\n",
      "train loss:0.0008715272968493441\n",
      "train loss:0.012898257592760785\n",
      "train loss:0.017052997532473923\n",
      "train loss:0.005158963354070168\n",
      "train loss:0.004450801375631235\n",
      "train loss:0.07637915041029551\n",
      "train loss:0.02629056754275689\n",
      "train loss:0.004844681008634281\n",
      "train loss:0.05158135113555168\n",
      "train loss:0.010178600593746798\n",
      "train loss:0.01009733402396016\n",
      "train loss:0.006414039405471394\n",
      "train loss:0.00402383722751736\n",
      "train loss:0.008287759876905451\n",
      "train loss:0.02240088324596166\n",
      "train loss:0.008241326853345152\n",
      "train loss:0.00651553111367334\n",
      "train loss:0.004049428666389646\n",
      "train loss:0.01217173690060299\n",
      "train loss:0.013236391513983288\n",
      "train loss:0.003482601659085294\n",
      "train loss:0.0043058408132367115\n",
      "train loss:0.011795151318564782\n",
      "train loss:0.0033575731084422517\n",
      "train loss:0.005906849601426794\n",
      "train loss:0.018550192286403432\n",
      "train loss:0.006960105098200711\n",
      "train loss:0.12152889536395389\n",
      "train loss:0.07018694243273503\n",
      "train loss:0.0062849330926726295\n",
      "train loss:0.016797337212307927\n",
      "train loss:0.005668083098798588\n",
      "train loss:0.0034450726358534055\n",
      "train loss:0.0011299703013679546\n",
      "train loss:0.003004244590704851\n",
      "train loss:0.00734760655442248\n",
      "train loss:0.0027545387635585063\n",
      "train loss:0.006112857545766522\n",
      "train loss:0.022567180571635044\n",
      "train loss:0.00989978190241926\n",
      "train loss:0.011418572402993949\n",
      "train loss:0.011053187532707214\n",
      "train loss:0.029505404284741403\n",
      "train loss:0.009293304746814456\n",
      "train loss:0.054271276686004837\n",
      "train loss:0.02070197778892581\n",
      "train loss:0.016729813573629564\n",
      "train loss:0.004478376035743205\n",
      "train loss:0.008001624928609912\n",
      "train loss:0.018910742076248165\n",
      "train loss:0.050002393773232436\n",
      "train loss:0.010704871967247143\n",
      "train loss:0.012426673673932177\n",
      "train loss:0.0065768120763634265\n",
      "train loss:0.028659958529974664\n",
      "train loss:0.005964271736234856\n",
      "train loss:0.005831281058384624\n",
      "train loss:0.014248233509796475\n",
      "train loss:0.014138418991199372\n",
      "train loss:0.005508876026893893\n",
      "train loss:0.004804855525957375\n",
      "train loss:0.00973317584477991\n",
      "train loss:0.007586031867995412\n",
      "train loss:0.012120211644362368\n",
      "train loss:0.016481894853428384\n",
      "train loss:0.00885841106079544\n",
      "train loss:0.0014396477481507129\n",
      "train loss:0.023312434824704284\n",
      "train loss:0.0169802924451984\n",
      "train loss:0.021850101653918663\n",
      "train loss:0.008116521881281315\n",
      "train loss:0.002260977231194026\n",
      "train loss:0.009112123262188694\n",
      "train loss:0.04112656680468528\n",
      "train loss:0.008685136637717967\n",
      "train loss:0.01814619657914447\n",
      "train loss:0.01352240102801864\n",
      "train loss:0.018507099896195114\n",
      "train loss:0.016521272252898636\n",
      "train loss:0.009126093451423442\n",
      "train loss:0.002329706852314892\n",
      "train loss:0.024726790024280368\n",
      "train loss:0.003965450494943083\n",
      "train loss:0.0221517596904669\n",
      "train loss:0.003181119710249961\n",
      "train loss:0.00799908265062816\n",
      "train loss:0.01846921554242472\n",
      "train loss:0.025394990959138667\n",
      "train loss:0.007975845643382419\n",
      "train loss:0.013948835707447546\n",
      "train loss:0.014148847784693587\n",
      "train loss:0.011358658763555874\n",
      "train loss:0.00787245936901933\n",
      "train loss:0.011204658284965084\n",
      "train loss:0.008252989462424459\n",
      "train loss:0.03430898156614227\n",
      "train loss:0.00836986259234041\n",
      "train loss:0.004293998569334233\n",
      "train loss:0.0021894635383569697\n",
      "train loss:0.040556046012470134\n",
      "train loss:0.06040013736311334\n",
      "train loss:0.035670934499521294\n",
      "train loss:0.004124430361427811\n",
      "train loss:0.012907610546017143\n",
      "train loss:0.001672786719066211\n",
      "train loss:0.0032264234292354588\n",
      "train loss:0.030465591813111925\n",
      "train loss:0.0029344580071952076\n",
      "train loss:0.0683939513092886\n",
      "train loss:0.005519677879837957\n",
      "train loss:0.015808955384849374\n",
      "train loss:0.0017826310681589178\n",
      "train loss:0.017264479734052547\n",
      "train loss:0.01131486100234178\n",
      "train loss:0.04427997908980182\n",
      "train loss:0.011206640529157678\n",
      "train loss:0.0020706658342890664\n",
      "train loss:0.02485148017639199\n",
      "train loss:0.00984745774545066\n",
      "train loss:0.013078801844387578\n",
      "train loss:0.011627939841200504\n",
      "train loss:0.002096595414304223\n",
      "train loss:0.0032085188116200013\n",
      "train loss:0.006851331243040821\n",
      "train loss:0.017064645596604047\n",
      "train loss:0.03399927399044993\n",
      "train loss:0.02215271631961586\n",
      "train loss:0.01744422495802828\n",
      "train loss:0.008718359289826479\n",
      "train loss:0.002772162670862297\n",
      "train loss:0.0062041055809513605\n",
      "train loss:0.04141171184755741\n",
      "train loss:0.003425969735021797\n",
      "train loss:0.009988251460009553\n",
      "train loss:0.005185687320187342\n",
      "train loss:0.003959357808884426\n",
      "train loss:0.04290378030424314\n",
      "train loss:0.012032399676866827\n",
      "train loss:0.009478167590734382\n",
      "train loss:0.00417150552568402\n",
      "train loss:0.02202691139277249\n",
      "train loss:0.003267494451491147\n",
      "train loss:0.0063630258660143536\n",
      "train loss:0.00573104479970142\n",
      "train loss:0.02423973833957135\n",
      "train loss:0.00730876238062423\n",
      "train loss:0.02071550487591296\n",
      "train loss:0.020998183008868597\n",
      "train loss:0.008563595019680466\n",
      "train loss:0.00493821530122139\n",
      "train loss:0.007950598250435547\n",
      "train loss:0.004579978118477334\n",
      "train loss:0.018584549466557715\n",
      "train loss:0.012452325312097135\n",
      "train loss:0.0010728629814708824\n",
      "train loss:0.0015269373725181079\n",
      "train loss:0.04678576744604737\n",
      "train loss:0.03002944743587163\n",
      "train loss:0.005439418357051695\n",
      "train loss:0.0076342440126368994\n",
      "train loss:0.013047054167205975\n",
      "train loss:0.012140762820525553\n",
      "train loss:0.008183185425341098\n",
      "train loss:0.02181118853253639\n",
      "train loss:0.004582261497290082\n",
      "train loss:0.0012590151115179132\n",
      "train loss:0.009051946787354189\n",
      "train loss:0.014635801298728235\n",
      "train loss:0.010201469600871389\n",
      "train loss:0.002746941550717451\n",
      "train loss:0.005071488754779232\n",
      "train loss:0.020213517308152656\n",
      "train loss:0.012793690585249902\n",
      "train loss:0.032472108448489266\n",
      "train loss:0.020549073557639415\n",
      "train loss:0.011717762045917585\n",
      "train loss:0.0165274533480219\n",
      "train loss:0.01782389102717058\n",
      "train loss:0.00532876012077835\n",
      "train loss:0.007978604602679634\n",
      "train loss:0.004773861446851641\n",
      "train loss:0.004892903909955582\n",
      "train loss:0.010288221008820136\n",
      "train loss:0.004454622984505817\n",
      "train loss:0.008616210762971232\n",
      "train loss:0.018048935920773307\n",
      "train loss:0.013554085649057267\n",
      "train loss:0.003940713255476431\n",
      "train loss:0.0408108195410386\n",
      "train loss:0.004769623843180546\n",
      "train loss:0.012159813277621194\n",
      "train loss:0.08954300217481506\n",
      "train loss:0.002621192230831584\n",
      "train loss:0.0115810787178361\n",
      "train loss:0.06582913846170318\n",
      "train loss:0.005104374781371808\n",
      "train loss:0.012583422142462622\n",
      "train loss:0.0031815916444407366\n",
      "train loss:0.01881535720750936\n",
      "train loss:0.004674180438788981\n",
      "train loss:0.010441336928767425\n",
      "train loss:0.007735627230331475\n",
      "train loss:0.014833671912354155\n",
      "train loss:0.01264003839775366\n",
      "train loss:0.006977263378289876\n",
      "train loss:0.002365598717167558\n",
      "train loss:0.016936327205060112\n",
      "train loss:0.0031759022285005895\n",
      "train loss:0.005616150475247506\n",
      "train loss:0.017998408176832084\n",
      "train loss:0.0007085849262595469\n",
      "train loss:0.02878810941848117\n",
      "train loss:0.008109742565496453\n",
      "train loss:0.015175548034823157\n",
      "train loss:0.014860336828452192\n",
      "train loss:0.01168359233876878\n",
      "train loss:0.060314562898407795\n",
      "train loss:0.003840739665441767\n",
      "train loss:0.002349786726064938\n",
      "train loss:0.01923331539368044\n",
      "train loss:0.0016639950046638786\n",
      "train loss:0.0014862615146351713\n",
      "train loss:0.0018456537906958088\n",
      "train loss:0.023012022320490176\n",
      "train loss:0.0016736513487656279\n",
      "train loss:0.013241216238175176\n",
      "train loss:0.004511592232246759\n",
      "train loss:0.009228191822734622\n",
      "train loss:0.02578251801356091\n",
      "train loss:0.017906071770847295\n",
      "train loss:0.00728781826968419\n",
      "train loss:0.01124085898957854\n",
      "train loss:0.0038462587477622284\n",
      "train loss:0.004026134389609677\n",
      "train loss:0.004841952994058787\n",
      "train loss:0.004311381588197032\n",
      "train loss:0.006676634538231772\n",
      "train loss:0.019686592863551133\n",
      "train loss:0.0025168718351562557\n",
      "train loss:0.0029590889528131846\n",
      "train loss:0.02406930874750251\n",
      "train loss:0.02830499705755563\n",
      "train loss:0.0014898146595991343\n",
      "train loss:0.02141078274703324\n",
      "train loss:0.005700535550339308\n",
      "train loss:0.02471390504477666\n",
      "train loss:0.005686638266068023\n",
      "train loss:0.012396517817806268\n",
      "train loss:0.0025721978031948635\n",
      "train loss:0.08534512817819236\n",
      "train loss:0.004727611469398681\n",
      "train loss:0.008491480842976991\n",
      "train loss:0.003275168220251911\n",
      "train loss:0.00299392594856804\n",
      "train loss:0.005320245429739096\n",
      "train loss:0.0025295003144774693\n",
      "train loss:0.002091988037683813\n",
      "train loss:0.0036465032901881063\n",
      "train loss:0.010756555365005587\n",
      "train loss:0.011501606020481591\n",
      "train loss:0.001744564344727561\n",
      "train loss:0.005908414368053042\n",
      "train loss:0.003817486388480952\n",
      "train loss:0.022789330323498024\n",
      "train loss:0.011216802643528444\n",
      "train loss:0.00254777867742854\n",
      "train loss:0.004492882345068742\n",
      "train loss:0.007783191562466571\n",
      "train loss:0.006197575464805051\n",
      "train loss:0.0076770278796106295\n",
      "train loss:0.005398234154418095\n",
      "train loss:0.026027778450227107\n",
      "train loss:0.0206010805337007\n",
      "train loss:0.005349121843339363\n",
      "train loss:0.006693006625516374\n",
      "train loss:0.006667607148055493\n",
      "train loss:0.007838686698442718\n",
      "train loss:0.0036313694684243984\n",
      "train loss:0.005077070943451357\n",
      "train loss:0.04968967712376032\n",
      "train loss:0.04994280862691559\n",
      "train loss:0.0020552206151219503\n",
      "train loss:0.007249736406067232\n",
      "train loss:0.0340143142485481\n",
      "train loss:0.011705687281255697\n",
      "train loss:0.004251736785287222\n",
      "train loss:0.02002104778275978\n",
      "train loss:0.0063104399302240065\n",
      "train loss:0.009771087072251715\n",
      "train loss:0.0031808352836084396\n",
      "train loss:0.00955709762002337\n",
      "train loss:0.03289582636066003\n",
      "train loss:0.05566654751349755\n",
      "train loss:0.053446971179359835\n",
      "train loss:0.001551512915832083\n",
      "train loss:0.01821029924737642\n",
      "train loss:0.003778395857371859\n",
      "train loss:0.01462109275220582\n",
      "train loss:0.05297125913123445\n",
      "train loss:0.0028367690383216617\n",
      "train loss:0.018501696510909736\n",
      "train loss:0.025385066937910362\n",
      "train loss:0.03203522921848927\n",
      "train loss:0.0010435708355327053\n",
      "train loss:0.0032562047010120444\n",
      "train loss:0.005012924671087569\n",
      "train loss:0.00715281468893608\n",
      "train loss:0.0047622404325347225\n",
      "train loss:0.031206795234564134\n",
      "train loss:0.0027466181229346674\n",
      "train loss:0.0009648066838841883\n",
      "train loss:0.006822023713702301\n",
      "train loss:0.015379090177995294\n",
      "train loss:0.058519258453480216\n",
      "train loss:0.013398043320194657\n",
      "train loss:0.04560657295558031\n",
      "train loss:0.010443489644388381\n",
      "train loss:0.007312719157620285\n",
      "train loss:0.002512813031288723\n",
      "train loss:0.006088166185854508\n",
      "train loss:0.011991558756056411\n",
      "train loss:0.03304052175705376\n",
      "train loss:0.004717806026980337\n",
      "train loss:0.014271206963310358\n",
      "train loss:0.013940354594987547\n",
      "train loss:0.018409426559954187\n",
      "train loss:0.0013320028996172855\n",
      "train loss:0.019763654633725262\n",
      "train loss:0.002487536126596273\n",
      "train loss:0.00824118276998173\n",
      "train loss:0.01795220661014123\n",
      "train loss:0.0069419180191027914\n",
      "train loss:0.023589383302571015\n",
      "train loss:0.010296433647902439\n",
      "train loss:0.015077478097524764\n",
      "train loss:0.0023806664948074104\n",
      "train loss:0.009018179521003967\n",
      "train loss:0.002431584606081714\n",
      "train loss:0.012073652480343623\n",
      "train loss:0.010724860278719058\n",
      "train loss:0.0010035502221655073\n",
      "train loss:0.001919361191590938\n",
      "train loss:0.005151428060912613\n",
      "train loss:0.01068607830867689\n",
      "current iter num:  4800\n",
      "=== epoch:9, train acc:0.99, test acc:0.986 ===\n",
      "train loss:0.0064810826284925575\n",
      "train loss:0.005941072168841275\n",
      "train loss:0.017468461059321552\n",
      "train loss:0.029136984406169117\n",
      "train loss:0.023918716306042538\n",
      "train loss:0.0037337314978619636\n",
      "train loss:0.001585205318900415\n",
      "train loss:0.004637999231253438\n",
      "train loss:0.005079081413739191\n",
      "train loss:0.0035853910054433687\n",
      "train loss:0.0139461411319306\n",
      "train loss:0.0034714087970751013\n",
      "train loss:0.0026364173042401923\n",
      "train loss:0.025756862967923504\n",
      "train loss:0.03319703512037009\n",
      "train loss:0.0033367981927210437\n",
      "train loss:0.011438087363593426\n",
      "train loss:0.005422855294343842\n",
      "train loss:0.01575915463157264\n",
      "train loss:0.012612522806722317\n",
      "train loss:0.008487894009170964\n",
      "train loss:0.015836158478925486\n",
      "train loss:0.010274431418700936\n",
      "train loss:0.0021645479787769277\n",
      "train loss:0.00671525594347817\n",
      "train loss:0.01233642236886405\n",
      "train loss:0.053759604982592385\n",
      "train loss:0.04892221633765015\n",
      "train loss:0.01278461720511453\n",
      "train loss:0.05703910005606227\n",
      "train loss:0.011245634916158513\n",
      "train loss:0.0011935972328145541\n",
      "train loss:0.016892605265100537\n",
      "train loss:0.001716034339815123\n",
      "train loss:0.030130155894906033\n",
      "train loss:0.005417832622511758\n",
      "train loss:0.023011804113446513\n",
      "train loss:0.019393880812907147\n",
      "train loss:0.0491588333250319\n",
      "train loss:0.01848445375303322\n",
      "train loss:0.0037432834649270613\n",
      "train loss:0.017574659137873948\n",
      "train loss:0.020350161664664926\n",
      "train loss:0.006038851727012921\n",
      "train loss:0.009707463081985753\n",
      "train loss:0.011133404457248064\n",
      "train loss:0.005896985634773882\n",
      "train loss:0.0037086380276153187\n",
      "train loss:0.0038062632979941487\n",
      "train loss:0.0068120403346970115\n",
      "train loss:0.014376911240788721\n",
      "train loss:0.006675129269956812\n",
      "train loss:0.0026643808334000183\n",
      "train loss:0.03948485686205863\n",
      "train loss:0.0020766995138120255\n",
      "train loss:0.004686882818891295\n",
      "train loss:0.002395278606753098\n",
      "train loss:0.0028182489988698993\n",
      "train loss:0.006409565254749109\n",
      "train loss:0.007437751843067982\n",
      "train loss:0.006122486562770606\n",
      "train loss:0.004878195085834047\n",
      "train loss:0.005905036985688776\n",
      "train loss:0.005420352312004878\n",
      "train loss:0.007107498791063974\n",
      "train loss:0.009484811855029075\n",
      "train loss:0.01194840097847934\n",
      "train loss:0.012796828682265067\n",
      "train loss:0.016693660891583008\n",
      "train loss:0.015313383086937336\n",
      "train loss:0.008261960425963683\n",
      "train loss:0.007123791281161018\n",
      "train loss:0.001686092768158755\n",
      "train loss:0.010598818516970834\n",
      "train loss:0.0027030127359401485\n",
      "train loss:0.025678649865215526\n",
      "train loss:0.029756832482565162\n",
      "train loss:0.03297316911988034\n",
      "train loss:0.043025159057161366\n",
      "train loss:0.0012702665411104654\n",
      "train loss:0.003095950751370107\n",
      "train loss:0.004432770458999289\n",
      "train loss:0.006358988894579004\n",
      "train loss:0.00261735033926111\n",
      "train loss:0.01954188004542248\n",
      "train loss:0.015118004386871036\n",
      "train loss:0.004709031252111266\n",
      "train loss:0.0029027589111463785\n",
      "train loss:0.005631485774917246\n",
      "train loss:0.003986591760676713\n",
      "train loss:0.005611911958459357\n",
      "train loss:0.020816999814896587\n",
      "train loss:0.01366821405755382\n",
      "train loss:0.003314989536258775\n",
      "train loss:0.002062471657005355\n",
      "train loss:0.0017479169153148425\n",
      "train loss:0.004115567713691009\n",
      "train loss:0.004253626982457756\n",
      "train loss:0.0057582794479336\n",
      "train loss:0.00799464011819138\n",
      "train loss:0.007500239257820895\n",
      "train loss:0.030447317359818222\n",
      "train loss:0.004294120348734178\n",
      "train loss:0.0015554330917717974\n",
      "train loss:0.004940685021930421\n",
      "train loss:0.03766241515425187\n",
      "train loss:0.029096452299632425\n",
      "train loss:0.013547070260929923\n",
      "train loss:0.007176824672434804\n",
      "train loss:0.001088299532558697\n",
      "train loss:0.010403878307331894\n",
      "train loss:0.02822462182514231\n",
      "train loss:0.03352948752391989\n",
      "train loss:0.0019991706917063894\n",
      "train loss:0.0050820296931435496\n",
      "train loss:0.002631619522300712\n",
      "train loss:0.0015980176652352828\n",
      "train loss:0.003201678894983171\n",
      "train loss:0.00616806525873838\n",
      "train loss:0.008390024966398036\n",
      "train loss:0.007381312365920385\n",
      "train loss:0.010032899836169585\n",
      "train loss:0.009422851970485428\n",
      "train loss:0.04140259974559878\n",
      "train loss:0.01071448261792614\n",
      "train loss:0.008287913575477123\n",
      "train loss:0.028693621649368534\n",
      "train loss:0.012853430505432428\n",
      "train loss:0.0051408058158359015\n",
      "train loss:0.006741367175671689\n",
      "train loss:0.0036258274456216146\n",
      "train loss:0.011867639271863168\n",
      "train loss:0.01302478895745391\n",
      "train loss:0.005835894238736375\n",
      "train loss:0.0017174841712088338\n",
      "train loss:0.0022939742716487797\n",
      "train loss:0.006250613462925994\n",
      "train loss:0.007754631690839182\n",
      "train loss:0.007466336711348408\n",
      "train loss:0.005177497892919773\n",
      "train loss:0.00407295800415885\n",
      "train loss:0.009984926270515464\n",
      "train loss:0.0033184790502947177\n",
      "train loss:0.01623584421663683\n",
      "train loss:0.003526790712603125\n",
      "train loss:0.002408020468055474\n",
      "train loss:0.006859235124172777\n",
      "train loss:0.011280182339120917\n",
      "train loss:0.0017486712050914382\n",
      "train loss:0.010771848218333215\n",
      "train loss:0.0030262172890765937\n",
      "train loss:0.0024005293731977763\n",
      "train loss:0.0019916222846979\n",
      "train loss:0.00679300369240473\n",
      "train loss:0.0030750191177533843\n",
      "train loss:0.00894008043899248\n",
      "train loss:0.003124234513659529\n",
      "train loss:0.05870922048628458\n",
      "train loss:0.010319443735773738\n",
      "train loss:0.00434228452988843\n",
      "train loss:0.027246973542625997\n",
      "train loss:0.026149322610981386\n",
      "train loss:0.0030250874117138684\n",
      "train loss:0.0015714747486331789\n",
      "train loss:0.012710971652178547\n",
      "train loss:0.0098232207425862\n",
      "train loss:0.006161523841410133\n",
      "train loss:0.005820126818063483\n",
      "train loss:0.006858814410591138\n",
      "train loss:0.002484430045319167\n",
      "train loss:0.008688219434777036\n",
      "train loss:0.040553571603296826\n",
      "train loss:0.00697484335192738\n",
      "train loss:0.02995322077833515\n",
      "train loss:0.00725396526985304\n",
      "train loss:0.010187012132483271\n",
      "train loss:0.0053462080910935085\n",
      "train loss:0.008045293739570337\n",
      "train loss:0.0564888440832503\n",
      "train loss:0.0053570319956998\n",
      "train loss:0.014826144633141606\n",
      "train loss:0.004167793904983713\n",
      "train loss:0.006545095724066561\n",
      "train loss:0.004910256864172903\n",
      "train loss:0.003310253486423895\n",
      "train loss:0.03711504372859806\n",
      "train loss:0.016748473483984318\n",
      "train loss:0.04272219003222465\n",
      "train loss:0.0071435516284595\n",
      "train loss:0.0027323121307103585\n",
      "train loss:0.007998079061324529\n",
      "train loss:0.0034062547724927742\n",
      "train loss:0.010762864672137742\n",
      "train loss:0.01015826219883377\n",
      "train loss:0.05742000335080696\n",
      "train loss:0.014281440904717424\n",
      "train loss:0.007022763810577291\n",
      "train loss:0.005832103166169571\n",
      "train loss:0.004612975922512804\n",
      "train loss:0.0029772543742962922\n",
      "train loss:0.0027648777946565855\n",
      "train loss:0.009751161452111096\n",
      "train loss:0.012818077787899286\n",
      "train loss:0.013902782556659525\n",
      "train loss:0.0015993536875414881\n",
      "train loss:0.0019509426289302991\n",
      "train loss:0.0033051055079857056\n",
      "train loss:0.006619621617831013\n",
      "train loss:0.003821122516013289\n",
      "train loss:0.021681907206286644\n",
      "train loss:0.004340166039627269\n",
      "train loss:0.004305878447784581\n",
      "train loss:0.0003766126143800301\n",
      "train loss:0.014474621342431281\n",
      "train loss:0.0025992068278959655\n",
      "train loss:0.0028152723140671314\n",
      "train loss:0.05403049070549011\n",
      "train loss:0.005127715473522246\n",
      "train loss:0.03794979109688796\n",
      "train loss:0.005697426070521188\n",
      "train loss:0.0020097986254728716\n",
      "train loss:0.0026794715184808326\n",
      "train loss:0.006364262087015871\n",
      "train loss:0.001966398695211305\n",
      "train loss:0.002726762585134157\n",
      "train loss:0.006754094156638916\n",
      "train loss:0.010660901466436026\n",
      "train loss:0.03367134733537209\n",
      "train loss:0.01917845935531378\n",
      "train loss:0.02198966264099026\n",
      "train loss:0.02100093904079116\n",
      "train loss:0.015119477140099506\n",
      "train loss:0.008836038663193643\n",
      "train loss:0.0062662212081442195\n",
      "train loss:0.09645256396922224\n",
      "train loss:0.0026948830070548453\n",
      "train loss:0.00483838245071596\n",
      "train loss:0.006531822215920242\n",
      "train loss:0.024123840659271485\n",
      "train loss:0.031537437181779204\n",
      "train loss:0.006554619579912792\n",
      "train loss:0.000922063215985808\n",
      "train loss:0.004422529079223903\n",
      "train loss:0.006532470825924973\n",
      "train loss:0.04121349090160567\n",
      "train loss:0.016496600745988573\n",
      "train loss:0.015711826533870515\n",
      "train loss:0.015920067757440547\n",
      "train loss:0.00830612529652436\n",
      "train loss:0.04612749133897382\n",
      "train loss:0.0009216214205561415\n",
      "train loss:0.0059595280376682595\n",
      "train loss:0.005313361896056376\n",
      "train loss:0.0025275071277514323\n",
      "train loss:0.017763686712244816\n",
      "train loss:0.016460711538580156\n",
      "train loss:0.018638177933561036\n",
      "train loss:0.0011770382566144441\n",
      "train loss:0.0030069302705529825\n",
      "train loss:0.006578756303702301\n",
      "train loss:0.03089318939967499\n",
      "train loss:0.02047640554464661\n",
      "train loss:0.00495224705244093\n",
      "train loss:0.0027832675943664947\n",
      "train loss:0.016988731282890957\n",
      "train loss:0.0017187146336366088\n",
      "train loss:0.004615321411651398\n",
      "train loss:0.012813288164012157\n",
      "train loss:0.00872413367706625\n",
      "train loss:0.00034998258343701147\n",
      "train loss:0.011992141222974977\n",
      "train loss:0.004530569703500115\n",
      "train loss:0.009488733256512298\n",
      "train loss:0.0006799830462131792\n",
      "train loss:0.003900080342169453\n",
      "train loss:0.0028624034586020135\n",
      "train loss:0.007466495929536089\n",
      "train loss:0.022927816556595343\n",
      "train loss:0.001621809421916301\n",
      "train loss:0.006331558057112044\n",
      "train loss:0.021472809589746466\n",
      "train loss:0.008441447702218574\n",
      "train loss:0.006818856599736241\n",
      "train loss:0.009930629449010141\n",
      "train loss:0.01108929628870522\n",
      "train loss:0.009645083684070297\n",
      "train loss:0.011519290756039436\n",
      "train loss:0.014699256240325658\n",
      "train loss:0.012926376189128296\n",
      "train loss:0.009975514700492431\n",
      "train loss:0.005333521648309136\n",
      "train loss:0.003226585572967232\n",
      "train loss:0.02083613757359014\n",
      "train loss:0.001084728876149567\n",
      "train loss:0.031780716606659466\n",
      "train loss:0.0030715732315737855\n",
      "train loss:0.017429767966343504\n",
      "train loss:0.054239596430055693\n",
      "train loss:0.028834301151380276\n",
      "train loss:0.005646245258217466\n",
      "train loss:0.009919512919590505\n",
      "train loss:0.011025100671452612\n",
      "train loss:0.013343291322541648\n",
      "train loss:0.008013186601642049\n",
      "train loss:0.024578953622915355\n",
      "train loss:0.03848868455074991\n",
      "train loss:0.01472895984730855\n",
      "train loss:0.0025528564617866593\n",
      "train loss:0.0047890784992974285\n",
      "train loss:0.07048724419190516\n",
      "train loss:0.007545193656680176\n",
      "train loss:0.005593691665589191\n",
      "train loss:0.046465268703280514\n",
      "train loss:0.025606366827856598\n",
      "train loss:0.008133681889764826\n",
      "train loss:0.004593604465289618\n",
      "train loss:0.004734356344444603\n",
      "train loss:0.008366958165085713\n",
      "train loss:0.0011262189164305946\n",
      "train loss:0.02224172585775073\n",
      "train loss:0.025963762945236772\n",
      "train loss:0.006104850315464728\n",
      "train loss:0.004450048917121336\n",
      "train loss:0.0309994172984149\n",
      "train loss:0.0009263513213311071\n",
      "train loss:0.006510009702642627\n",
      "train loss:0.002323831728498438\n",
      "train loss:0.05550585299035459\n",
      "train loss:0.01100465760279603\n",
      "train loss:0.0020555972087444264\n",
      "train loss:0.018585347128254007\n",
      "train loss:0.003620257089019686\n",
      "train loss:0.01247396444393833\n",
      "train loss:0.0033936305667152515\n",
      "train loss:0.012650943252426206\n",
      "train loss:0.003997697825748736\n",
      "train loss:0.006962173563285404\n",
      "train loss:0.00922085095396574\n",
      "train loss:0.00748292387773886\n",
      "train loss:0.005278960649345764\n",
      "train loss:0.0017299672442636735\n",
      "train loss:0.018954394179363843\n",
      "train loss:0.019819016908594046\n",
      "train loss:0.0029556913324107884\n",
      "train loss:0.01592050231552468\n",
      "train loss:0.014985642620401396\n",
      "train loss:0.004765962904949769\n",
      "train loss:0.007427042862170028\n",
      "train loss:0.03585359449462932\n",
      "train loss:0.005434254622569237\n",
      "train loss:0.013485016841849438\n",
      "train loss:0.005820286344277857\n",
      "train loss:0.01419297515671318\n",
      "train loss:0.0032659193768159013\n",
      "train loss:0.011725781506537782\n",
      "train loss:0.008705790946786418\n",
      "train loss:0.0011671321356260373\n",
      "train loss:0.007953406506365042\n",
      "train loss:0.011505298700804589\n",
      "train loss:0.003791414709675018\n",
      "train loss:0.0022438870550861357\n",
      "train loss:0.035371278646078766\n",
      "train loss:0.012104643163737161\n",
      "train loss:0.009687768314595384\n",
      "train loss:0.024673939808422342\n",
      "train loss:0.02033261085469719\n",
      "train loss:0.008679522051390146\n",
      "train loss:0.00927706206830716\n",
      "train loss:0.00568908770897179\n",
      "train loss:0.004061814087087892\n",
      "train loss:0.014613480659366917\n",
      "train loss:0.008796328059776868\n",
      "train loss:0.02548241565285646\n",
      "train loss:0.003267155076219328\n",
      "train loss:0.014333932247753032\n",
      "train loss:0.012650829814488038\n",
      "train loss:0.018796427034396556\n",
      "train loss:0.0014721517089557185\n",
      "train loss:0.011315564111778286\n",
      "train loss:0.003493970238085281\n",
      "train loss:0.012541211822723757\n",
      "train loss:0.0040164066937962905\n",
      "train loss:0.0033636727721696002\n",
      "train loss:0.02295540084439986\n",
      "train loss:0.00230465855485531\n",
      "train loss:0.013886516177382506\n",
      "train loss:0.009570413743216401\n",
      "train loss:0.06781042064262374\n",
      "train loss:0.009856663868473304\n",
      "train loss:0.003151089048496579\n",
      "train loss:0.008749776094927223\n",
      "train loss:0.0013940270330587434\n",
      "train loss:0.002714696266235398\n",
      "train loss:0.00238483127363203\n",
      "train loss:0.016930559925415013\n",
      "train loss:0.010365772335607091\n",
      "train loss:0.018946228475170124\n",
      "train loss:0.005220742861567814\n",
      "train loss:0.03644309988620259\n",
      "train loss:0.01548900007672287\n",
      "train loss:0.0014481861922857858\n",
      "train loss:0.004306121807144363\n",
      "train loss:0.003986950643285418\n",
      "train loss:0.014948862988926141\n",
      "train loss:0.014844182049423687\n",
      "train loss:0.008237579003314456\n",
      "train loss:0.009747528744121636\n",
      "train loss:0.05215402182560257\n",
      "train loss:0.005759644137464692\n",
      "train loss:0.0036257451883043236\n",
      "train loss:0.00752118785793191\n",
      "train loss:0.0028515443206956577\n",
      "train loss:0.004022373253498007\n",
      "train loss:0.0022386052923845136\n",
      "train loss:0.0015424780796461318\n",
      "train loss:0.028399961954861365\n",
      "train loss:0.00570680069993789\n",
      "train loss:0.0017851468004825624\n",
      "train loss:0.002142835939372487\n",
      "train loss:0.0018695788031109994\n",
      "train loss:0.013587797635458415\n",
      "train loss:0.007331607612781459\n",
      "train loss:0.034357639869465016\n",
      "train loss:0.01611464573595188\n",
      "train loss:0.008294378683897988\n",
      "train loss:0.011682440876426501\n",
      "train loss:0.003253909843302467\n",
      "train loss:0.01086310560912804\n",
      "train loss:0.001837539794329971\n",
      "train loss:0.006926459855756847\n",
      "train loss:0.016251873648124193\n",
      "train loss:0.0061529404662198865\n",
      "train loss:0.005033125324807601\n",
      "train loss:0.00749783969905218\n",
      "train loss:0.0007766291369541298\n",
      "train loss:0.025733838682970765\n",
      "train loss:0.005844511543208864\n",
      "train loss:0.012038953836820208\n",
      "train loss:0.007689419153855379\n",
      "train loss:0.009741387351397384\n",
      "train loss:0.04927222707853649\n",
      "train loss:0.007645405062653421\n",
      "train loss:0.007568471704791267\n",
      "train loss:0.016373797628321717\n",
      "train loss:0.006562464772312499\n",
      "train loss:0.010836986932917543\n",
      "train loss:0.1523821862425639\n",
      "train loss:0.0007987084242080151\n",
      "train loss:0.04587485730248535\n",
      "train loss:0.012524313094848822\n",
      "train loss:0.012587325045520759\n",
      "train loss:0.029698977937050928\n",
      "train loss:0.0029304992326690065\n",
      "train loss:0.00436235867672015\n",
      "train loss:0.0031781298604869664\n",
      "train loss:0.050565944514476886\n",
      "train loss:0.0009704991244590061\n",
      "train loss:0.0011808638195712987\n",
      "train loss:0.014330375036854198\n",
      "train loss:0.009296537960643964\n",
      "train loss:0.017724556217878103\n",
      "train loss:0.00791433344603544\n",
      "train loss:0.003381422180955733\n",
      "train loss:0.015422658523095385\n",
      "train loss:0.010746105605430814\n",
      "train loss:0.004288823523800737\n",
      "train loss:0.006807940788140055\n",
      "train loss:0.0011071570318815951\n",
      "train loss:0.008280414119499906\n",
      "train loss:0.0053733584263323175\n",
      "train loss:0.02370475031506016\n",
      "train loss:0.008531430769366018\n",
      "train loss:0.009254276422875712\n",
      "train loss:0.01445516651995337\n",
      "train loss:0.03482763404199111\n",
      "train loss:0.01333987587473408\n",
      "train loss:0.006131482746902602\n",
      "train loss:0.008526648338530706\n",
      "train loss:0.00590449760704543\n",
      "train loss:0.006734627956758506\n",
      "train loss:0.001887440044919986\n",
      "train loss:0.00210267629091257\n",
      "train loss:0.019488538376171337\n",
      "train loss:0.0070759296173174705\n",
      "train loss:0.010052937384665575\n",
      "train loss:0.013038270620572138\n",
      "train loss:0.004112384288135975\n",
      "train loss:0.00485407511774781\n",
      "train loss:0.003895614026154172\n",
      "train loss:0.007034010996037754\n",
      "train loss:0.007546539369177849\n",
      "train loss:0.00492648232244341\n",
      "train loss:0.014457841709997475\n",
      "train loss:0.0021576145202655186\n",
      "train loss:0.009270986228404672\n",
      "train loss:0.0055691589115535204\n",
      "train loss:0.014665904301578103\n",
      "train loss:0.007110234070624935\n",
      "train loss:0.010594169208841462\n",
      "train loss:0.0036294769424927674\n",
      "train loss:0.02055267865179836\n",
      "train loss:0.007195949291232656\n",
      "train loss:0.020756110780118117\n",
      "train loss:0.004096916707282293\n",
      "train loss:0.01601785943122983\n",
      "train loss:0.002796776130595386\n",
      "train loss:0.0004353209555769688\n",
      "train loss:0.0012227830839261356\n",
      "train loss:0.002327304821713813\n",
      "train loss:0.0006844356470083284\n",
      "train loss:0.0008183706642681344\n",
      "train loss:0.02272284532146933\n",
      "train loss:0.0054108858641232415\n",
      "train loss:0.007535031443255488\n",
      "train loss:0.019271896788739817\n",
      "train loss:0.004683343822973581\n",
      "train loss:0.0007608089996347079\n",
      "train loss:0.010996161752051177\n",
      "train loss:0.0008510470006425334\n",
      "train loss:0.002024396574767692\n",
      "train loss:0.011024950596786593\n",
      "train loss:0.009817754783205245\n",
      "train loss:0.0028483683459265326\n",
      "train loss:0.025421157763726953\n",
      "train loss:0.003911996932329619\n",
      "train loss:0.0488043616044076\n",
      "train loss:0.0062862809672466526\n",
      "train loss:0.005182084107837279\n",
      "train loss:0.0029289648884155205\n",
      "train loss:0.007331574922681014\n",
      "train loss:0.005067838446937816\n",
      "train loss:0.0027649770389503786\n",
      "train loss:0.031260450896575545\n",
      "train loss:0.006143035951137524\n",
      "train loss:0.0029206524959520972\n",
      "train loss:0.005901542602526126\n",
      "train loss:0.0023740750782530536\n",
      "train loss:0.04908967720434587\n",
      "train loss:0.0035679193673318708\n",
      "train loss:0.02198944371156826\n",
      "train loss:0.0012324715309857035\n",
      "train loss:0.01319986044596467\n",
      "train loss:0.0016399979681832727\n",
      "train loss:0.018505858660500495\n",
      "train loss:0.03234866648267944\n",
      "train loss:0.02491111724192557\n",
      "train loss:0.02144798352292988\n",
      "train loss:0.0010724605849057158\n",
      "train loss:0.05982091993676536\n",
      "train loss:0.012318647682162336\n",
      "train loss:0.004770215733460587\n",
      "train loss:0.008789853136340503\n",
      "train loss:0.007531718680936554\n",
      "train loss:0.004578510566168667\n",
      "train loss:0.0074816337346317345\n",
      "train loss:0.003950120842619337\n",
      "train loss:0.014697844601349035\n",
      "train loss:0.005297142142165989\n",
      "train loss:0.013206160350985397\n",
      "train loss:0.0055476087237587264\n",
      "train loss:0.012523976756726053\n",
      "train loss:0.0032180457505844996\n",
      "train loss:0.008268408233768827\n",
      "train loss:0.005413136772131872\n",
      "train loss:0.0360070989227985\n",
      "train loss:0.051735974054022194\n",
      "train loss:0.010574524887933558\n",
      "train loss:0.008126831291311448\n",
      "train loss:0.0043950884025572385\n",
      "train loss:0.028637363590942027\n",
      "train loss:0.005290428160185854\n",
      "train loss:0.017109534769536826\n",
      "train loss:0.006009748715349566\n",
      "train loss:0.01042690293584211\n",
      "train loss:0.00664430315175485\n",
      "train loss:0.004199050476108281\n",
      "train loss:0.0050209515823226684\n",
      "train loss:0.0007160026585666618\n",
      "train loss:0.0037142499933790233\n",
      "train loss:0.0005572837730101591\n",
      "train loss:0.07018491344170072\n",
      "train loss:0.001517835041876423\n",
      "train loss:0.0056025934034160145\n",
      "train loss:0.07410463979317553\n",
      "train loss:0.003867224170299928\n",
      "train loss:0.00447212739952356\n",
      "train loss:0.002492712777370579\n",
      "train loss:0.005985805989389076\n",
      "train loss:0.0062500486368874675\n",
      "train loss:0.00480202295022682\n",
      "train loss:0.009187369833486251\n",
      "train loss:0.009373520259254637\n",
      "train loss:0.004020174082434776\n",
      "train loss:0.01614354412804692\n",
      "train loss:0.0045370651488347395\n",
      "train loss:0.0020681431841044636\n",
      "train loss:0.018591343573136576\n",
      "train loss:0.018304309245328478\n",
      "train loss:0.007996388643215783\n",
      "train loss:0.007235186508291611\n",
      "current iter num:  5400\n",
      "=== epoch:10, train acc:0.995, test acc:0.986 ===\n",
      "train loss:0.007119043493074053\n",
      "train loss:0.015821137139850675\n",
      "train loss:0.006698012793007278\n",
      "train loss:0.00165596072632748\n",
      "train loss:0.0005346967201251955\n",
      "train loss:0.08722479764435677\n",
      "train loss:0.0029975984078819563\n",
      "train loss:0.0004611847953286095\n",
      "train loss:0.0058889440521342415\n",
      "train loss:0.012384076630041225\n",
      "train loss:0.002649008406838298\n",
      "train loss:0.00964494613215964\n",
      "train loss:0.0014157299610167851\n",
      "train loss:0.013715777828868458\n",
      "train loss:0.014435996318365252\n",
      "train loss:0.009047762029233795\n",
      "train loss:0.009126165126086492\n",
      "train loss:0.0025646412650466737\n",
      "train loss:0.0008277018780761713\n",
      "train loss:0.018334454971122814\n",
      "train loss:0.009004929614175115\n",
      "train loss:0.004650695306377396\n",
      "train loss:0.0011211115664504245\n",
      "train loss:0.0027718611124550453\n",
      "train loss:0.0030259248521291304\n",
      "train loss:0.005824859047797729\n",
      "train loss:0.10863461356735792\n",
      "train loss:0.006903151878581862\n",
      "train loss:0.005507990419292043\n",
      "train loss:0.004593407465992655\n",
      "train loss:0.0037072896323621487\n",
      "train loss:0.007875196256918819\n",
      "train loss:0.002915686444744423\n",
      "train loss:0.004970486570071313\n",
      "train loss:0.0011816523316461938\n",
      "train loss:0.027758079480444113\n",
      "train loss:0.011585119964595192\n",
      "train loss:0.0036649284523727083\n",
      "train loss:0.0138913979493907\n",
      "train loss:0.007820148665454319\n",
      "train loss:0.01625616387816603\n",
      "train loss:0.028176085954508618\n",
      "train loss:0.01490388176671425\n",
      "train loss:0.008856727766627543\n",
      "train loss:0.014232048471637265\n",
      "train loss:0.0036298512779184156\n",
      "train loss:0.009309966288982606\n",
      "train loss:0.004074945381086952\n",
      "train loss:0.052474233127628514\n",
      "train loss:0.011235829117309616\n",
      "train loss:0.008382834540993122\n",
      "train loss:0.011987925760310792\n",
      "train loss:0.001895497351446339\n",
      "train loss:0.007948431413053628\n",
      "train loss:0.00289438749825553\n",
      "train loss:0.011667058116436525\n",
      "train loss:0.004664209803421149\n",
      "train loss:0.02249310513707059\n",
      "train loss:0.014349533808662756\n",
      "train loss:0.0013675917559916042\n",
      "train loss:0.004838291739228498\n",
      "train loss:0.000523927243713391\n",
      "train loss:0.02437828800645724\n",
      "train loss:0.002825464671528576\n",
      "train loss:0.004749506360669672\n",
      "train loss:0.001072511151977899\n",
      "train loss:0.0016645422555441628\n",
      "train loss:0.006339991440431877\n",
      "train loss:0.027056043990733535\n",
      "train loss:0.006120588330350488\n",
      "train loss:0.008869514520895468\n",
      "train loss:0.02248102532133495\n",
      "train loss:0.01231372015017391\n",
      "train loss:0.003416592686507643\n",
      "train loss:0.006836507530841144\n",
      "train loss:0.00449353512452006\n",
      "train loss:0.021199243085459986\n",
      "train loss:0.15892696698694586\n",
      "train loss:0.010727224916963016\n",
      "train loss:0.0009123572847623748\n",
      "train loss:0.009234449222442553\n",
      "train loss:0.007652082044573302\n",
      "train loss:0.030392952234902574\n",
      "train loss:0.0037715400776943157\n",
      "train loss:0.004628042496019971\n",
      "train loss:0.007225174723031444\n",
      "train loss:0.008221050988531813\n",
      "train loss:0.0069451721338840665\n",
      "train loss:0.016391279004540085\n",
      "train loss:0.0018089577093947857\n",
      "train loss:0.00832156643848141\n",
      "train loss:0.00359094073945477\n",
      "train loss:0.0103388903589848\n",
      "train loss:0.0009831139811713812\n",
      "train loss:0.013676209811450854\n",
      "train loss:0.0019661990641432706\n",
      "train loss:0.010427195854642524\n",
      "train loss:0.007674368405077055\n",
      "train loss:0.0027235995668917178\n",
      "train loss:0.0041907790692683735\n",
      "train loss:0.005652603574034893\n",
      "train loss:0.012853934914801\n",
      "train loss:0.008732722784981661\n",
      "train loss:0.0019020268958103844\n",
      "train loss:0.002432059982135335\n",
      "train loss:0.06359500779575032\n",
      "train loss:0.0022431694976714463\n",
      "train loss:0.006906450074302223\n",
      "train loss:0.0007493309436931591\n",
      "train loss:0.008229953905586741\n",
      "train loss:0.011126969779080696\n",
      "train loss:0.006278364884441545\n",
      "train loss:0.01720597885428867\n",
      "train loss:0.0036115600879865805\n",
      "train loss:0.011978868827459832\n",
      "train loss:0.00023958933128691577\n",
      "train loss:0.004554940074627219\n",
      "train loss:0.002652815038293294\n",
      "train loss:0.0009511929505301949\n",
      "train loss:0.009441887746304216\n",
      "train loss:0.008007659176626348\n",
      "train loss:0.004327661956573236\n",
      "train loss:0.001286562378157184\n",
      "train loss:0.00845708365959726\n",
      "train loss:0.006754643402405466\n",
      "train loss:0.033125115520135966\n",
      "train loss:0.009166639133229236\n",
      "train loss:0.006317953873228401\n",
      "train loss:0.0017399549488093507\n",
      "train loss:0.0036924966227350297\n",
      "train loss:0.0008562635884291124\n",
      "train loss:0.0028836718044295486\n",
      "train loss:0.00607577649441171\n",
      "train loss:0.00788423747288967\n",
      "train loss:0.008051572711395612\n",
      "train loss:0.005706392920691175\n",
      "train loss:0.022705036740247445\n",
      "train loss:0.007584214599959231\n",
      "train loss:0.008242171099236062\n",
      "train loss:0.009019505553636089\n",
      "train loss:0.0033474691700178328\n",
      "train loss:0.005216108909646965\n",
      "train loss:0.0037112869119145923\n",
      "train loss:0.006022815492791612\n",
      "train loss:0.007775449103049177\n",
      "train loss:0.0024756957579275714\n",
      "train loss:0.0160128130761476\n",
      "train loss:0.011898333344998669\n",
      "train loss:0.0023681206812854283\n",
      "train loss:0.021384875045365702\n",
      "train loss:0.003220927332378916\n",
      "train loss:0.007154556349807403\n",
      "train loss:0.003980882011849206\n",
      "train loss:0.010664886771724695\n",
      "train loss:0.004510722446630068\n",
      "train loss:0.005320741374032368\n",
      "train loss:0.008410293001466722\n",
      "train loss:0.00351516094256537\n",
      "train loss:0.0044657148178953885\n",
      "train loss:0.006570634268151892\n",
      "train loss:0.011584142384670346\n",
      "train loss:0.00198380704063643\n",
      "train loss:0.0025537601785123433\n",
      "train loss:0.02819032800885797\n",
      "train loss:0.04858115150090155\n",
      "train loss:0.005507271276997442\n",
      "train loss:0.0033457884665950783\n",
      "train loss:0.004512384489334321\n",
      "train loss:0.06838804870854424\n",
      "train loss:0.0030090572637600107\n",
      "train loss:0.002380239360980513\n",
      "train loss:0.02978646035037631\n",
      "train loss:0.002961366443724334\n",
      "train loss:0.007588541165751096\n",
      "train loss:0.006797874679463622\n",
      "train loss:0.002990833219198182\n",
      "train loss:0.008666152843063817\n",
      "train loss:0.007785455178552115\n",
      "train loss:0.0029470070909542777\n",
      "train loss:0.029028694435607735\n",
      "train loss:0.01017332768495275\n",
      "train loss:0.005375177308715865\n",
      "train loss:0.0008664533278578091\n",
      "train loss:0.011301164603577456\n",
      "train loss:0.01713630877893409\n",
      "train loss:0.021382962939017456\n",
      "train loss:0.004384413460718073\n",
      "train loss:0.01060432041989715\n",
      "train loss:0.011448622819230158\n",
      "train loss:0.0014227608397413857\n",
      "train loss:0.010733675986595113\n",
      "train loss:0.004995548221580164\n",
      "train loss:0.011608061878579162\n",
      "train loss:0.005613935269030375\n",
      "train loss:0.007522146510598063\n",
      "train loss:0.009882667918947919\n",
      "train loss:0.03463406424725709\n",
      "train loss:0.006562617986191841\n",
      "train loss:0.010290904031282001\n",
      "train loss:0.005037565905923932\n",
      "train loss:0.0040563338072889055\n",
      "train loss:0.0167599638100409\n",
      "train loss:0.004470411557113617\n",
      "train loss:0.010422170705228434\n",
      "train loss:0.005313461877040883\n",
      "train loss:0.0013528619178214756\n",
      "train loss:0.008199989011958942\n",
      "train loss:0.003205500220550901\n",
      "train loss:0.01577525322192467\n",
      "train loss:0.0028694140141315933\n",
      "train loss:0.010283802726974031\n",
      "train loss:0.0029244331580988246\n",
      "train loss:0.07326845716703906\n",
      "train loss:0.00506822334566144\n",
      "train loss:0.008957624687180588\n",
      "train loss:0.00867551741371587\n",
      "train loss:0.0065713180890834035\n",
      "train loss:0.006611865122172369\n",
      "train loss:0.013310470356728783\n",
      "train loss:0.005898450747570677\n",
      "train loss:0.007939106838119301\n",
      "train loss:0.010504232105017185\n",
      "train loss:0.015592671707019444\n",
      "train loss:0.010055883516749546\n",
      "train loss:0.008959925209229063\n",
      "train loss:0.003670135947791178\n",
      "train loss:0.03902867589704067\n",
      "train loss:0.009243614419833361\n",
      "train loss:0.007576288893596159\n",
      "train loss:0.002460877029304855\n",
      "train loss:0.0016776150131909628\n",
      "train loss:0.014992186611838392\n",
      "train loss:0.004827459567144388\n",
      "train loss:0.029247781435273592\n",
      "train loss:0.0033068094835471303\n",
      "train loss:0.040577244848828534\n",
      "train loss:0.0020923348867736154\n",
      "train loss:0.027494076576012993\n",
      "train loss:0.005441808804574323\n",
      "train loss:0.025258191954132713\n",
      "train loss:0.006027255477640191\n",
      "train loss:0.009813440048051616\n",
      "train loss:0.0032319516368283664\n",
      "train loss:0.0035471881731916973\n",
      "train loss:0.008522015769963132\n",
      "train loss:0.0006530383722076625\n",
      "train loss:0.04093107536437143\n",
      "train loss:0.0010727373933650437\n",
      "train loss:0.0076658089880681235\n",
      "train loss:0.0176060299180075\n",
      "train loss:0.003983073975366379\n",
      "train loss:0.0010564333049360773\n",
      "train loss:0.004767855309392948\n",
      "train loss:0.0114276857619333\n",
      "train loss:0.0040711417822747965\n",
      "train loss:0.021785156009729568\n",
      "train loss:0.004137331764387235\n",
      "train loss:0.0014888097238665147\n",
      "train loss:0.019134820304931632\n",
      "train loss:0.0017343164822689926\n",
      "train loss:0.0017909130932783394\n",
      "train loss:0.003635008988287562\n",
      "train loss:0.007492240995227396\n",
      "train loss:0.005401929532311068\n",
      "train loss:0.0032739113995981528\n",
      "train loss:0.0068159495188317865\n",
      "train loss:0.0005072173176992873\n",
      "train loss:0.02697621795124906\n",
      "train loss:0.0022251675998259716\n",
      "train loss:0.014039189098695668\n",
      "train loss:0.03355527445733084\n",
      "train loss:0.001996980539673718\n",
      "train loss:0.017655067950892726\n",
      "train loss:0.010265612070209073\n",
      "train loss:0.014032199428406455\n",
      "train loss:0.010215954338770254\n",
      "train loss:0.015905565913708408\n",
      "train loss:0.012755385223888445\n",
      "train loss:0.013355526180317368\n",
      "train loss:0.04034740850599343\n",
      "train loss:0.020225791854199805\n",
      "train loss:0.005065662427408707\n",
      "train loss:0.007863743366769772\n",
      "train loss:0.005206539286005827\n",
      "train loss:0.01608212441451487\n",
      "train loss:0.010446882150877699\n",
      "train loss:0.009185227642244716\n",
      "train loss:0.006005464089418798\n",
      "train loss:0.015059001034337653\n",
      "train loss:0.006196466762325451\n",
      "train loss:0.020039993183603742\n",
      "train loss:0.024949388672624747\n",
      "train loss:0.0027180977079921125\n",
      "train loss:0.010631296247825224\n",
      "train loss:0.0015363507692065558\n",
      "train loss:0.0019164495293234613\n",
      "train loss:0.008659860987709123\n",
      "train loss:0.007419038646626669\n",
      "train loss:0.15269264327436585\n",
      "train loss:0.002037625646626064\n",
      "train loss:0.0005865890779790001\n",
      "train loss:0.01711700864855376\n",
      "train loss:0.01965389524927726\n",
      "train loss:0.010739554176154435\n",
      "train loss:0.014766999253938463\n",
      "train loss:0.005270591162104279\n",
      "train loss:0.010767340088770092\n",
      "train loss:0.011550949021879984\n",
      "train loss:0.008456511237174662\n",
      "train loss:0.008938037238293882\n",
      "train loss:0.006741495860672628\n",
      "train loss:0.005129653625110472\n",
      "train loss:0.012323399194353801\n",
      "train loss:0.0021075912243327354\n",
      "train loss:0.01085871009053354\n",
      "train loss:0.016922670608621623\n",
      "train loss:0.002546938223112273\n",
      "train loss:0.002384531122319105\n",
      "train loss:0.008889192546731444\n",
      "train loss:0.06500712197609118\n",
      "train loss:0.005518001999124612\n",
      "train loss:0.008638845607963009\n",
      "train loss:0.0012165665878981547\n",
      "train loss:0.0030249262843841333\n",
      "train loss:0.017826608775361706\n",
      "train loss:0.008311058572304493\n",
      "train loss:0.000676383278719835\n",
      "train loss:0.004882955122667384\n",
      "train loss:0.002890187173593054\n",
      "train loss:0.034228608445652856\n",
      "train loss:0.001980401317286495\n",
      "train loss:0.012879727217206462\n",
      "train loss:0.003506175661317944\n",
      "train loss:0.004479635541958507\n",
      "train loss:0.004116442492121915\n",
      "train loss:0.011142155356669832\n",
      "train loss:0.007544181543758445\n",
      "train loss:0.01903188465485238\n",
      "train loss:0.0022680095516191758\n",
      "train loss:0.00029678858539009445\n",
      "train loss:0.00434369498872847\n",
      "train loss:0.0023162564683683303\n",
      "train loss:0.016280550162094426\n",
      "train loss:0.015999264128261165\n",
      "train loss:0.0017788706451772349\n",
      "train loss:0.014481027035802492\n",
      "train loss:0.0012964952208483732\n",
      "train loss:0.020721817315285475\n",
      "train loss:0.004796559970086126\n",
      "train loss:0.0018359258108906723\n",
      "train loss:0.006375305298574183\n",
      "train loss:0.007935137867442996\n",
      "train loss:0.002031942279923944\n",
      "train loss:0.02107782010361335\n",
      "train loss:0.0013989809091301188\n",
      "train loss:0.008537301345763923\n",
      "train loss:0.01344534170770054\n",
      "train loss:0.0027496720568527823\n",
      "train loss:0.0029120608952016274\n",
      "train loss:0.009364271317478568\n",
      "train loss:0.002567480736884263\n",
      "train loss:0.003330395600766339\n",
      "train loss:0.0005022683883216099\n",
      "train loss:0.026563176077876262\n",
      "train loss:0.001425087978422464\n",
      "train loss:0.05258761991576251\n",
      "train loss:0.009019906365218898\n",
      "train loss:0.003939655761101762\n",
      "train loss:0.0023467897369512246\n",
      "train loss:0.012092275991490296\n",
      "train loss:0.0019422550556061566\n",
      "train loss:0.001954263534268125\n",
      "train loss:0.006058374695562451\n",
      "train loss:0.005473038802948642\n",
      "train loss:0.0016959238654198879\n",
      "train loss:0.006110234514129234\n",
      "train loss:0.0050506296292238\n",
      "train loss:0.003201649727051229\n",
      "train loss:0.001562891267675903\n",
      "train loss:0.005654700243912094\n",
      "train loss:0.01331882220184976\n",
      "train loss:0.00859746403474832\n",
      "train loss:0.002440044724233774\n",
      "train loss:0.004294467497272639\n",
      "train loss:0.002349125512804855\n",
      "train loss:0.043252368866434646\n",
      "train loss:0.0027470526429875354\n",
      "train loss:0.0114764002221567\n",
      "train loss:0.0031618822885772246\n",
      "train loss:0.0059938779709049475\n",
      "train loss:0.002155842070890126\n",
      "train loss:0.006800837485252436\n",
      "train loss:0.005040222040947757\n",
      "train loss:0.004596742810922812\n",
      "train loss:0.0021450168308361924\n",
      "train loss:0.01361165423612789\n",
      "train loss:0.03344754118785269\n",
      "train loss:0.0036260305923120494\n",
      "train loss:0.0037456026041433697\n",
      "train loss:0.0069857430100413375\n",
      "train loss:0.003638740992271384\n",
      "train loss:0.003986227355894876\n",
      "train loss:0.0020893278559096567\n",
      "train loss:0.011542054560438289\n",
      "train loss:0.03642148752834895\n",
      "train loss:0.008586364961599419\n",
      "train loss:0.0023735054958734137\n",
      "train loss:0.009859325506623315\n",
      "train loss:0.00701558115510123\n",
      "train loss:0.004068046037825341\n",
      "train loss:0.022304266702634936\n",
      "train loss:0.007604341558523081\n",
      "train loss:0.008930265177637902\n",
      "train loss:0.00600482400736936\n",
      "train loss:0.013274803265998418\n",
      "train loss:0.04940910806368731\n",
      "train loss:0.005364720380727603\n",
      "train loss:0.016227982266179084\n",
      "train loss:0.0010707332685649248\n",
      "train loss:0.006694914823840895\n",
      "train loss:0.0031523063439277215\n",
      "train loss:0.007345311371973235\n",
      "train loss:0.0195501158561517\n",
      "train loss:0.0017466184078005686\n",
      "train loss:0.0034815209289087577\n",
      "train loss:0.004842998522652209\n",
      "train loss:0.01129268339533421\n",
      "train loss:0.002086478774799293\n",
      "train loss:0.01483880725596356\n",
      "train loss:0.0058431465827332475\n",
      "train loss:0.009994920337653389\n",
      "train loss:0.011175602020240441\n",
      "train loss:0.003832121889060214\n",
      "train loss:0.000994101827362305\n",
      "train loss:0.0007407819200372968\n",
      "train loss:0.004871813704539826\n",
      "train loss:0.0046101198126519164\n",
      "train loss:0.01896766024355936\n",
      "train loss:0.0007191264607128065\n",
      "train loss:0.009876934447473975\n",
      "train loss:0.0034889112098449994\n",
      "train loss:0.0009613335415776139\n",
      "train loss:0.004518659435698963\n",
      "train loss:0.0003491601378560507\n",
      "train loss:0.0010752139225928762\n",
      "train loss:0.003737092398038956\n",
      "train loss:0.008688728935772477\n",
      "train loss:0.011928160878104586\n",
      "train loss:0.00342881939269241\n",
      "train loss:0.0002965681102576017\n",
      "train loss:0.0023674866783509317\n",
      "train loss:0.004787132992240984\n",
      "train loss:0.0009601336789745308\n",
      "train loss:0.006000550645851203\n",
      "train loss:0.012646523207956959\n",
      "train loss:0.0059521026002184564\n",
      "train loss:0.009915566038125054\n",
      "train loss:0.004887907046039646\n",
      "train loss:0.01813349317903085\n",
      "train loss:0.005307315652369222\n",
      "train loss:0.002160233913161923\n",
      "train loss:0.0001769465163481935\n",
      "train loss:0.010165138593191438\n",
      "train loss:0.0067997259587231065\n",
      "train loss:0.004259291950277741\n",
      "train loss:0.003828362954420977\n",
      "train loss:0.007497555126614279\n",
      "train loss:0.001821434856477564\n",
      "train loss:0.004857332144632621\n",
      "train loss:0.0077393499752411435\n",
      "train loss:0.006302431329198992\n",
      "train loss:0.0061083861926268124\n",
      "train loss:0.01196391338425346\n",
      "train loss:0.003770918589944662\n",
      "train loss:0.005851001435333391\n",
      "train loss:0.0006158317093926655\n",
      "train loss:0.015803194608484707\n",
      "train loss:0.013981829990300293\n",
      "train loss:0.013314759461047425\n",
      "train loss:0.08220367944442022\n",
      "train loss:0.0040791439776101\n",
      "train loss:0.0027593683862635445\n",
      "train loss:0.007571307797493983\n",
      "train loss:0.001564122285759783\n",
      "train loss:0.008692247566332493\n",
      "train loss:0.0027479991634986097\n",
      "train loss:0.00839519165231246\n",
      "train loss:0.0010754212206901914\n",
      "train loss:0.007849225391785857\n",
      "train loss:0.02818941039103924\n",
      "train loss:0.00966710974884173\n",
      "train loss:0.002108823934712844\n",
      "train loss:0.003742617832888183\n",
      "train loss:0.0008603861811099363\n",
      "train loss:0.01374727028185356\n",
      "train loss:0.006811573451648162\n",
      "train loss:0.01000791190840652\n",
      "train loss:0.003649443483216909\n",
      "train loss:0.048849930430468244\n",
      "train loss:0.003121689709433959\n",
      "train loss:0.00956697767723782\n",
      "train loss:0.011871775949299382\n",
      "train loss:0.0016765835442512091\n",
      "train loss:0.030658252376754968\n",
      "train loss:0.0062569565025845176\n",
      "train loss:0.0049751548571976036\n",
      "train loss:0.0016461143398527169\n",
      "train loss:0.005695748548548563\n",
      "train loss:0.011748357125478252\n",
      "train loss:0.008978905242647184\n",
      "train loss:0.028976272921980736\n",
      "train loss:0.0037547757344213433\n",
      "train loss:0.014440723856418819\n",
      "train loss:0.010177683311170867\n",
      "train loss:0.0018034297711411905\n",
      "train loss:0.020673796261048007\n",
      "train loss:0.009771789168936168\n",
      "train loss:0.0022058265977737026\n",
      "train loss:0.004225848672784927\n",
      "train loss:0.002085673129682904\n",
      "train loss:0.030574181458156227\n",
      "train loss:0.023303234979628284\n",
      "train loss:0.003400318349006698\n",
      "train loss:0.003782199411239977\n",
      "train loss:0.01772663672676292\n",
      "train loss:0.017404840109473203\n",
      "train loss:0.007861578918150173\n",
      "train loss:0.021696476394519115\n",
      "train loss:0.013523954188907467\n",
      "train loss:0.020658727235975272\n",
      "train loss:0.003765187216848083\n",
      "train loss:0.0014884194792361815\n",
      "train loss:0.005642049286954983\n",
      "train loss:0.008753366820585003\n",
      "train loss:0.004559743945163785\n",
      "train loss:0.0022822288860037533\n",
      "train loss:0.011797782925773118\n",
      "train loss:0.007087906555685934\n",
      "train loss:0.0229484771560865\n",
      "train loss:0.008022753103554341\n",
      "train loss:0.011818933349269369\n",
      "train loss:0.011757980810008617\n",
      "train loss:0.0014506051746441908\n",
      "train loss:0.016593202207129808\n",
      "train loss:0.00456288589947337\n",
      "train loss:0.0036133015474982787\n",
      "train loss:0.00889513362427367\n",
      "train loss:0.003708303145249636\n",
      "train loss:0.0051438755468337415\n",
      "train loss:0.007615916738188498\n",
      "train loss:0.004377501815696539\n",
      "train loss:0.01582285469720749\n",
      "train loss:0.005826224172046184\n",
      "train loss:0.006163610587682573\n",
      "train loss:0.003994223433974545\n",
      "train loss:0.008292613885709808\n",
      "train loss:0.061868933027510835\n",
      "train loss:0.0018319001299690343\n",
      "train loss:0.0008861013964330919\n",
      "train loss:0.033568645049642604\n",
      "train loss:0.003069867836489027\n",
      "train loss:0.004079182291314415\n",
      "train loss:0.0025747247472816632\n",
      "train loss:0.002604339951505879\n",
      "train loss:0.00755464502453232\n",
      "train loss:0.06784134107099189\n",
      "train loss:0.001190306877133966\n",
      "train loss:0.0047412924506529475\n",
      "train loss:0.005608682778825313\n",
      "train loss:0.03703811849624925\n",
      "train loss:0.01803262715174083\n",
      "train loss:0.00839148816820423\n",
      "train loss:0.007376656742605866\n",
      "train loss:0.002695619781378644\n",
      "train loss:0.007305342186975843\n",
      "train loss:0.004064798737374887\n",
      "train loss:0.006849036259114688\n",
      "train loss:0.004755184914112273\n",
      "train loss:0.007301239704101381\n",
      "train loss:0.0029698611485828785\n",
      "train loss:0.012467235744379947\n",
      "train loss:0.007263625819445313\n",
      "train loss:0.005526124514819982\n",
      "train loss:0.0017668562768653404\n",
      "train loss:0.0013296913823465832\n",
      "train loss:0.0030140626304370273\n",
      "train loss:0.0054206217588168544\n",
      "train loss:0.013683191155477092\n",
      "train loss:0.0050855172535555415\n",
      "train loss:0.0037346332046437184\n",
      "train loss:0.009899939073357435\n",
      "train loss:0.01805234283050241\n",
      "train loss:0.011764138624254515\n",
      "train loss:0.0016166808960472925\n",
      "train loss:0.010251105106573247\n",
      "train loss:0.0009777325336640295\n",
      "train loss:0.0015193239445983888\n",
      "train loss:0.00825927447425278\n",
      "train loss:0.009111659215785627\n",
      "train loss:0.0016705828952332635\n",
      "current iter num:  6000\n",
      "=== epoch:11, train acc:0.992, test acc:0.985 ===\n",
      "train loss:0.004272888305132703\n",
      "train loss:0.005804071241425402\n",
      "train loss:0.011151610795574092\n",
      "train loss:0.0072668046210732215\n",
      "train loss:0.0006442582226511408\n",
      "train loss:0.002241096414799619\n",
      "train loss:0.026539065132241038\n",
      "train loss:0.0014842402369466482\n",
      "train loss:0.012895852423082674\n",
      "train loss:0.0014945017550496097\n",
      "train loss:0.0044072506098759194\n",
      "train loss:0.004107625021004915\n",
      "train loss:0.00880630770020119\n",
      "train loss:0.000746613182762052\n",
      "train loss:0.004714117120295274\n",
      "train loss:0.006594071378297626\n",
      "train loss:0.010744120606046887\n",
      "train loss:0.008751867495708766\n",
      "train loss:0.0009767932849039137\n",
      "train loss:0.0031208946298393802\n",
      "train loss:0.0015203260684942036\n",
      "train loss:0.0008819066946929429\n",
      "train loss:0.0059226120028600246\n",
      "train loss:0.003607556491780003\n",
      "train loss:0.0023978657319566345\n",
      "train loss:0.0004436545706259911\n",
      "train loss:0.0015993207446661573\n",
      "train loss:0.002229246953513725\n",
      "train loss:0.0019765066862215933\n",
      "train loss:0.016591900885343167\n",
      "train loss:0.0002159346558647633\n",
      "train loss:0.007673811512692961\n",
      "train loss:0.002955530484650344\n",
      "train loss:0.004397716992378923\n",
      "train loss:0.019499225038542387\n",
      "train loss:0.005714500749464108\n",
      "train loss:0.0034319283653097\n",
      "train loss:0.0008824483073131392\n",
      "train loss:0.0009105475263781852\n",
      "train loss:0.007051783834090986\n",
      "train loss:0.0057509197620565266\n",
      "train loss:0.011911349737717873\n",
      "train loss:0.0013780640362920041\n",
      "train loss:0.0031962837748359925\n",
      "train loss:0.01874912258982505\n",
      "train loss:0.005465346933560569\n",
      "train loss:0.012062794612337096\n",
      "train loss:0.003172006787809256\n",
      "train loss:0.005497928280506862\n",
      "train loss:0.0023107070786030025\n",
      "train loss:0.008252053841433803\n",
      "train loss:0.0013458926123678292\n",
      "train loss:0.012386837055220675\n",
      "train loss:0.0035793652717848678\n",
      "train loss:0.012277579158052897\n",
      "train loss:0.008316915845464233\n",
      "train loss:0.005354778177075677\n",
      "train loss:0.009176890505713742\n",
      "train loss:0.0018931840327202937\n",
      "train loss:0.004733613461269947\n",
      "train loss:0.003583378784585502\n",
      "train loss:0.005027940211879008\n",
      "train loss:0.003342695660745713\n",
      "train loss:0.0028489775530447636\n",
      "train loss:0.005449803038485752\n",
      "train loss:0.0025506936071786734\n",
      "train loss:0.01856602601755948\n",
      "train loss:0.010864592677654867\n",
      "train loss:0.002797294374526093\n",
      "train loss:0.0010277065003733176\n",
      "train loss:0.002560080520004008\n",
      "train loss:0.005900386436626017\n",
      "train loss:0.013189736642599042\n",
      "train loss:0.0022874886411370725\n",
      "train loss:0.002162176563157295\n",
      "train loss:0.0028722130573914785\n",
      "train loss:0.0002565556546187979\n",
      "train loss:0.00034714372285019594\n",
      "train loss:0.00043789214967642653\n",
      "train loss:0.0022543780710334517\n",
      "train loss:0.004401573148256664\n",
      "train loss:0.011215825862872272\n",
      "train loss:0.0016517655032376522\n",
      "train loss:0.0016433487482150249\n",
      "train loss:0.001663456372389985\n",
      "train loss:0.007216540849603421\n",
      "train loss:0.0018400628430715383\n",
      "train loss:0.004862099403552504\n",
      "train loss:0.0008843663732959446\n",
      "train loss:0.002855082698327926\n",
      "train loss:0.01792172259158521\n",
      "train loss:0.0015199730375931157\n",
      "train loss:0.006073329171885592\n",
      "train loss:0.012464035402849403\n",
      "train loss:0.005978258160095412\n",
      "train loss:0.0040150984203425385\n",
      "train loss:0.03559529901895113\n",
      "train loss:0.002612404124726786\n",
      "train loss:0.002038074270368577\n",
      "train loss:0.003392031739489024\n",
      "train loss:0.01648030454800458\n",
      "train loss:0.0010237667195262476\n",
      "train loss:0.010079756323099922\n",
      "train loss:0.0012741212311259884\n",
      "train loss:0.02325044536163197\n",
      "train loss:0.016897031674428385\n",
      "train loss:0.008857553585079572\n",
      "train loss:0.0008147699429744129\n",
      "train loss:0.04591285944015746\n",
      "train loss:0.001507441566776127\n",
      "train loss:0.013243178418111547\n",
      "train loss:0.0044138552544037086\n",
      "train loss:0.039431625320273694\n",
      "train loss:0.0008334978958221693\n",
      "train loss:0.0014971241079992973\n",
      "train loss:0.014142098757125483\n",
      "train loss:0.0011075457248045087\n",
      "train loss:0.018586535560378977\n",
      "train loss:0.0009985088537557945\n",
      "train loss:0.008826313623569289\n",
      "train loss:0.002541111095740367\n",
      "train loss:0.006901419464721769\n",
      "train loss:0.013471855887268413\n",
      "train loss:0.011928122621869179\n",
      "train loss:0.003975865931774731\n",
      "train loss:0.005198374374554473\n",
      "train loss:0.03848062185323323\n",
      "train loss:0.002508846332082932\n",
      "train loss:0.01997024805329155\n",
      "train loss:0.0007881860596816287\n",
      "train loss:0.008336961862508629\n",
      "train loss:0.00091389408993782\n",
      "train loss:0.001260901627044424\n",
      "train loss:0.002794873973411612\n",
      "train loss:0.004593960489690171\n",
      "train loss:0.00131993102522066\n",
      "train loss:0.02299209576412961\n",
      "train loss:0.004522904749273429\n",
      "train loss:0.005996622751738258\n",
      "train loss:0.02382234848891386\n",
      "train loss:0.0020880998045753056\n",
      "train loss:0.0014156906937092375\n",
      "train loss:0.006353114089902478\n",
      "train loss:0.0018151920565276638\n",
      "train loss:0.03749639579242346\n",
      "train loss:0.001353926177402041\n",
      "train loss:0.008439949813741692\n",
      "train loss:0.002576417676951377\n",
      "train loss:0.0021054116641400365\n",
      "train loss:0.002130962871268207\n",
      "train loss:0.000523499701401979\n",
      "train loss:0.009335312341494111\n",
      "train loss:0.001452104630661474\n",
      "train loss:0.0018298202530783604\n",
      "train loss:0.0015874732965962305\n",
      "train loss:0.0019403008455208973\n",
      "train loss:0.009110438627379472\n",
      "train loss:0.017377841296690424\n",
      "train loss:0.010846991693310222\n",
      "train loss:0.009930322295384503\n",
      "train loss:0.0007121745084239398\n",
      "train loss:0.0037856290431155\n",
      "train loss:0.003308090685306513\n",
      "train loss:0.0008899636175813473\n",
      "train loss:0.0043765600322012085\n",
      "train loss:0.0012720435156770152\n",
      "train loss:0.0020504124423032385\n",
      "train loss:0.023219240018916655\n",
      "train loss:0.0006566242642080928\n",
      "train loss:0.004551342495394094\n",
      "train loss:0.004789411334655181\n",
      "train loss:0.003709191398324912\n",
      "train loss:0.0038560369421375357\n",
      "train loss:0.01215741858075855\n",
      "train loss:0.002325748715163198\n",
      "train loss:0.0009380479008019464\n",
      "train loss:0.004488457895236408\n",
      "train loss:0.0015143198586718253\n",
      "train loss:0.0003134153973652503\n",
      "train loss:0.0013053169493408962\n",
      "train loss:0.008655430484361673\n",
      "train loss:0.001946011127558688\n",
      "train loss:0.015326160968850271\n",
      "train loss:0.0034364435673827952\n",
      "train loss:0.0030284595770482163\n",
      "train loss:0.0024089799488020112\n",
      "train loss:0.004722539483043377\n",
      "train loss:0.0011783369714832845\n",
      "train loss:0.0005617446114953832\n",
      "train loss:0.004233959000043057\n",
      "train loss:0.0008814526038136215\n",
      "train loss:0.0032503271056009636\n",
      "train loss:0.0035609613194588917\n",
      "train loss:0.0032641989296219786\n",
      "train loss:0.0003030825520258588\n",
      "train loss:0.0012246234319711396\n",
      "train loss:0.00814471382671599\n",
      "train loss:0.001587613479408187\n",
      "train loss:0.0015132419733916577\n",
      "train loss:0.0063080405758885625\n",
      "train loss:0.0022112603650957497\n",
      "train loss:0.016301296549393655\n",
      "train loss:0.0006823960828814912\n",
      "train loss:0.00036190135831055785\n",
      "train loss:0.0015176317296379162\n",
      "train loss:0.007367116037172588\n",
      "train loss:0.006404043991892081\n",
      "train loss:0.0017359070622364363\n",
      "train loss:0.0021471815907793354\n",
      "train loss:0.0028549922770327767\n",
      "train loss:0.0006813171509235424\n",
      "train loss:0.0020431537654317386\n",
      "train loss:0.0009451550249691765\n",
      "train loss:0.0026068099903702353\n",
      "train loss:0.01018478252551765\n",
      "train loss:0.008496344325038879\n",
      "train loss:0.0037313794965074015\n",
      "train loss:0.00574882096027854\n",
      "train loss:0.003586595156616912\n",
      "train loss:0.0027455826388872555\n",
      "train loss:0.021294459937321326\n",
      "train loss:0.016448905317919583\n",
      "train loss:0.0012362906366022626\n",
      "train loss:0.0031084020550771357\n",
      "train loss:0.0012727572101622513\n",
      "train loss:0.004197502851796342\n",
      "train loss:0.0006072186557099313\n",
      "train loss:0.013544375942659236\n",
      "train loss:0.008691476504314037\n",
      "train loss:0.00160960339470622\n",
      "train loss:0.016839040565390397\n",
      "train loss:0.004191393554756833\n",
      "train loss:0.017176093755758584\n",
      "train loss:0.0013191647521347985\n",
      "train loss:0.0005082107928633259\n",
      "train loss:0.01895868252810576\n",
      "train loss:0.008609779650356074\n",
      "train loss:0.003530578290396691\n",
      "train loss:0.002639252820631183\n",
      "train loss:0.009827355298812656\n",
      "train loss:0.004474926944099417\n",
      "train loss:0.003131040156075932\n",
      "train loss:0.004848199020912289\n",
      "train loss:0.0003706451074795722\n",
      "train loss:0.001584687351811112\n",
      "train loss:0.004520272363070683\n",
      "train loss:0.03171051742662252\n",
      "train loss:0.002978219950258833\n",
      "train loss:0.01580371134051684\n",
      "train loss:0.03859150249280121\n",
      "train loss:0.0062181032596056625\n",
      "train loss:0.002651705722039327\n",
      "train loss:0.002470519031256757\n",
      "train loss:0.001299807492960864\n",
      "train loss:0.00020256291151441382\n",
      "train loss:0.0022468768162237644\n",
      "train loss:0.0193889457668472\n",
      "train loss:0.000395495980085901\n",
      "train loss:0.00254875895690786\n",
      "train loss:0.011032273649798734\n",
      "train loss:0.0012042694799101574\n",
      "train loss:0.009019110528469746\n",
      "train loss:0.002195628032564151\n",
      "train loss:0.010253154435980998\n",
      "train loss:0.0012242181964715127\n",
      "train loss:0.004641980683892753\n",
      "train loss:0.006849096013050643\n",
      "train loss:0.0010013595507517678\n",
      "train loss:0.00791599111229828\n",
      "train loss:0.014621743577070774\n",
      "train loss:0.0031463704569642736\n",
      "train loss:0.00015149977604616733\n",
      "train loss:0.00488549695887627\n",
      "train loss:0.009775710304177085\n",
      "train loss:0.031939439046718664\n",
      "train loss:0.005673612737373058\n",
      "train loss:0.0016679380502025729\n",
      "train loss:0.0035858849380574036\n",
      "train loss:0.002814273906828816\n",
      "train loss:0.004697146767277572\n",
      "train loss:0.023101967804783078\n",
      "train loss:0.025273225154794066\n",
      "train loss:0.001921761592650413\n",
      "train loss:0.021547952333862166\n",
      "train loss:0.003590268093553049\n",
      "train loss:0.008038919211329557\n",
      "train loss:0.0024647170062759034\n",
      "train loss:0.00229821824907575\n",
      "train loss:0.007875296162879582\n",
      "train loss:0.014083075629059088\n",
      "train loss:0.01448115664289697\n",
      "train loss:0.02258961884911745\n",
      "train loss:0.023580551371860072\n",
      "train loss:0.023672599274275473\n",
      "train loss:0.02000834494423006\n",
      "train loss:0.038810667834119454\n",
      "train loss:0.0031738912100609034\n",
      "train loss:0.0021317648784579934\n",
      "train loss:0.016095433754285012\n",
      "train loss:0.01022458493629136\n",
      "train loss:0.006663740366231854\n",
      "train loss:0.002988157673631336\n",
      "train loss:0.0015748660863236655\n",
      "train loss:0.05539643939848721\n",
      "train loss:0.00197140705781266\n",
      "train loss:0.007700937871534724\n",
      "train loss:0.013616187937816344\n",
      "train loss:0.008399635722528742\n",
      "train loss:0.00268137147179609\n",
      "train loss:0.002301128065859528\n",
      "train loss:0.007908706956071617\n",
      "train loss:0.004078172920607733\n",
      "train loss:0.003791267208522478\n",
      "train loss:0.003138892578986086\n",
      "train loss:0.007532280043882551\n",
      "train loss:0.0021340336683832923\n",
      "train loss:0.006036992649372779\n",
      "train loss:0.0042471894072344364\n",
      "train loss:0.0003182606489053768\n",
      "train loss:0.002708882969760155\n",
      "train loss:0.0010183615171859995\n",
      "train loss:0.002897002212580834\n",
      "train loss:0.002118519161557561\n",
      "train loss:0.003835017877853042\n",
      "train loss:0.002277488237709648\n",
      "train loss:0.011963268213603339\n",
      "train loss:0.009173705105114546\n",
      "train loss:0.005494305386117024\n",
      "train loss:0.010746765544005396\n",
      "train loss:0.005379004700815288\n",
      "train loss:0.0007848356617959852\n",
      "train loss:0.008051597053274438\n",
      "train loss:0.0036412251838492133\n",
      "train loss:0.0012885941798696616\n",
      "train loss:0.00612403276677792\n",
      "train loss:0.001768983554441983\n",
      "train loss:0.0007793587724068907\n",
      "train loss:0.013535453750069986\n",
      "train loss:0.031131198649272263\n",
      "train loss:0.0025614955587740357\n",
      "train loss:0.004823905494367018\n",
      "train loss:0.0021521938368899026\n",
      "train loss:0.0021160848938378547\n",
      "train loss:0.007009020555193617\n",
      "train loss:0.0026124260387953326\n",
      "train loss:0.003959365762290932\n",
      "train loss:0.023259146466128647\n",
      "train loss:0.01351376547619055\n",
      "train loss:0.007731923590874473\n",
      "train loss:0.0019393065568863441\n",
      "train loss:0.002758813161548609\n",
      "train loss:0.0034702315017025825\n",
      "train loss:0.00913247581169128\n",
      "train loss:0.005715080144670552\n",
      "train loss:0.008229721356750855\n",
      "train loss:0.004290239188096001\n",
      "train loss:0.01211508430673421\n",
      "train loss:0.004801331774809335\n",
      "train loss:0.0047284114376256615\n",
      "train loss:0.005202526468394101\n",
      "train loss:0.012396776603996901\n",
      "train loss:0.00045875631489838736\n",
      "train loss:0.0024410545164055995\n",
      "train loss:0.0025702049509039697\n",
      "train loss:0.0014289214719647996\n",
      "train loss:0.005602065700203027\n",
      "train loss:0.009989605600165968\n",
      "train loss:0.00307283304542912\n",
      "train loss:0.011086554164310296\n",
      "train loss:0.009528515506587336\n",
      "train loss:0.04894814679372229\n",
      "train loss:0.005167017225265807\n",
      "train loss:0.006571098346834531\n",
      "train loss:0.002624755456661047\n",
      "train loss:0.01695585922657673\n",
      "train loss:0.006696089067615489\n",
      "train loss:0.002022979919126329\n",
      "train loss:0.0018107453306404112\n",
      "train loss:0.001605739536677875\n",
      "train loss:0.007801828783788342\n",
      "train loss:0.0037703547987061104\n",
      "train loss:0.00024704081500432517\n",
      "train loss:0.00885759042366952\n",
      "train loss:0.001350906007435608\n",
      "train loss:0.0008392742013137177\n",
      "train loss:0.0013607052294811533\n",
      "train loss:0.007130959689399743\n",
      "train loss:0.0013026799060157652\n",
      "train loss:0.0031901922171657772\n",
      "train loss:0.0029185225751028297\n",
      "train loss:0.002892897484312604\n",
      "train loss:0.034505305477471854\n",
      "train loss:0.004776496709440459\n",
      "train loss:0.006620180854814932\n",
      "train loss:0.002076022434452926\n",
      "train loss:0.003954267106224294\n",
      "train loss:0.0031754343312054333\n",
      "train loss:0.0014227740275895738\n",
      "train loss:0.002695558144014775\n",
      "train loss:0.006134547423585269\n",
      "train loss:0.0024140331563365022\n",
      "train loss:0.0003677840889595144\n",
      "train loss:0.003617205626110344\n",
      "train loss:0.0034227674444616894\n",
      "train loss:0.0014111387414327884\n",
      "train loss:0.002374981951773158\n",
      "train loss:0.0006445388315862581\n",
      "train loss:0.0022467623925359905\n",
      "train loss:0.003985749944336813\n",
      "train loss:0.005180161424263216\n",
      "train loss:0.002109490189210239\n",
      "train loss:0.0026509544735870434\n",
      "train loss:0.006833282440661518\n",
      "train loss:0.00026170167393239864\n",
      "train loss:0.0020409686041621073\n",
      "train loss:0.005430203726896188\n",
      "train loss:0.001202089420555764\n",
      "train loss:0.0010540556715012681\n",
      "train loss:0.06666314271557094\n",
      "train loss:0.0005037114913492036\n",
      "train loss:0.0045030942537135\n",
      "train loss:0.0015975644291399166\n",
      "train loss:0.002031472674241452\n",
      "train loss:0.01557169625970891\n",
      "train loss:0.005998424831667662\n",
      "train loss:0.0010080483862721893\n",
      "train loss:0.03434039881073429\n",
      "train loss:0.002679815477604738\n",
      "train loss:0.0029627745209225283\n",
      "train loss:0.0030417133032315025\n",
      "train loss:0.0032029949809818125\n",
      "train loss:0.002897749379059806\n",
      "train loss:0.005333517140168122\n",
      "train loss:0.0013244103541963628\n",
      "train loss:0.006253495060720391\n",
      "train loss:0.006422324858767395\n",
      "train loss:0.005125844117732144\n",
      "train loss:0.006037343044373276\n",
      "train loss:0.0019073307110944063\n",
      "train loss:0.0026243731487120415\n",
      "train loss:0.0019276857697309484\n",
      "train loss:0.021331004059557245\n",
      "train loss:0.001699663351707097\n",
      "train loss:0.00162459782447628\n",
      "train loss:0.003475480561241449\n",
      "train loss:0.001510313785499464\n",
      "train loss:0.0017115854160202665\n",
      "train loss:0.0029259231186814726\n",
      "train loss:0.018640035118208106\n",
      "train loss:0.0021284742390743293\n",
      "train loss:0.00236013543056308\n",
      "train loss:0.0011301556078757858\n",
      "train loss:0.0008593421722304595\n",
      "train loss:0.005908311046951611\n",
      "train loss:0.0006636573693597358\n",
      "train loss:0.0023141963621113463\n",
      "train loss:0.001640796139067236\n",
      "train loss:0.0012319016236713913\n",
      "train loss:0.0019737159850364717\n",
      "train loss:0.0013831824484573261\n",
      "train loss:0.004760735591727585\n",
      "train loss:0.012483456756546614\n",
      "train loss:0.0010290885204518499\n",
      "train loss:0.0006282303124265237\n",
      "train loss:0.0012010729926370694\n",
      "train loss:0.0015210464226599837\n",
      "train loss:0.0027458665309803313\n",
      "train loss:0.007125909072563847\n",
      "train loss:0.0024331136635455637\n",
      "train loss:0.03872403842344693\n",
      "train loss:0.004166536885206775\n",
      "train loss:0.002309221068436597\n",
      "train loss:0.0008810485082979924\n",
      "train loss:0.0020268002551395202\n",
      "train loss:0.029358174303528942\n",
      "train loss:0.00036462596315520445\n",
      "train loss:0.0042531750592006805\n",
      "train loss:0.0036619954851818954\n",
      "train loss:0.0026821741559209787\n",
      "train loss:0.0014670624088217743\n",
      "train loss:0.0003723219999884983\n",
      "train loss:0.007478220032918763\n",
      "train loss:0.006259134425611475\n",
      "train loss:0.002831483617012501\n",
      "train loss:0.005112664146616973\n",
      "train loss:0.008050975775865212\n",
      "train loss:0.003211739793829399\n",
      "train loss:0.0002820296228121844\n",
      "train loss:0.0027328591020308983\n",
      "train loss:0.026776535334556737\n",
      "train loss:0.0026210421267864226\n",
      "train loss:0.0030277981526789567\n",
      "train loss:0.003965623190433949\n",
      "train loss:0.0024242931994594165\n",
      "train loss:0.012796607076950874\n",
      "train loss:0.003766296378345778\n",
      "train loss:0.006955550778067203\n",
      "train loss:0.00039489271623076387\n",
      "train loss:0.004383483182107416\n",
      "train loss:0.0049612659637271565\n",
      "train loss:0.0077221497654784066\n",
      "train loss:0.0029059837625389755\n",
      "train loss:0.008585378961358709\n",
      "train loss:0.0012212347411528792\n",
      "train loss:0.0008082701489684499\n",
      "train loss:0.0010520237242262333\n",
      "train loss:0.00906914026047424\n",
      "train loss:0.00016665468421746263\n",
      "train loss:0.00042318654578403053\n",
      "train loss:0.01217517595930434\n",
      "train loss:0.0011570103142082698\n",
      "train loss:0.006766050791334093\n",
      "train loss:0.012024914826581362\n",
      "train loss:0.022573251958029755\n",
      "train loss:0.0038415886463898396\n",
      "train loss:0.008717429943837153\n",
      "train loss:0.004660655265867845\n",
      "train loss:0.0038064534957828683\n",
      "train loss:0.04092495044815785\n",
      "train loss:0.01212104397360028\n",
      "train loss:0.0047073014993256924\n",
      "train loss:0.004405860057348817\n",
      "train loss:0.01065472467174882\n",
      "train loss:0.024991204726543367\n",
      "train loss:0.0045446727128963735\n",
      "train loss:0.008098828872021693\n",
      "train loss:0.009406985491331225\n",
      "train loss:0.015115946492139319\n",
      "train loss:0.008643822736762462\n",
      "train loss:0.001630112352832414\n",
      "train loss:0.011508099829628074\n",
      "train loss:0.006429198308999867\n",
      "train loss:0.0203473490821883\n",
      "train loss:0.0673867476623808\n",
      "train loss:0.010792136732417886\n",
      "train loss:0.004343045354330954\n",
      "train loss:0.013079735871567908\n",
      "train loss:0.004232763217235535\n",
      "train loss:0.005389085850719462\n",
      "train loss:0.0018180846985629323\n",
      "train loss:0.004702979891675442\n",
      "train loss:0.0017271668017652308\n",
      "train loss:0.00040389564313892177\n",
      "train loss:0.006587706006851104\n",
      "train loss:0.0014382851047549025\n",
      "train loss:0.06266147208273778\n",
      "train loss:0.004141135628476443\n",
      "train loss:0.0015258581058521523\n",
      "train loss:0.0022893729233174243\n",
      "train loss:0.009308195881088607\n",
      "train loss:0.011645971533075554\n",
      "train loss:0.009001554767094204\n",
      "train loss:0.004707837596164116\n",
      "train loss:0.0019246237655404287\n",
      "train loss:0.009935056684652905\n",
      "train loss:0.005368541787874236\n",
      "train loss:0.0019557520116515177\n",
      "train loss:0.0039055928191139544\n",
      "train loss:0.013075746939635242\n",
      "train loss:0.0027795040811737213\n",
      "train loss:0.0007862323982210272\n",
      "train loss:0.003181482792284378\n",
      "train loss:0.007242957640632303\n",
      "train loss:0.0005997388858645675\n",
      "train loss:0.0020060247682044023\n",
      "train loss:0.009282212113132965\n",
      "train loss:0.06746018394566479\n",
      "train loss:0.002288289636014704\n",
      "train loss:0.009984197832587138\n",
      "train loss:0.0017605803801698352\n",
      "train loss:0.0004687054557003845\n",
      "train loss:0.004046654811970739\n",
      "train loss:0.005307669736087953\n",
      "train loss:0.0018582187991449325\n",
      "train loss:0.006224304575754519\n",
      "train loss:0.010635970477058705\n",
      "train loss:0.0018143565766369498\n",
      "train loss:0.01726554688146523\n",
      "train loss:0.006184130418291847\n",
      "train loss:0.008622135183029223\n",
      "train loss:0.00374890605334115\n",
      "train loss:0.005223551215954176\n",
      "train loss:0.0022535123413093185\n",
      "train loss:0.006798127499683171\n",
      "train loss:0.004258338718630996\n",
      "train loss:0.009926557042628103\n",
      "train loss:0.0010748820413615872\n",
      "train loss:0.019366583182359214\n",
      "train loss:0.014969385440686565\n",
      "train loss:0.0005511044857013564\n",
      "train loss:0.0027085380579542663\n",
      "train loss:0.000918394006049994\n",
      "train loss:0.0026980626086900387\n",
      "train loss:0.019191314194924917\n",
      "train loss:0.007190716854303301\n",
      "train loss:0.006941055759537925\n",
      "train loss:0.012331452126676015\n",
      "train loss:0.0011945426961516249\n",
      "train loss:0.0016811289871940906\n",
      "train loss:0.0013665096421554834\n",
      "current iter num:  6600\n",
      "=== epoch:12, train acc:0.99, test acc:0.986 ===\n",
      "train loss:0.02810714599316236\n",
      "train loss:0.002110575342615892\n",
      "train loss:0.008507129364249612\n",
      "train loss:0.021293788521443795\n",
      "train loss:0.0008389352420616601\n",
      "train loss:0.03258376806975315\n",
      "train loss:0.0032907360161835487\n",
      "train loss:0.0018844197836158831\n",
      "train loss:0.004086788096219101\n",
      "train loss:0.0016986363031657001\n",
      "train loss:0.014616670825319478\n",
      "train loss:0.00040801337066979497\n",
      "train loss:0.004382896430942953\n",
      "train loss:0.006585951065534139\n",
      "train loss:0.009410171062239413\n",
      "train loss:0.0017585152486093172\n",
      "train loss:0.006096680675163506\n",
      "train loss:0.003859727309659169\n",
      "train loss:0.007676465007492691\n",
      "train loss:0.009631989354829663\n",
      "train loss:0.002138355766565234\n",
      "train loss:0.005425913855052026\n",
      "train loss:0.0012606050930256366\n",
      "train loss:0.0016918472166406187\n",
      "train loss:0.0208397954220249\n",
      "train loss:0.0010165514529051017\n",
      "train loss:0.0032126297430067536\n",
      "train loss:0.0021953458865860195\n",
      "train loss:0.003269386086160177\n",
      "train loss:0.0012328852808468488\n",
      "train loss:0.0027462822597042152\n",
      "train loss:0.0009125030934605453\n",
      "train loss:0.0030747267463230483\n",
      "train loss:0.0009456259489678245\n",
      "train loss:0.0020370629929873665\n",
      "train loss:0.007581178212389551\n",
      "train loss:0.03768337730697744\n",
      "train loss:0.010320075604922072\n",
      "train loss:0.016395330660706246\n",
      "train loss:0.0014502493685679793\n",
      "train loss:0.0022112496945620613\n",
      "train loss:0.007168110183660095\n",
      "train loss:0.002328193859391065\n",
      "train loss:0.006812986109710213\n",
      "train loss:0.011187265507511797\n",
      "train loss:0.011670871662416189\n",
      "train loss:0.020461809006249002\n",
      "train loss:0.0004516408771314731\n",
      "train loss:0.0016950509209657425\n",
      "train loss:0.004129991056511123\n",
      "train loss:0.08641146531072737\n",
      "train loss:0.001994385546452061\n",
      "train loss:0.0018239030018915867\n",
      "train loss:0.001047012151505027\n",
      "train loss:0.011027094640524891\n",
      "train loss:0.0005035658164773852\n",
      "train loss:0.00915258998564348\n",
      "train loss:0.0009419280743107606\n",
      "train loss:0.0008905396016257549\n",
      "train loss:0.0005647151898998245\n",
      "train loss:0.0012476759483341982\n",
      "train loss:0.0019552546597699323\n",
      "train loss:0.004350778481555754\n",
      "train loss:0.010732277845335097\n",
      "train loss:0.005834451626424934\n",
      "train loss:0.0008891972437953283\n",
      "train loss:0.002326879127392014\n",
      "train loss:0.0015636062229791175\n",
      "train loss:0.0048242465748723875\n",
      "train loss:0.004294531148636772\n",
      "train loss:0.015070536840638915\n",
      "train loss:0.0016861839532846226\n",
      "train loss:9.921915323568094e-05\n",
      "train loss:0.010269324495953934\n",
      "train loss:0.0007073059364153945\n",
      "train loss:0.005347633872774749\n",
      "train loss:0.0013738442717762427\n",
      "train loss:0.005026266989175038\n",
      "train loss:0.0031324295912782286\n",
      "train loss:0.009393951942166866\n",
      "train loss:0.0027364633128970934\n",
      "train loss:0.0006345453269367486\n",
      "train loss:0.0037579127935103178\n",
      "train loss:0.010439825056199612\n",
      "train loss:0.0006685966971341315\n",
      "train loss:0.0063000311168713696\n",
      "train loss:0.007084822576816068\n",
      "train loss:0.003940606824293735\n",
      "train loss:0.0007748438062018413\n",
      "train loss:0.0037374636773147485\n",
      "train loss:0.0052749553944042285\n",
      "train loss:0.016620630300690595\n",
      "train loss:0.007850519948379745\n",
      "train loss:0.009361168297742164\n",
      "train loss:0.008536221270563511\n",
      "train loss:0.019603789673222273\n",
      "train loss:0.004111727231447185\n",
      "train loss:0.0017442598973189058\n",
      "train loss:0.009940114739517524\n",
      "train loss:0.005885942581118644\n",
      "train loss:0.0027786763107903617\n",
      "train loss:0.003821744887199345\n",
      "train loss:0.0025899719654183407\n",
      "train loss:0.0024578108323540055\n",
      "train loss:0.0013465383910231785\n",
      "train loss:0.006268943435181019\n",
      "train loss:0.013380756691714304\n",
      "train loss:0.004321115281227207\n",
      "train loss:0.0003003448438586996\n",
      "train loss:0.0045105510436600875\n",
      "train loss:0.0008920726402073991\n",
      "train loss:0.004563506818165312\n",
      "train loss:0.010533676432343948\n",
      "train loss:0.005567481705398859\n",
      "train loss:0.0005125952052783075\n",
      "train loss:0.0048284358206493105\n",
      "train loss:0.00028724051641337015\n",
      "train loss:0.002283010936117243\n",
      "train loss:0.0008472594810866163\n",
      "train loss:0.001866764267896339\n",
      "train loss:0.008459484116779305\n",
      "train loss:0.01178749048617294\n",
      "train loss:0.004801657529544717\n",
      "train loss:0.004785242129543105\n",
      "train loss:0.0010230608526850054\n",
      "train loss:0.0015129585789926752\n",
      "train loss:0.011204237003668556\n",
      "train loss:0.0008964394801807656\n",
      "train loss:0.00173154280767354\n",
      "train loss:0.0007341336855394946\n",
      "train loss:0.0007823084612280997\n",
      "train loss:0.0021686853319468164\n",
      "train loss:0.0099379978921762\n",
      "train loss:0.0010008033001412731\n",
      "train loss:0.0025597107286649433\n",
      "train loss:0.011589844891433639\n",
      "train loss:0.0017172487529412965\n",
      "train loss:0.0012585920993404159\n",
      "train loss:0.03669160978036713\n",
      "train loss:0.0029289497263608384\n",
      "train loss:0.003262801856898817\n",
      "train loss:0.0007147206396629908\n",
      "train loss:0.0027576593709810354\n",
      "train loss:0.0010668440465688267\n",
      "train loss:0.005293592842647687\n",
      "train loss:0.008178270316917672\n",
      "train loss:0.012630315664849987\n",
      "train loss:0.0012131150062274926\n",
      "train loss:0.0018297505462313065\n",
      "train loss:0.0025076181514063715\n",
      "train loss:0.004299633547572438\n",
      "train loss:0.01997646536645513\n",
      "train loss:0.0019708207695047\n",
      "train loss:0.001731737394128781\n",
      "train loss:0.013165292842068022\n",
      "train loss:0.005321313338994132\n",
      "train loss:0.004788970232285641\n",
      "train loss:0.009227990832568714\n",
      "train loss:0.0016079818361584453\n",
      "train loss:0.01228454165150359\n",
      "train loss:0.005599362745594704\n",
      "train loss:0.0007372989377565764\n",
      "train loss:0.002507200362789358\n",
      "train loss:0.0037036499167551035\n",
      "train loss:0.019323817875532733\n",
      "train loss:0.012286346337142404\n",
      "train loss:0.0021429950726317925\n",
      "train loss:0.004610359333662829\n",
      "train loss:0.003503143508011244\n",
      "train loss:0.004569002201634227\n",
      "train loss:0.004971147678197063\n",
      "train loss:0.006628511134195891\n",
      "train loss:0.0018804275709569378\n",
      "train loss:0.008637244865745993\n",
      "train loss:0.0027918350017336687\n",
      "train loss:0.0022713226619292346\n",
      "train loss:0.004247800987870408\n",
      "train loss:0.0029288951405708735\n",
      "train loss:0.0049659012137241\n",
      "train loss:0.0012326697003910666\n",
      "train loss:0.0015285046069849263\n",
      "train loss:0.003988377542983881\n",
      "train loss:0.007732179054146925\n",
      "train loss:0.007443796308034534\n",
      "train loss:0.0023064620481760075\n",
      "train loss:0.0005912113113033634\n",
      "train loss:0.004987986292582355\n",
      "train loss:0.009574806817344626\n",
      "train loss:0.015556543925191487\n",
      "train loss:0.015202808098525969\n",
      "train loss:0.004711933457137368\n",
      "train loss:0.04431956037274345\n",
      "train loss:0.004763091653407685\n",
      "train loss:0.000242073305506296\n",
      "train loss:0.0006305532950457456\n",
      "train loss:0.0010543633450048392\n",
      "train loss:0.0029359154760285667\n",
      "train loss:0.004672211241544769\n",
      "train loss:0.0058553841537213654\n",
      "train loss:0.004958825213057854\n",
      "train loss:0.02152358255968548\n",
      "train loss:0.0027632656056897575\n",
      "train loss:0.0012863214685026611\n",
      "train loss:0.0010738395683344826\n",
      "train loss:0.005467414795099993\n",
      "train loss:0.002098906307923762\n",
      "train loss:0.0022102944823771163\n",
      "train loss:0.0010372952009090654\n",
      "train loss:0.0006131641458907494\n",
      "train loss:0.010094783119931934\n",
      "train loss:0.001032344358621423\n",
      "train loss:0.0007662836923059287\n",
      "train loss:0.0038312057848975734\n",
      "train loss:0.11060528091244932\n",
      "train loss:0.017790939558429527\n",
      "train loss:0.005273290909400224\n",
      "train loss:0.0013273518263085392\n",
      "train loss:0.005981284649479186\n",
      "train loss:0.009740001403509661\n",
      "train loss:0.00593035477595671\n",
      "train loss:0.0017630288660118313\n",
      "train loss:0.0010045581566779712\n",
      "train loss:0.012733488427391271\n",
      "train loss:0.005277039245923839\n",
      "train loss:0.004638178543610004\n",
      "train loss:0.002378376719943979\n",
      "train loss:0.0022664069364905786\n",
      "train loss:0.005166541362331454\n",
      "train loss:0.005367658336499814\n",
      "train loss:0.0006153773235304196\n",
      "train loss:0.005026817436590705\n",
      "train loss:0.007086714585490092\n",
      "train loss:0.0012359704638652581\n",
      "train loss:0.009300948481875196\n",
      "train loss:0.0013776455902993782\n",
      "train loss:0.07688019473584659\n",
      "train loss:0.0028650659995931188\n",
      "train loss:0.005642433296483815\n",
      "train loss:0.006320725720622374\n",
      "train loss:0.0012379011496314795\n",
      "train loss:0.0009018566279777\n",
      "train loss:0.0174531111046077\n",
      "train loss:0.059465665385645954\n",
      "train loss:0.0047788092971291505\n",
      "train loss:0.0035546141480898913\n",
      "train loss:0.001899991708172314\n",
      "train loss:0.0022977160203394957\n",
      "train loss:0.0036316096027671483\n",
      "train loss:0.0017266831368801508\n",
      "train loss:0.0003358918396724204\n",
      "train loss:0.0006900076376761198\n",
      "train loss:0.002959624853114403\n",
      "train loss:0.007903025879053014\n",
      "train loss:0.0012317676307275736\n",
      "train loss:0.0008007640477438564\n",
      "train loss:0.009625115446070082\n",
      "train loss:0.013133372277791842\n",
      "train loss:0.008738823106456677\n",
      "train loss:0.003086091090063428\n",
      "train loss:0.005359469572732548\n",
      "train loss:0.00040163581441580103\n",
      "train loss:0.0015374261491364605\n",
      "train loss:0.010222556900276009\n",
      "train loss:0.0032195888068005676\n",
      "train loss:0.0003424691178474962\n",
      "train loss:0.015932394334480692\n",
      "train loss:0.00040868620682473893\n",
      "train loss:0.006663448058263292\n",
      "train loss:0.002667414489078857\n",
      "train loss:0.00047415242010542103\n",
      "train loss:0.0009989218279764151\n",
      "train loss:0.03286209843502595\n",
      "train loss:0.007402370516940296\n",
      "train loss:0.0014277725786157678\n",
      "train loss:0.0001536131803080895\n",
      "train loss:0.023581917081382207\n",
      "train loss:0.0006668928874656608\n",
      "train loss:0.001196164319702539\n",
      "train loss:0.009436409172411042\n",
      "train loss:0.012527895034927225\n",
      "train loss:0.003134315247109849\n",
      "train loss:0.013641034656648333\n",
      "train loss:0.004328916054280347\n",
      "train loss:0.006701949041130553\n",
      "train loss:0.013817037623141657\n",
      "train loss:0.002770632233496033\n",
      "train loss:0.003980876701569446\n",
      "train loss:0.0037261609743906125\n",
      "train loss:0.014149663348934362\n",
      "train loss:0.004876769251284879\n",
      "train loss:0.0024675983600813346\n",
      "train loss:0.0063478086008418\n",
      "train loss:0.013341822918916325\n",
      "train loss:0.0025947834369829696\n",
      "train loss:0.006652748370008916\n",
      "train loss:0.0001625426867942361\n",
      "train loss:0.000690569677234113\n",
      "train loss:0.0005089344647978623\n",
      "train loss:0.0023358911897469747\n",
      "train loss:0.02288434680097625\n",
      "train loss:0.005065731799125015\n",
      "train loss:0.003103707561840452\n",
      "train loss:0.001389086072299972\n",
      "train loss:0.0008575941857368542\n",
      "train loss:0.0008191502298602307\n",
      "train loss:0.0030809631044095404\n",
      "train loss:0.008073025392885964\n",
      "train loss:0.006512090984954093\n",
      "train loss:0.018206255215772457\n",
      "train loss:0.0017547526989066316\n",
      "train loss:0.004667955373546949\n",
      "train loss:0.00015422429679668733\n",
      "train loss:0.0059455218588025886\n",
      "train loss:0.012665817036731215\n",
      "train loss:0.0021844017825524446\n",
      "train loss:0.0012972772697452822\n",
      "train loss:0.004524111830933136\n",
      "train loss:0.0019583000495981984\n",
      "train loss:0.0007884365795469101\n",
      "train loss:0.00742574980912819\n",
      "train loss:0.0008892803531687207\n",
      "train loss:0.002908983439575174\n",
      "train loss:0.001103086907078221\n",
      "train loss:0.009349516469017103\n",
      "train loss:0.006389627749333175\n",
      "train loss:0.007087052978697887\n",
      "train loss:0.008183122953872649\n",
      "train loss:0.0005093449791699783\n",
      "train loss:0.005437210284917664\n",
      "train loss:0.0007132426295242801\n",
      "train loss:0.0011883110196779392\n",
      "train loss:0.0007862009475722443\n",
      "train loss:0.0006048711828761646\n",
      "train loss:0.005825906825129275\n",
      "train loss:0.0014547161362672852\n",
      "train loss:0.0012812394274784253\n",
      "train loss:0.002302859778985206\n",
      "train loss:0.005316521738996487\n",
      "train loss:0.0006085435113282299\n",
      "train loss:0.00019241041346683194\n",
      "train loss:0.00031931170030287495\n",
      "train loss:0.0006954665527590428\n",
      "train loss:0.007846071492538308\n",
      "train loss:0.001912652424120655\n",
      "train loss:0.002920261674090832\n",
      "train loss:0.0037996775819166394\n",
      "train loss:0.0072938768748326225\n",
      "train loss:0.0007879177537400312\n",
      "train loss:0.023868195473592063\n",
      "train loss:0.008200948919385682\n",
      "train loss:0.003258429708818971\n",
      "train loss:0.0013780114735730065\n",
      "train loss:0.0011637091150057861\n",
      "train loss:0.0038895149447982953\n",
      "train loss:0.002041611627881881\n",
      "train loss:0.027012800190841887\n",
      "train loss:0.001036607836606416\n",
      "train loss:0.002227264006941907\n",
      "train loss:0.007925353757669005\n",
      "train loss:0.03893515449810889\n",
      "train loss:0.0012336422884712409\n",
      "train loss:0.0005131283731971283\n",
      "train loss:0.00323736144368845\n",
      "train loss:0.016129026346020803\n",
      "train loss:0.005531058169290365\n",
      "train loss:0.006865791600910954\n",
      "train loss:0.00845962996077942\n",
      "train loss:0.009262896067834578\n",
      "train loss:0.0021283750694273598\n",
      "train loss:0.0019167165055640478\n",
      "train loss:0.011927638941299456\n",
      "train loss:0.002897536740430072\n",
      "train loss:0.008214520953582313\n",
      "train loss:0.0019114939489393433\n",
      "train loss:0.0007776899297935931\n",
      "train loss:0.0018615722852941767\n",
      "train loss:0.005938392269908518\n",
      "train loss:0.02292315586108629\n",
      "train loss:0.01409527712726918\n",
      "train loss:0.0011447641791031457\n",
      "train loss:0.03407824629551761\n",
      "train loss:0.004786331101275124\n",
      "train loss:0.0005685344289962095\n",
      "train loss:0.0027929674076506844\n",
      "train loss:0.000873793254738836\n",
      "train loss:0.0011448570813692059\n",
      "train loss:0.0013440441849758647\n",
      "train loss:0.00212207148488554\n",
      "train loss:0.0017440274019879603\n",
      "train loss:0.0026740556801053404\n",
      "train loss:0.002896828748565095\n",
      "train loss:0.009745468444017748\n",
      "train loss:0.002830965220720256\n",
      "train loss:0.022955194774796118\n",
      "train loss:0.0027194748181878616\n",
      "train loss:0.003238435405108567\n",
      "train loss:0.0018539881711780368\n",
      "train loss:0.0030328172803701747\n",
      "train loss:0.006792228353643999\n",
      "train loss:0.0002957070863559508\n",
      "train loss:0.005831876082572518\n",
      "train loss:0.0054662266893120955\n",
      "train loss:0.004615669351124421\n",
      "train loss:0.0071423691337750015\n",
      "train loss:0.0004971257411123961\n",
      "train loss:0.0003074018889450079\n",
      "train loss:0.0022724322477365743\n",
      "train loss:0.0033968942122129544\n",
      "train loss:0.00983806716547139\n",
      "train loss:0.020615699329866913\n",
      "train loss:0.002271827401209897\n",
      "train loss:0.0010893223192081933\n",
      "train loss:0.0003025444623746455\n",
      "train loss:0.0006083272815740288\n",
      "train loss:0.0020137768531545276\n",
      "train loss:0.0020834544650565716\n",
      "train loss:0.007130041392142333\n",
      "train loss:0.0027016960756019526\n",
      "train loss:0.004637676458639834\n",
      "train loss:0.044152946606172146\n",
      "train loss:0.0054787240428777395\n",
      "train loss:0.00228157110285817\n",
      "train loss:0.0008431192412286783\n",
      "train loss:0.004787755895870636\n",
      "train loss:0.0028936221137927085\n",
      "train loss:0.0012910121467751585\n",
      "train loss:0.0016665478646686132\n",
      "train loss:0.0012077067195800345\n",
      "train loss:0.0007995684182173354\n",
      "train loss:0.005832516732674049\n",
      "train loss:0.002276582977905339\n",
      "train loss:0.00015575303449355345\n",
      "train loss:0.0029349752855261056\n",
      "train loss:0.003413141697510843\n",
      "train loss:0.0013469352536924824\n",
      "train loss:0.002133049949376778\n",
      "train loss:0.02192777944735486\n",
      "train loss:0.0012099480304755966\n",
      "train loss:0.0003643956248435287\n",
      "train loss:0.009352867032653702\n",
      "train loss:0.001996325835894146\n",
      "train loss:0.006343100474888951\n",
      "train loss:0.002940783064876006\n",
      "train loss:0.0007090003495564957\n",
      "train loss:0.008990873102516294\n",
      "train loss:0.0030752782230995808\n",
      "train loss:0.0031346295447956026\n",
      "train loss:0.003984618385306168\n",
      "train loss:0.008288387775315258\n",
      "train loss:0.024701078717213495\n",
      "train loss:0.013127850101396308\n",
      "train loss:0.0029722136937477216\n",
      "train loss:0.005074487837165544\n",
      "train loss:0.009590665570010233\n",
      "train loss:0.006344894577594042\n",
      "train loss:0.004218254648671801\n",
      "train loss:0.004672443863101272\n",
      "train loss:0.002741742586458783\n",
      "train loss:0.000405707309653858\n",
      "train loss:0.007474381680095412\n",
      "train loss:0.005024443669966384\n",
      "train loss:0.007710260377567121\n",
      "train loss:0.0006591379289899381\n",
      "train loss:0.0014282730392663199\n",
      "train loss:0.0023925123327133107\n",
      "train loss:0.002472932658134731\n",
      "train loss:0.0008472210526652465\n",
      "train loss:0.007140454260279609\n",
      "train loss:0.0005475424534488293\n",
      "train loss:0.0024091229627335093\n",
      "train loss:0.0008162979310529715\n",
      "train loss:0.005370246124312622\n",
      "train loss:0.0008872877959882757\n",
      "train loss:0.002764681565355174\n",
      "train loss:0.0008673339979920301\n",
      "train loss:0.005313194007027587\n",
      "train loss:0.007830163619141062\n",
      "train loss:0.005198307990943796\n",
      "train loss:0.001953609602697212\n",
      "train loss:0.0002761493613745781\n",
      "train loss:0.003799625283305098\n",
      "train loss:0.08905881462940558\n",
      "train loss:0.0005579412226299353\n",
      "train loss:0.015914786352312218\n",
      "train loss:0.0013477279358332524\n",
      "train loss:0.00012171413825306016\n",
      "train loss:0.008778348497821385\n",
      "train loss:0.016708945285166706\n",
      "train loss:0.013040417380883199\n",
      "train loss:0.004967714925731336\n",
      "train loss:0.029656035554879372\n",
      "train loss:0.003799153956181262\n",
      "train loss:0.006464312937722615\n",
      "train loss:0.0021376518258333244\n",
      "train loss:0.009869087754396816\n",
      "train loss:0.013991000101222723\n",
      "train loss:0.007458729884803763\n",
      "train loss:0.017106619751517945\n",
      "train loss:0.0023540775798883066\n",
      "train loss:0.00048609582813087174\n",
      "train loss:0.0027408398920587074\n",
      "train loss:0.0003589130006055242\n",
      "train loss:0.002393250060861206\n",
      "train loss:0.013599296992666298\n",
      "train loss:0.01493600852352474\n",
      "train loss:0.003252770860337501\n",
      "train loss:0.0008317340627617389\n",
      "train loss:0.0036563154612096575\n",
      "train loss:0.0012326311457718704\n",
      "train loss:0.010427748403172303\n",
      "train loss:0.0021775976441713056\n",
      "train loss:0.001267721908013513\n",
      "train loss:0.015319381901155274\n",
      "train loss:0.003181288056331023\n",
      "train loss:0.002393896481208186\n",
      "train loss:0.0026702615836762013\n",
      "train loss:0.013693690560938152\n",
      "train loss:0.004043365677951365\n",
      "train loss:0.0003972854558521261\n",
      "train loss:0.001589083128801781\n",
      "train loss:0.0014957164861547309\n",
      "train loss:0.0036380530052674254\n",
      "train loss:0.00289684512304725\n",
      "train loss:0.0016849265567172274\n",
      "train loss:0.007453238265270879\n",
      "train loss:0.006708972279361018\n",
      "train loss:0.0068000152000026945\n",
      "train loss:0.0019337612342986232\n",
      "train loss:0.001749245419958536\n",
      "train loss:0.0004654576264797309\n",
      "train loss:0.01376339080207271\n",
      "train loss:0.011003531742616455\n",
      "train loss:0.005554595345561308\n",
      "train loss:0.0008653407031993859\n",
      "train loss:0.0011493153039295293\n",
      "train loss:0.0018483500388646915\n",
      "train loss:0.0011405591815015115\n",
      "train loss:0.004862560905638114\n",
      "train loss:0.004562197448083101\n",
      "train loss:0.0007047103717520831\n",
      "train loss:0.0064850921088121585\n",
      "train loss:0.008378803657998679\n",
      "train loss:0.000498562446078769\n",
      "train loss:0.003929297850964643\n",
      "train loss:0.0005360087932390402\n",
      "train loss:0.001460400957067155\n",
      "train loss:0.01780195691443612\n",
      "train loss:0.002617048626933216\n",
      "train loss:0.008661019809799027\n",
      "train loss:0.00795910245902908\n",
      "train loss:0.005033722869726665\n",
      "train loss:0.0017274500598896327\n",
      "train loss:0.0172596363193929\n",
      "train loss:0.0008747357995351237\n",
      "train loss:0.0016555101490629029\n",
      "train loss:0.0016940138363725665\n",
      "train loss:0.05804505370101177\n",
      "train loss:0.0015261373216527373\n",
      "train loss:0.011202789434675446\n",
      "train loss:0.014569374819688664\n",
      "train loss:0.07277416410973285\n",
      "train loss:0.0006181519802763891\n",
      "train loss:0.007129436487349024\n",
      "train loss:0.010337607492738847\n",
      "train loss:0.01630136367377243\n",
      "train loss:0.002933082354127049\n",
      "train loss:0.006192425730479167\n",
      "train loss:0.004816872913481238\n",
      "train loss:0.001270616594233131\n",
      "train loss:0.004851115715388473\n",
      "train loss:0.0026992963673039714\n",
      "train loss:0.0039307614177159535\n",
      "train loss:0.02037645989985631\n",
      "train loss:0.00055379220143487\n",
      "train loss:0.0020304110444863926\n",
      "train loss:0.008543400859381179\n",
      "train loss:0.002648798039548714\n",
      "train loss:0.011738316979599133\n",
      "train loss:0.004187379751196803\n",
      "train loss:0.004222540167489252\n",
      "train loss:0.0017982349718513966\n",
      "train loss:0.0034234393483004756\n",
      "train loss:0.006871787795916994\n",
      "train loss:0.0011037963535912142\n",
      "train loss:0.005944888162036454\n",
      "train loss:0.0036144735004562155\n",
      "train loss:0.004174057921211281\n",
      "train loss:0.0008085447118363835\n",
      "train loss:0.007192559006945606\n",
      "train loss:0.0008347314662730049\n",
      "train loss:0.002352194723367737\n",
      "train loss:0.00607520230209486\n",
      "train loss:0.005318394231881431\n",
      "train loss:0.0028560575783812808\n",
      "train loss:0.00031086743533525124\n",
      "train loss:0.015987970649369947\n",
      "train loss:0.0025388132879287277\n",
      "train loss:0.0020072978761912917\n",
      "train loss:0.0015834379084091093\n",
      "train loss:0.00235180925933032\n",
      "current iter num:  7200\n",
      "=== epoch:13, train acc:0.998, test acc:0.987 ===\n",
      "train loss:0.0057046534232575745\n",
      "train loss:0.002740643435451656\n",
      "train loss:0.0022219465835503836\n",
      "train loss:0.0015704004599414513\n",
      "train loss:0.003558144292683523\n",
      "train loss:0.017862678408716127\n",
      "train loss:0.0027872595094661956\n",
      "train loss:0.0021817489184273228\n",
      "train loss:0.002140956703025064\n",
      "train loss:0.0017076797006789464\n",
      "train loss:0.003609788716160084\n",
      "train loss:0.026963454765298808\n",
      "train loss:0.0031731639119975674\n",
      "train loss:0.0017671987305179684\n",
      "train loss:0.005542774359508956\n",
      "train loss:0.002106497977212813\n",
      "train loss:0.0034892879942480127\n",
      "train loss:0.006962412470221942\n",
      "train loss:0.002440769484849369\n",
      "train loss:0.002019779518623734\n",
      "train loss:0.005928356578581192\n",
      "train loss:0.0031638506984345643\n",
      "train loss:0.00604570860543013\n",
      "train loss:0.0038337684244599424\n",
      "train loss:0.006315510634254134\n",
      "train loss:0.0016343203764398056\n",
      "train loss:0.0006115250411916308\n",
      "train loss:0.0020733992909733427\n",
      "train loss:0.0017820308423881026\n",
      "train loss:0.0010685888416712317\n",
      "train loss:0.005368801511875575\n",
      "train loss:0.0008910141984904234\n",
      "train loss:0.00023976919884184615\n",
      "train loss:0.008045345776768194\n",
      "train loss:0.0031265217159226343\n",
      "train loss:0.007642593978577241\n",
      "train loss:0.0008368785462553295\n",
      "train loss:0.0037341400334707474\n",
      "train loss:0.006964379394225668\n",
      "train loss:0.0009736685050045894\n",
      "train loss:7.634409887691193e-05\n",
      "train loss:0.001420289710557479\n",
      "train loss:0.006790657566739898\n",
      "train loss:0.0008974277280857626\n",
      "train loss:0.016996509179093514\n",
      "train loss:0.004699060110582019\n",
      "train loss:0.0018498925232904484\n",
      "train loss:0.0006194154210850507\n",
      "train loss:0.001957101161371967\n",
      "train loss:0.02046720269117058\n",
      "train loss:0.0035695693866203315\n",
      "train loss:0.0027857920739188975\n",
      "train loss:0.006898382631360908\n",
      "train loss:0.03358940704059804\n",
      "train loss:0.003169020294532377\n",
      "train loss:0.000773685084712406\n",
      "train loss:0.004783315982487447\n",
      "train loss:0.002526171491043422\n",
      "train loss:0.0043670015657363\n",
      "train loss:0.0016821736513193402\n",
      "train loss:0.023457957595472968\n",
      "train loss:0.01449612150896735\n",
      "train loss:0.0005990190796212884\n",
      "train loss:0.0006341036347279696\n",
      "train loss:0.002743264371844208\n",
      "train loss:0.006791722449762016\n",
      "train loss:0.0022788228339190686\n",
      "train loss:0.0005775964767129906\n",
      "train loss:0.0009845122962541149\n",
      "train loss:0.006887620055135937\n",
      "train loss:0.007539137376773589\n",
      "train loss:0.0003182297303505279\n",
      "train loss:0.0075609271992664664\n",
      "train loss:0.0032210901214749597\n",
      "train loss:0.002978153204076798\n",
      "train loss:0.0040188666278907145\n",
      "train loss:0.0011607037602420838\n",
      "train loss:0.002609574578088523\n",
      "train loss:0.0006890697618036312\n",
      "train loss:0.0044562277394532114\n",
      "train loss:0.007635631398702174\n",
      "train loss:0.0008678204673927429\n",
      "train loss:0.0014899377364161858\n",
      "train loss:0.0020349610684059372\n",
      "train loss:0.0016042653168452966\n",
      "train loss:0.00834532652806928\n",
      "train loss:0.0010438863432665907\n",
      "train loss:0.005036675204373741\n",
      "train loss:0.0049216986812903925\n",
      "train loss:0.002988100657157138\n",
      "train loss:0.00537933961872132\n",
      "train loss:0.002409485787050244\n",
      "train loss:0.0020116339754282266\n",
      "train loss:0.010681725352877232\n",
      "train loss:0.0026613383543452706\n",
      "train loss:0.00199506341869591\n",
      "train loss:0.0025107455455653975\n",
      "train loss:0.0024478184068009137\n",
      "train loss:0.004524392507998075\n",
      "train loss:0.009608643478653147\n",
      "train loss:0.0029947757311169286\n",
      "train loss:0.0009931302730553282\n",
      "train loss:0.0036331811274506053\n",
      "train loss:0.00138425338219399\n",
      "train loss:0.0008633720420485254\n",
      "train loss:0.0030976658919184084\n",
      "train loss:0.000939396371937225\n",
      "train loss:0.0009875820892653363\n",
      "train loss:0.008861481372085325\n",
      "train loss:0.00039040444303505554\n",
      "train loss:0.000190504752844799\n",
      "train loss:0.0047686030085031614\n",
      "train loss:0.002993052162079313\n",
      "train loss:0.0027008655591306465\n",
      "train loss:0.010553149391122181\n",
      "train loss:0.0024618155422119377\n",
      "train loss:0.00271840653104841\n",
      "train loss:0.0008499833349431117\n",
      "train loss:0.0018868624089366458\n",
      "train loss:0.0005008096697925186\n",
      "train loss:0.001390304932991476\n",
      "train loss:0.004808002503778874\n",
      "train loss:0.0036355823486324884\n",
      "train loss:0.002194394547228888\n",
      "train loss:0.0016874628176260535\n",
      "train loss:0.0035033116789576725\n",
      "train loss:0.002178437969531466\n",
      "train loss:0.0009406125243010515\n",
      "train loss:0.0013277975905099458\n",
      "train loss:0.005248279775532513\n",
      "train loss:0.002248925738449731\n",
      "train loss:0.00042987756075528455\n",
      "train loss:0.0006278374092919161\n",
      "train loss:0.0031648874142021057\n",
      "train loss:0.00581103334804005\n",
      "train loss:0.0028617038034630983\n",
      "train loss:0.0008430339662803979\n",
      "train loss:0.0001500589781819603\n",
      "train loss:0.02402111172545722\n",
      "train loss:0.0017104790663179414\n",
      "train loss:0.016447900791668236\n",
      "train loss:0.004650089660586735\n",
      "train loss:0.003116691211752192\n",
      "train loss:0.0021550942500107276\n",
      "train loss:0.0009795889351887153\n",
      "train loss:0.002577382660876444\n",
      "train loss:0.004651585777885943\n",
      "train loss:0.0021190913131688438\n",
      "train loss:0.002433212064174023\n",
      "train loss:0.0005515074019003137\n",
      "train loss:0.0009267180284376093\n",
      "train loss:0.0071069698169703086\n",
      "train loss:0.0004379138287657205\n",
      "train loss:0.0021409679311626506\n",
      "train loss:0.0005755414980100431\n",
      "train loss:0.0010787474969703584\n",
      "train loss:0.0037238370029050566\n",
      "train loss:0.015165476228732011\n",
      "train loss:0.0021254448600046385\n",
      "train loss:0.0026135454245867096\n",
      "train loss:0.0009027300718044863\n",
      "train loss:0.0006860980729810116\n",
      "train loss:0.00020870427278928602\n",
      "train loss:0.012708248400895392\n",
      "train loss:0.005853859452609387\n",
      "train loss:0.0004982524327662331\n",
      "train loss:0.007808647220037638\n",
      "train loss:0.002059114957305376\n",
      "train loss:0.0027829735521934236\n",
      "train loss:0.004743083715616699\n",
      "train loss:0.005040974074881524\n",
      "train loss:0.0024228031332051017\n",
      "train loss:0.005108377742753839\n",
      "train loss:0.005624678393894273\n",
      "train loss:0.0007086173271387755\n",
      "train loss:0.003310656495612413\n",
      "train loss:0.0023513240462638544\n",
      "train loss:0.015009043339725317\n",
      "train loss:0.0011451697674315787\n",
      "train loss:0.005456686040445489\n",
      "train loss:0.0019305252704515771\n",
      "train loss:0.013908026132828884\n",
      "train loss:0.005248429956528411\n",
      "train loss:0.0015347595400507072\n",
      "train loss:0.0016742108273008188\n",
      "train loss:0.0030347387008898795\n",
      "train loss:0.011654352010634939\n",
      "train loss:0.002307847688033787\n",
      "train loss:0.0017589393827944944\n",
      "train loss:0.0009650289103071602\n",
      "train loss:0.002963395336986686\n",
      "train loss:0.0010617367948402977\n",
      "train loss:0.0004446244755817625\n",
      "train loss:0.006775631300084044\n",
      "train loss:0.009702713572421832\n",
      "train loss:0.0009972264710732554\n",
      "train loss:0.00032509332299274153\n",
      "train loss:0.004278528965076512\n",
      "train loss:0.0015179439039376308\n",
      "train loss:0.005132525732532909\n",
      "train loss:0.0015263021749797267\n",
      "train loss:0.0018930648380649478\n",
      "train loss:0.008177926114080896\n",
      "train loss:0.0005941736422750126\n",
      "train loss:0.006352007204256473\n",
      "train loss:0.0016894187185538911\n",
      "train loss:0.000308260092145549\n",
      "train loss:0.0009667717950832984\n",
      "train loss:0.0003717677535537873\n",
      "train loss:0.0010805877191761223\n",
      "train loss:0.0004798191088570884\n",
      "train loss:0.0012462878853377434\n",
      "train loss:0.001980116402023407\n",
      "train loss:0.0067464863810768935\n",
      "train loss:0.0024808391606264656\n",
      "train loss:0.00034800109767481004\n",
      "train loss:0.010001799933045714\n",
      "train loss:0.0025869428265477166\n",
      "train loss:0.005240201332269017\n",
      "train loss:0.002482055770078528\n",
      "train loss:0.001133522436822721\n",
      "train loss:0.0035272041549511955\n",
      "train loss:0.0020276070113939464\n",
      "train loss:0.003800680825832672\n",
      "train loss:0.0007799885281634253\n",
      "train loss:0.012487003503881116\n",
      "train loss:0.015236190906193898\n",
      "train loss:0.0015518324321760737\n",
      "train loss:0.0005313389604005181\n",
      "train loss:0.004995483711185449\n",
      "train loss:0.005870464199923148\n",
      "train loss:0.010472472066806208\n",
      "train loss:0.003011937780502434\n",
      "train loss:0.013132480494547204\n",
      "train loss:0.006975274703361534\n",
      "train loss:0.00022045479924736564\n",
      "train loss:0.03006786691422036\n",
      "train loss:0.0024277483343239778\n",
      "train loss:0.009201302434395013\n",
      "train loss:0.007209413935689521\n",
      "train loss:0.008203472770081903\n",
      "train loss:0.007353896759393551\n",
      "train loss:0.0009396951094525996\n",
      "train loss:0.000604014944805598\n",
      "train loss:0.001684669762443875\n",
      "train loss:0.0014075520237640585\n",
      "train loss:0.006594846721086905\n",
      "train loss:0.01024195036611738\n",
      "train loss:0.0016540779171391232\n",
      "train loss:0.014093596936537299\n",
      "train loss:0.0021700474983605256\n",
      "train loss:0.002347634786329047\n",
      "train loss:0.0019212142102919092\n",
      "train loss:0.013358756708583337\n",
      "train loss:0.004320854525395424\n",
      "train loss:0.0018959672227397439\n",
      "train loss:0.001048070005855505\n",
      "train loss:0.0005184944116116507\n",
      "train loss:0.007623348721190961\n",
      "train loss:0.009484966133522826\n",
      "train loss:0.003929776199193714\n",
      "train loss:0.001474839495983074\n",
      "train loss:0.004015358903324482\n",
      "train loss:0.0006655332601838183\n",
      "train loss:0.007378757030460673\n",
      "train loss:0.008071026123667413\n",
      "train loss:0.0013312261179612974\n",
      "train loss:0.019495503454621383\n",
      "train loss:0.0012048759143630195\n",
      "train loss:0.004687119241529632\n",
      "train loss:0.0022751713208835327\n",
      "train loss:0.009279804361686914\n",
      "train loss:0.012345711705175172\n",
      "train loss:0.009257351071142542\n",
      "train loss:0.0038969739497654783\n",
      "train loss:0.0022341321288182306\n",
      "train loss:0.0009792001643487225\n",
      "train loss:0.00742038193111457\n",
      "train loss:0.018936329304478566\n",
      "train loss:0.001344741156142716\n",
      "train loss:0.04803396296707351\n",
      "train loss:0.002113507193100986\n",
      "train loss:0.0011961637421316806\n",
      "train loss:0.011862796495320554\n",
      "train loss:0.0038625791394033297\n",
      "train loss:0.004310761411107536\n",
      "train loss:0.002699814077302417\n",
      "train loss:0.0038444510038405403\n",
      "train loss:0.0038188225527948346\n",
      "train loss:0.0012873694385851717\n",
      "train loss:0.00261384160563937\n",
      "train loss:0.0014310049986576204\n",
      "train loss:0.0008452459513745367\n",
      "train loss:0.0014426097821447\n",
      "train loss:0.0018770733587032671\n",
      "train loss:0.004047156974025754\n",
      "train loss:0.0019981373469513944\n",
      "train loss:0.002418558378081245\n",
      "train loss:0.0003770317024577588\n",
      "train loss:0.0016003581788655763\n",
      "train loss:0.004872190185491995\n",
      "train loss:0.0021954077497636174\n",
      "train loss:0.0007822613135515498\n",
      "train loss:0.007713029717649101\n",
      "train loss:0.00015173408108101811\n",
      "train loss:0.000498248912847047\n",
      "train loss:0.0008401812203604237\n",
      "train loss:0.041902112434454214\n",
      "train loss:0.004688274902972069\n",
      "train loss:0.00045326513348317694\n",
      "train loss:0.00916130798641291\n",
      "train loss:0.005319236922662102\n",
      "train loss:0.0014214373419479903\n",
      "train loss:0.00905823950911201\n",
      "train loss:0.0010145003079507033\n",
      "train loss:0.000560208704568977\n",
      "train loss:0.014942823708222006\n",
      "train loss:0.00503072770607178\n",
      "train loss:0.0008436079097221841\n",
      "train loss:0.0005069272291331489\n",
      "train loss:0.0025380884659656883\n",
      "train loss:0.014881091818865448\n",
      "train loss:0.0006200020509193784\n",
      "train loss:0.0010215566486344702\n",
      "train loss:0.005663830815994002\n",
      "train loss:0.009680878704291424\n",
      "train loss:0.0006753639494730083\n",
      "train loss:0.0053114633957402095\n",
      "train loss:0.0009058304401586986\n",
      "train loss:0.0015998321734272355\n",
      "train loss:0.00999404670935097\n",
      "train loss:0.006983201182304211\n",
      "train loss:0.002527824719117171\n",
      "train loss:0.00018643900710734395\n",
      "train loss:0.0006404570016257314\n",
      "train loss:0.086698524956006\n",
      "train loss:0.004228918631094883\n",
      "train loss:0.0026401735118058794\n",
      "train loss:0.005665940192505225\n",
      "train loss:0.003764467788830586\n",
      "train loss:0.006638771542172809\n",
      "train loss:0.0006750756121370817\n",
      "train loss:0.0031252930601204203\n",
      "train loss:0.06804020085763007\n",
      "train loss:0.018895661785727887\n",
      "train loss:0.004054424193146841\n",
      "train loss:0.02426691024984843\n",
      "train loss:0.02540570580022014\n",
      "train loss:0.0021828326062984797\n",
      "train loss:0.0026234705541680046\n",
      "train loss:0.04545608130956181\n",
      "train loss:0.0021157312051707775\n",
      "train loss:0.002930966116611943\n",
      "train loss:0.0015934285161065174\n",
      "train loss:0.014028688739987067\n",
      "train loss:0.0006548561860634113\n",
      "train loss:0.0012276996212527477\n",
      "train loss:0.032479075441072265\n",
      "train loss:0.011372116451601957\n",
      "train loss:0.0015027161586021497\n",
      "train loss:0.00173804802379273\n",
      "train loss:0.0038796539070826753\n",
      "train loss:0.0013085542692134536\n",
      "train loss:0.011870142444626417\n",
      "train loss:0.0013270314024936391\n",
      "train loss:0.0001419797538056273\n",
      "train loss:0.00671408797666215\n",
      "train loss:0.007616257571564287\n",
      "train loss:0.01157911833370643\n",
      "train loss:0.010026508527866245\n",
      "train loss:0.0031434835707004768\n",
      "train loss:0.01591203619150212\n",
      "train loss:0.004568163740164104\n",
      "train loss:0.0015837718928575053\n",
      "train loss:0.002877679237276014\n",
      "train loss:0.004284351611793511\n",
      "train loss:0.0013331600967885612\n",
      "train loss:0.002272130910965127\n",
      "train loss:0.007356816284387031\n",
      "train loss:0.00179289465614764\n",
      "train loss:0.006445100966650313\n",
      "train loss:0.005263526413731679\n",
      "train loss:0.003954913772248354\n",
      "train loss:0.005212695915387354\n",
      "train loss:0.000586633665777852\n",
      "train loss:0.0013735944922712833\n",
      "train loss:0.002878350500252788\n",
      "train loss:0.011113115340907113\n",
      "train loss:0.0018801734118236626\n",
      "train loss:0.0010383739135062925\n",
      "train loss:0.004657051855345493\n",
      "train loss:0.0034628619931787587\n",
      "train loss:0.003179435365172785\n",
      "train loss:0.002202813451810471\n",
      "train loss:0.00788768783231698\n",
      "train loss:0.0009473158523491175\n",
      "train loss:0.001131522685070298\n",
      "train loss:0.0026384801908256165\n",
      "train loss:0.0030826666366358673\n",
      "train loss:0.0073062683409861615\n",
      "train loss:0.00490497472883517\n",
      "train loss:0.0019205904601632385\n",
      "train loss:0.00962384052567368\n",
      "train loss:0.0018294718865191175\n",
      "train loss:0.001065813286492336\n",
      "train loss:0.004423227880608072\n",
      "train loss:0.009713437121890238\n",
      "train loss:0.0042730032476861746\n",
      "train loss:0.002443728482939048\n",
      "train loss:0.011369306090659542\n",
      "train loss:0.0011679860879867463\n",
      "train loss:0.0041101536323346535\n",
      "train loss:0.005483552239254675\n",
      "train loss:0.000540588855011159\n",
      "train loss:0.0012559298758968037\n",
      "train loss:0.004368036864165155\n",
      "train loss:0.001670686138815673\n",
      "train loss:0.0008410143726868503\n",
      "train loss:0.006305265455183339\n",
      "train loss:0.0038272082325139425\n",
      "train loss:0.006036421902136513\n",
      "train loss:0.00022223954232825056\n",
      "train loss:0.0020972602651746835\n",
      "train loss:0.0017409028633193296\n",
      "train loss:0.0039765389055898374\n",
      "train loss:0.0006499037964096021\n",
      "train loss:0.0006925695490659766\n",
      "train loss:0.0021716877887438513\n",
      "train loss:0.001582746311605744\n",
      "train loss:0.001887120077013443\n",
      "train loss:0.017443671184036273\n",
      "train loss:0.0031041063238324345\n",
      "train loss:0.027360544614163193\n",
      "train loss:0.00018552489512711282\n",
      "train loss:0.0032125582143804742\n",
      "train loss:0.0006652992278566237\n",
      "train loss:0.002572007561992569\n",
      "train loss:0.0009780642846248617\n",
      "train loss:0.005127871309666064\n",
      "train loss:0.0051319667424715954\n",
      "train loss:0.005747424938437934\n",
      "train loss:0.0007730913545047114\n",
      "train loss:0.0006002360484633046\n",
      "train loss:0.0017014712490836003\n",
      "train loss:0.000317223909238989\n",
      "train loss:3.4393758060418414e-05\n",
      "train loss:0.0007339918873379003\n",
      "train loss:0.02400721099438099\n",
      "train loss:0.00026177694159542036\n",
      "train loss:0.006398058890809172\n",
      "train loss:0.0019266102153499718\n",
      "train loss:0.0029502161724420167\n",
      "train loss:0.013469456808763993\n",
      "train loss:0.013308023619223851\n",
      "train loss:0.0037799629480460557\n",
      "train loss:0.0025473022594793703\n",
      "train loss:0.00015040174334519173\n",
      "train loss:0.0021027975100281343\n",
      "train loss:0.0013881704349749265\n",
      "train loss:0.00019906919983567057\n",
      "train loss:0.005260513391172407\n",
      "train loss:0.004892717216994884\n",
      "train loss:0.0020676197302877015\n",
      "train loss:0.00036201441080552357\n",
      "train loss:0.000598172333350284\n",
      "train loss:0.0017513059612159496\n",
      "train loss:0.0060235256851008615\n",
      "train loss:0.0006453746814474808\n",
      "train loss:0.00150220525685232\n",
      "train loss:0.00037845587996828225\n",
      "train loss:0.0010033294690449232\n",
      "train loss:0.002562353257713512\n",
      "train loss:0.06859505765630941\n",
      "train loss:0.0012069007671300432\n",
      "train loss:0.018312532982226612\n",
      "train loss:0.0012802087421189387\n",
      "train loss:0.001907155499576468\n",
      "train loss:0.0003530592834062571\n",
      "train loss:0.006205653269556245\n",
      "train loss:0.0019624316574237436\n",
      "train loss:0.000755274136793486\n",
      "train loss:0.0014949193754617387\n",
      "train loss:0.0006771649798850808\n",
      "train loss:0.001005105060021633\n",
      "train loss:0.04326531297377992\n",
      "train loss:0.0042444878962382485\n",
      "train loss:0.0020444015017788354\n",
      "train loss:0.0019535153202115286\n",
      "train loss:0.0022598644067438737\n",
      "train loss:0.002954310040830514\n",
      "train loss:0.002076856397366836\n",
      "train loss:0.005531225653801607\n",
      "train loss:0.002698187951956201\n",
      "train loss:0.0023301036196123333\n",
      "train loss:0.0006876493663864453\n",
      "train loss:0.0013256442204091488\n",
      "train loss:0.010663702129222968\n",
      "train loss:0.013173219383198235\n",
      "train loss:0.0033488905558247055\n",
      "train loss:0.0024400641044122384\n",
      "train loss:0.028703024195188605\n",
      "train loss:0.003868798550831623\n",
      "train loss:0.002947659954607458\n",
      "train loss:0.010746734998126624\n",
      "train loss:0.009672129836418356\n",
      "train loss:0.006044163715192373\n",
      "train loss:0.003177845697118142\n",
      "train loss:0.0004025008407272579\n",
      "train loss:0.006665962484352048\n",
      "train loss:0.0016182176680941671\n",
      "train loss:0.0034168680645546025\n",
      "train loss:0.0017572682847327276\n",
      "train loss:0.00124154386918872\n",
      "train loss:0.0025424334192748256\n",
      "train loss:0.0002650718143256648\n",
      "train loss:0.0007400494386578256\n",
      "train loss:0.0025193783135636894\n",
      "train loss:0.002360457162180337\n",
      "train loss:0.0014162248599744573\n",
      "train loss:0.006890253539993591\n",
      "train loss:0.0007659851478594185\n",
      "train loss:0.0023590524940187733\n",
      "train loss:0.00453148915918538\n",
      "train loss:0.001742284778480807\n",
      "train loss:0.0012291949390028126\n",
      "train loss:0.0007508951595499565\n",
      "train loss:0.0017855378254868748\n",
      "train loss:0.0037020923587237752\n",
      "train loss:0.0034302452241825963\n",
      "train loss:0.00011651135661844545\n",
      "train loss:0.0019106955347430556\n",
      "train loss:0.0013820616898755867\n",
      "train loss:0.001670698153143546\n",
      "train loss:0.0028316880130472228\n",
      "train loss:0.0069965748195623435\n",
      "train loss:0.0027063167672229695\n",
      "train loss:0.0045664315866560555\n",
      "train loss:0.001842005404670209\n",
      "train loss:0.0013906857335404623\n",
      "train loss:0.0003235867377187477\n",
      "train loss:0.0016620888431374459\n",
      "train loss:0.0021921466937508054\n",
      "train loss:0.003165530139579955\n",
      "train loss:0.029682133672751618\n",
      "train loss:0.015575336649253275\n",
      "train loss:0.0038456919694409997\n",
      "train loss:0.0032812251163270465\n",
      "train loss:0.009597840374518947\n",
      "train loss:0.000563087373779032\n",
      "train loss:0.0023900767578048164\n",
      "train loss:0.001984577171254892\n",
      "train loss:0.004676803075513887\n",
      "train loss:0.013529388887406473\n",
      "train loss:0.0030359990693950563\n",
      "train loss:0.004631182646915554\n",
      "train loss:0.004877952729791019\n",
      "train loss:0.0021750427543407607\n",
      "train loss:0.00229421632914585\n",
      "train loss:0.0030987155416964795\n",
      "train loss:0.0012932411594377153\n",
      "train loss:0.0032283049670447914\n",
      "train loss:0.0006816969767126816\n",
      "train loss:0.0020799642898593697\n",
      "train loss:0.004569477088273444\n",
      "train loss:0.0006806380933365014\n",
      "train loss:0.0023698918915224416\n",
      "train loss:0.002246642846457152\n",
      "train loss:0.0028937937826237275\n",
      "train loss:0.005795702466517122\n",
      "train loss:0.0036889381922414442\n",
      "train loss:0.002019047010686859\n",
      "train loss:0.002989465087783702\n",
      "train loss:0.003902554870351488\n",
      "train loss:0.001577001378287706\n",
      "train loss:0.006533450648360803\n",
      "train loss:0.00031150664588747796\n",
      "train loss:0.0002581503640456931\n",
      "train loss:0.0016569018503290835\n",
      "train loss:0.0033176814392664182\n",
      "train loss:0.006692434145418459\n",
      "train loss:0.004345378842347433\n",
      "train loss:0.002594778391009903\n",
      "train loss:0.0028759989736009974\n",
      "train loss:0.004251848029764253\n",
      "train loss:0.0017013946026294608\n",
      "train loss:0.0028979345880443446\n",
      "train loss:0.00047281066020909046\n",
      "train loss:0.005328741134434228\n",
      "train loss:0.0015552441637129016\n",
      "train loss:0.010174598050529287\n",
      "train loss:0.0007310351224726475\n",
      "train loss:0.0009627505098739335\n",
      "train loss:0.0023409394155172983\n",
      "train loss:0.005877006950157415\n",
      "train loss:0.0063515105360780425\n",
      "train loss:0.006881630237066461\n",
      "train loss:0.002165578408631103\n",
      "train loss:0.0011440672588733805\n",
      "train loss:0.002794926388677993\n",
      "train loss:0.001357551300821395\n",
      "current iter num:  7800\n",
      "=== epoch:14, train acc:0.997, test acc:0.987 ===\n",
      "train loss:0.00035788321137709\n",
      "train loss:0.0009577796706011659\n",
      "train loss:0.0008493868870839199\n",
      "train loss:0.019884025919665343\n",
      "train loss:0.0008139882057549994\n",
      "train loss:0.0007520222774274127\n",
      "train loss:0.00010539269448413541\n",
      "train loss:0.0001889043165808019\n",
      "train loss:0.001957499757837155\n",
      "train loss:0.0028222094852390954\n",
      "train loss:0.0023303787213333786\n",
      "train loss:0.004933000308115949\n",
      "train loss:0.0004794437953245355\n",
      "train loss:0.002265852048560975\n",
      "train loss:0.009556420398873399\n",
      "train loss:0.0035807137043688477\n",
      "train loss:0.0028130813811694387\n",
      "train loss:0.00587997658437323\n",
      "train loss:0.017410592199812988\n",
      "train loss:0.002387810686766431\n",
      "train loss:0.010238831049491496\n",
      "train loss:0.002198663909206861\n",
      "train loss:0.0003267415682409381\n",
      "train loss:0.0009333340981916159\n",
      "train loss:0.001995885768869976\n",
      "train loss:0.004321975197345243\n",
      "train loss:0.001590580553045975\n",
      "train loss:0.0008999592851885632\n",
      "train loss:0.00016788948267238622\n",
      "train loss:0.00047719529058387817\n",
      "train loss:0.0003852725399043431\n",
      "train loss:0.001332684469442989\n",
      "train loss:0.000711953038775855\n",
      "train loss:0.0008279569412351065\n",
      "train loss:0.0030682051800984722\n",
      "train loss:0.0007881723768292366\n",
      "train loss:0.0006460287258690979\n",
      "train loss:0.0017041564097182219\n",
      "train loss:0.0029305418454604916\n",
      "train loss:0.010940147704878783\n",
      "train loss:0.0013730115421903854\n",
      "train loss:0.0016746099234057302\n",
      "train loss:0.003721421105532954\n",
      "train loss:0.002505499717045568\n",
      "train loss:0.0017876174670960496\n",
      "train loss:0.0028337277074014255\n",
      "train loss:0.0023147020323068842\n",
      "train loss:0.00048190780330335673\n",
      "train loss:0.004588358809089137\n",
      "train loss:0.0012842472063838337\n",
      "train loss:0.0017339945497609416\n",
      "train loss:0.0009551088578122313\n",
      "train loss:0.001389531275454624\n",
      "train loss:0.0014930219294962764\n",
      "train loss:0.0034999877666210166\n",
      "train loss:0.0016283821908012928\n",
      "train loss:0.004433888908330004\n",
      "train loss:0.0009462231317613398\n",
      "train loss:0.08938537243087881\n",
      "train loss:0.019324218888674125\n",
      "train loss:0.007609086034856168\n",
      "train loss:0.0034411597728915243\n",
      "train loss:0.004168941300042638\n",
      "train loss:0.0006663677268329457\n",
      "train loss:0.0035434610796564463\n",
      "train loss:0.0034933877602805123\n",
      "train loss:0.002312946425688639\n",
      "train loss:0.006864117672232395\n",
      "train loss:0.001032274315292646\n",
      "train loss:0.002789909049928227\n",
      "train loss:0.0005437408134519352\n",
      "train loss:0.0035446566746641793\n",
      "train loss:0.0014825350506467577\n",
      "train loss:0.032928692628265664\n",
      "train loss:0.001373016731537805\n",
      "train loss:0.00017830123791717117\n",
      "train loss:0.0005123974925842708\n",
      "train loss:0.0006782866745181483\n",
      "train loss:0.0011027133775930798\n",
      "train loss:0.0071671472622182745\n",
      "train loss:0.0005146748686763413\n",
      "train loss:0.0010606941770515057\n",
      "train loss:0.0010411398364922086\n",
      "train loss:0.004902597087015642\n",
      "train loss:0.0036462867373092574\n",
      "train loss:0.0009651169628599831\n",
      "train loss:0.0007225670260253317\n",
      "train loss:0.0025372359739599475\n",
      "train loss:0.002871040118188551\n",
      "train loss:0.0009801477833803383\n",
      "train loss:0.008913938991819912\n",
      "train loss:0.001894260442451324\n",
      "train loss:0.0006391526996659637\n",
      "train loss:0.0010449765271451689\n",
      "train loss:0.00880868312468709\n",
      "train loss:0.010205110719477441\n",
      "train loss:0.00900966127504799\n",
      "train loss:0.004960471682517793\n",
      "train loss:0.002217481788813641\n",
      "train loss:0.001984294185960566\n",
      "train loss:0.001938120033266628\n",
      "train loss:0.004693853963510599\n",
      "train loss:0.001174314682085795\n",
      "train loss:0.0004450906884510941\n",
      "train loss:0.003562739959978502\n",
      "train loss:0.0015644701455452515\n",
      "train loss:0.0024097247315182056\n",
      "train loss:0.0022236547775919273\n",
      "train loss:0.0026713850605504896\n",
      "train loss:0.0017921078574755622\n",
      "train loss:0.0013344484529289743\n",
      "train loss:0.008157059225258579\n",
      "train loss:0.002909298148473106\n",
      "train loss:0.0013502881621891677\n",
      "train loss:0.001301660351987568\n",
      "train loss:0.009339170113856675\n",
      "train loss:0.0009047261811892315\n",
      "train loss:0.0052330483128425085\n",
      "train loss:0.0022530266816346597\n",
      "train loss:0.0015185304361941753\n",
      "train loss:0.00871062588625402\n",
      "train loss:0.0015674718453413047\n",
      "train loss:0.0026360421656685317\n",
      "train loss:0.002216966681521431\n",
      "train loss:0.010900378296344453\n",
      "train loss:0.0008974400257400077\n",
      "train loss:0.00019070107182691748\n",
      "train loss:0.00392580069664975\n",
      "train loss:0.005910334858977375\n",
      "train loss:0.0007663160250329598\n",
      "train loss:0.0019338150010499767\n",
      "train loss:0.010367481454625745\n",
      "train loss:0.0024052367419766406\n",
      "train loss:0.001161317337813332\n",
      "train loss:0.0005653508247099559\n",
      "train loss:0.0015001024171307203\n",
      "train loss:0.0027945396422126745\n",
      "train loss:0.001342307277197247\n",
      "train loss:0.002558871301349422\n",
      "train loss:0.003689028797274638\n",
      "train loss:0.0014410469986119313\n",
      "train loss:0.006435642107085541\n",
      "train loss:0.0026237208502469674\n",
      "train loss:0.0006261877738038074\n",
      "train loss:0.00021858295739782708\n",
      "train loss:0.0012901134868472425\n",
      "train loss:0.0011856641152741731\n",
      "train loss:0.0008275966741152897\n",
      "train loss:0.006092821693556464\n",
      "train loss:0.005247577180402134\n",
      "train loss:0.001403779447107319\n",
      "train loss:0.00036834765075804027\n",
      "train loss:0.00030359768219705293\n",
      "train loss:0.0015601462583464158\n",
      "train loss:0.011419775293364043\n",
      "train loss:0.00011720267345036284\n",
      "train loss:0.006009706845370482\n",
      "train loss:0.0022535826693514728\n",
      "train loss:0.0020595592376866315\n",
      "train loss:0.00559371257988974\n",
      "train loss:0.0034308561671099534\n",
      "train loss:0.006206469188015281\n",
      "train loss:0.0023757675094564536\n",
      "train loss:0.00799095728287177\n",
      "train loss:0.0010280558014226467\n",
      "train loss:0.004228420193474496\n",
      "train loss:0.004612467569782175\n",
      "train loss:0.001840029040850984\n",
      "train loss:0.0015576724335875029\n",
      "train loss:0.0007577743195069128\n",
      "train loss:0.005209451027001342\n",
      "train loss:0.0008413037679616211\n",
      "train loss:0.001491332432921764\n",
      "train loss:0.04145219698157421\n",
      "train loss:0.0035561406042520015\n",
      "train loss:0.0039360545410362414\n",
      "train loss:0.0009655580062488395\n",
      "train loss:0.0021989012707945214\n",
      "train loss:0.0020411643976479825\n",
      "train loss:0.0036369779455512775\n",
      "train loss:0.0032549729988338837\n",
      "train loss:0.0026930810300232443\n",
      "train loss:0.0007874288793405454\n",
      "train loss:0.006678518790983885\n",
      "train loss:0.0017039094635833386\n",
      "train loss:0.0022074198496248886\n",
      "train loss:0.0014805880404149742\n",
      "train loss:0.0004927997742034318\n",
      "train loss:0.005059103923394627\n",
      "train loss:0.002137150578266487\n",
      "train loss:0.0026112170665856136\n",
      "train loss:0.01874563594548211\n",
      "train loss:0.00043344651474553177\n",
      "train loss:0.003282066693800578\n",
      "train loss:0.001209536987478619\n",
      "train loss:0.0033356548669325557\n",
      "train loss:0.003476621101425686\n",
      "train loss:0.03005773723635087\n",
      "train loss:0.0005439012843046414\n",
      "train loss:0.011978710554032442\n",
      "train loss:0.0035481156328154577\n",
      "train loss:0.0016644708544552172\n",
      "train loss:0.005495277154974535\n",
      "train loss:0.006344479033922581\n",
      "train loss:0.0008824813696078471\n",
      "train loss:0.007608088755724816\n",
      "train loss:0.007198723288856073\n",
      "train loss:0.00783871499193246\n",
      "train loss:0.006535285912420261\n",
      "train loss:0.0018656943140040667\n",
      "train loss:0.0004709594820415671\n",
      "train loss:0.003477205355013874\n",
      "train loss:0.003962871334770049\n",
      "train loss:0.0017676642601995873\n",
      "train loss:0.0021561651746275696\n",
      "train loss:0.000664473883178436\n",
      "train loss:0.002633750874759342\n",
      "train loss:0.011293939789673042\n",
      "train loss:0.0014266524876441467\n",
      "train loss:0.008247159439209326\n",
      "train loss:0.005094348017423738\n",
      "train loss:0.011924922912167443\n",
      "train loss:0.004255911694986354\n",
      "train loss:0.0005674804916979073\n",
      "train loss:0.0006150463054695147\n",
      "train loss:0.0004200158449595431\n",
      "train loss:0.0003629194791798808\n",
      "train loss:0.006141643754791664\n",
      "train loss:0.00056129852989084\n",
      "train loss:0.0004274145969075593\n",
      "train loss:0.0009917539439563458\n",
      "train loss:0.0001683756484761391\n",
      "train loss:0.005497622001394669\n",
      "train loss:0.00047417925889743667\n",
      "train loss:0.0013430478360854353\n",
      "train loss:0.0008723241988456901\n",
      "train loss:0.0021108383621978224\n",
      "train loss:0.010694676808255588\n",
      "train loss:0.0015326855304635722\n",
      "train loss:0.005606458186638539\n",
      "train loss:0.0041213133274556535\n",
      "train loss:0.0003734564427685489\n",
      "train loss:0.0031998861096974567\n",
      "train loss:4.291430162450943e-05\n",
      "train loss:0.0002959557479569816\n",
      "train loss:0.0033968076670154294\n",
      "train loss:0.001077845883103597\n",
      "train loss:0.0011311697037558717\n",
      "train loss:0.0010303292522062977\n",
      "train loss:0.003713400238404229\n",
      "train loss:0.0006654892770883003\n",
      "train loss:0.0011675830614308051\n",
      "train loss:0.0021539846162033304\n",
      "train loss:0.00020167528463399534\n",
      "train loss:0.0011362475890842282\n",
      "train loss:0.005775202950503412\n",
      "train loss:0.0024146735068527707\n",
      "train loss:0.0013054141497153132\n",
      "train loss:0.003832416865111392\n",
      "train loss:0.0005663853628407433\n",
      "train loss:0.004825989009642592\n",
      "train loss:0.006861115869124822\n",
      "train loss:0.0023176593941064963\n",
      "train loss:0.020547986626831855\n",
      "train loss:0.0020561893414417125\n",
      "train loss:0.0004080764294576169\n",
      "train loss:0.0028516340147381075\n",
      "train loss:0.002183646021978639\n",
      "train loss:0.0005413993349236852\n",
      "train loss:0.002170781329725847\n",
      "train loss:0.00046911074133488976\n",
      "train loss:0.0029794183288685714\n",
      "train loss:0.000560363582640884\n",
      "train loss:0.0011820584696289604\n",
      "train loss:0.0024147321937437625\n",
      "train loss:0.0011151749089382728\n",
      "train loss:0.004389463807555255\n",
      "train loss:0.0006641010348768038\n",
      "train loss:0.013400798337590884\n",
      "train loss:0.0006046296663455388\n",
      "train loss:0.004677887734170471\n",
      "train loss:0.0006936691843093823\n",
      "train loss:0.0011175523797394551\n",
      "train loss:0.003042052887740829\n",
      "train loss:0.000786965865391278\n",
      "train loss:0.0004136741743474916\n",
      "train loss:0.0006670576339997074\n",
      "train loss:0.0033137605389650083\n",
      "train loss:0.004195981949970237\n",
      "train loss:0.0003718328224983226\n",
      "train loss:0.0008452841620931753\n",
      "train loss:0.003310828240279722\n",
      "train loss:0.0008191898410743792\n",
      "train loss:0.0012188684243159913\n",
      "train loss:0.01906650301463728\n",
      "train loss:0.0005560842513233198\n",
      "train loss:0.029954698609318285\n",
      "train loss:0.0006503659184007617\n",
      "train loss:0.00279392871483113\n",
      "train loss:0.004877717687965361\n",
      "train loss:0.000777924893894321\n",
      "train loss:0.00021368636487529446\n",
      "train loss:0.004594595671521285\n",
      "train loss:0.018859048203228954\n",
      "train loss:0.0011839914938666088\n",
      "train loss:0.007516877945743124\n",
      "train loss:0.0017606787937582643\n",
      "train loss:0.0017571947571276255\n",
      "train loss:0.005209568729369129\n",
      "train loss:0.006318120101519378\n",
      "train loss:0.0055211722765620905\n",
      "train loss:0.0030285118425668533\n",
      "train loss:0.0016730768097602342\n",
      "train loss:0.0007911408386996149\n",
      "train loss:0.00020321163180325936\n",
      "train loss:0.00588726299362059\n",
      "train loss:0.0021967050433665865\n",
      "train loss:0.0007318600580950552\n",
      "train loss:0.0006941597279356108\n",
      "train loss:0.002587823790249753\n",
      "train loss:0.002534670975452531\n",
      "train loss:0.0041576115008808544\n",
      "train loss:0.004361867910012801\n",
      "train loss:0.001532679245032158\n",
      "train loss:0.004044498654739974\n",
      "train loss:0.0027261499486528895\n",
      "train loss:0.004222452202103558\n",
      "train loss:0.0005717650277363511\n",
      "train loss:0.0015130853839539183\n",
      "train loss:0.00031498289240173975\n",
      "train loss:0.005240815715369165\n",
      "train loss:0.0034683570634359644\n",
      "train loss:0.0034628723462504724\n",
      "train loss:0.0060815779072743\n",
      "train loss:0.008114721603691746\n",
      "train loss:0.005212857900415053\n",
      "train loss:0.004675406539297055\n",
      "train loss:0.0005755594791540053\n",
      "train loss:0.000811961676534109\n",
      "train loss:0.000872328172493054\n",
      "train loss:0.000704096219855925\n",
      "train loss:0.0015159221422506896\n",
      "train loss:0.0015260397120646777\n",
      "train loss:0.004225760681893222\n",
      "train loss:0.007465235516132339\n",
      "train loss:0.0035922387283552283\n",
      "train loss:0.005770790283600134\n",
      "train loss:0.003978241890041429\n",
      "train loss:0.0015943337116088663\n",
      "train loss:0.0015125440841136251\n",
      "train loss:0.003230581945122238\n",
      "train loss:0.004960658429496191\n",
      "train loss:0.0040261186315163995\n",
      "train loss:0.0022025588302930854\n",
      "train loss:0.005091277268135709\n",
      "train loss:0.0005103554350856487\n",
      "train loss:0.0018133208408893208\n",
      "train loss:0.004390503167593538\n",
      "train loss:0.006352108319981346\n",
      "train loss:0.0020897701091688804\n",
      "train loss:0.000586787712435613\n",
      "train loss:0.0013803561038800522\n",
      "train loss:0.005748133779036184\n",
      "train loss:0.0009947813633774244\n",
      "train loss:0.005306767022122424\n",
      "train loss:0.0024656204984841385\n",
      "train loss:0.002153143409028063\n",
      "train loss:0.0013133763979622562\n",
      "train loss:0.002087181149841776\n",
      "train loss:0.0016276060300036796\n",
      "train loss:0.0023916969553403446\n",
      "train loss:0.0012325128027003574\n",
      "train loss:0.001244759275899393\n",
      "train loss:0.0007700442938706879\n",
      "train loss:0.0015685695793271536\n",
      "train loss:0.0005644107235637094\n",
      "train loss:0.0039040670848296205\n",
      "train loss:0.006363919998719656\n",
      "train loss:0.002043157499334106\n",
      "train loss:0.005877730896003991\n",
      "train loss:0.0018137168708331991\n",
      "train loss:0.0021695873108747393\n",
      "train loss:0.0015722371249183827\n",
      "train loss:0.0010224903764643495\n",
      "train loss:0.00423196175385197\n",
      "train loss:0.00026236262769974297\n",
      "train loss:0.0008935307405063138\n",
      "train loss:0.007740151752433442\n",
      "train loss:0.002830624224389218\n",
      "train loss:0.0011921753438933526\n",
      "train loss:0.006176337923220659\n",
      "train loss:0.003383857179507741\n",
      "train loss:0.0008356832825111922\n",
      "train loss:0.0020347432483220607\n",
      "train loss:0.0038890149194841044\n",
      "train loss:0.0010297847084144065\n",
      "train loss:0.0008370802023775456\n",
      "train loss:0.00775397739600125\n",
      "train loss:0.0007448247329326015\n",
      "train loss:0.005488879859674706\n",
      "train loss:0.004725589771150472\n",
      "train loss:0.0022071843210550446\n",
      "train loss:0.0021107664730787945\n",
      "train loss:0.0010462383400672351\n",
      "train loss:0.021735335221818062\n",
      "train loss:0.017389017498773262\n",
      "train loss:0.0027451900656896644\n",
      "train loss:0.004245011610708833\n",
      "train loss:0.0036750177498017893\n",
      "train loss:0.011677628993249334\n",
      "train loss:0.0004020613835326433\n",
      "train loss:0.0060378116694345925\n",
      "train loss:0.011416890846395864\n",
      "train loss:0.004580711569687246\n",
      "train loss:0.007875751558102212\n",
      "train loss:0.003235406738480142\n",
      "train loss:0.00040677849181307215\n",
      "train loss:0.005323961632453463\n",
      "train loss:0.00454713213597745\n",
      "train loss:0.013259392279259246\n",
      "train loss:0.005113242773854738\n",
      "train loss:0.0012956347314762799\n",
      "train loss:0.011695595620581643\n",
      "train loss:0.0006290571412387682\n",
      "train loss:0.010113452521110169\n",
      "train loss:0.0007919505978089457\n",
      "train loss:0.0016566514148166494\n",
      "train loss:0.002801706961129032\n",
      "train loss:0.02032565559145892\n",
      "train loss:0.004473451719518401\n",
      "train loss:0.018056608232560868\n",
      "train loss:0.00323536463712406\n",
      "train loss:0.000871176892370731\n",
      "train loss:0.003970727743245603\n",
      "train loss:0.0019241308380488127\n",
      "train loss:0.000986838191161608\n",
      "train loss:0.0062833444302277655\n",
      "train loss:0.0026689776932791705\n",
      "train loss:0.005495097058853796\n",
      "train loss:0.02752872903123468\n",
      "train loss:0.0022063931174262762\n",
      "train loss:0.004820471798910247\n",
      "train loss:0.006086024975648894\n",
      "train loss:0.0021365896549946656\n",
      "train loss:0.0016740387418707292\n",
      "train loss:0.006924032425535036\n",
      "train loss:0.001684997653945556\n",
      "train loss:0.0022332204562948506\n",
      "train loss:0.007388543731600361\n",
      "train loss:0.004270291599835842\n",
      "train loss:0.009510466170779412\n",
      "train loss:0.021104164902334358\n",
      "train loss:0.0012722914146273512\n",
      "train loss:0.0005093721713086097\n",
      "train loss:0.03567664005946704\n",
      "train loss:0.003696451732327278\n",
      "train loss:0.001692516946894784\n",
      "train loss:0.0026201263865386144\n",
      "train loss:0.002866100680705899\n",
      "train loss:0.0004442379300620253\n",
      "train loss:0.00558165225655017\n",
      "train loss:0.0039203861672437385\n",
      "train loss:0.046922591267638696\n",
      "train loss:0.0022089082981477893\n",
      "train loss:0.0008113890335436008\n",
      "train loss:0.001750054305911531\n",
      "train loss:0.0014825098462954158\n",
      "train loss:0.0018027937645672858\n",
      "train loss:0.0002586354173444543\n",
      "train loss:0.0021096476213334297\n",
      "train loss:0.008477410870520445\n",
      "train loss:0.0026883204598425426\n",
      "train loss:0.0030284511212074596\n",
      "train loss:0.003291815755846663\n",
      "train loss:0.006409680878176393\n",
      "train loss:0.008986519069943705\n",
      "train loss:0.0037682922716210064\n",
      "train loss:0.0008477065267948713\n",
      "train loss:0.00028154248683604784\n",
      "train loss:0.0033340294857343274\n",
      "train loss:0.011526668830640208\n",
      "train loss:0.002519359494320682\n",
      "train loss:0.0014024651999186139\n",
      "train loss:0.005686302623591259\n",
      "train loss:0.0036652538211512853\n",
      "train loss:0.0009723823333542662\n",
      "train loss:0.00014059475010027987\n",
      "train loss:0.006757008074483648\n",
      "train loss:0.0034334039594819082\n",
      "train loss:0.0055214823284420985\n",
      "train loss:0.0015222680317181314\n",
      "train loss:0.009142793370149947\n",
      "train loss:0.0007762323190918438\n",
      "train loss:0.0004389919904051116\n",
      "train loss:0.0050994364378262325\n",
      "train loss:0.0035651463893404674\n",
      "train loss:0.00326645800141655\n",
      "train loss:0.003256306126748444\n",
      "train loss:0.00686280557535501\n",
      "train loss:0.017520737257987008\n",
      "train loss:0.0007246469585188727\n",
      "train loss:0.006533361799688605\n",
      "train loss:0.0013833815124385368\n",
      "train loss:0.0012277662804126483\n",
      "train loss:0.006310481476915855\n",
      "train loss:0.0014540167651328226\n",
      "train loss:0.00073325282852318\n",
      "train loss:0.00048184776786522934\n",
      "train loss:0.004711081288815961\n",
      "train loss:0.0029393887230294204\n",
      "train loss:0.0019275306096267952\n",
      "train loss:0.003798294165776723\n",
      "train loss:0.005396063651811731\n",
      "train loss:0.001230321163131343\n",
      "train loss:0.0008765033623130118\n",
      "train loss:0.00418166244972494\n",
      "train loss:0.0025753798333070684\n",
      "train loss:0.0025156404040736972\n",
      "train loss:0.010125088893449943\n",
      "train loss:0.008316798233885519\n",
      "train loss:0.002282922210252595\n",
      "train loss:0.0012022990717399174\n",
      "train loss:0.00037836697856941185\n",
      "train loss:0.01602549498112498\n",
      "train loss:0.010370639215557208\n",
      "train loss:0.0012333120301722188\n",
      "train loss:0.0014394305847767982\n",
      "train loss:0.002016111041445953\n",
      "train loss:0.0012900871745632964\n",
      "train loss:0.003973215157521565\n",
      "train loss:0.01291167759937161\n",
      "train loss:0.0009179861098348574\n",
      "train loss:0.004612420300990358\n",
      "train loss:0.006460066502642504\n",
      "train loss:0.0010016305847600452\n",
      "train loss:0.0004248325958613914\n",
      "train loss:0.002066573285216558\n",
      "train loss:0.0060400615142541525\n",
      "train loss:0.010277196798064261\n",
      "train loss:0.006283917190153971\n",
      "train loss:0.001446026512411894\n",
      "train loss:0.0005241525219204547\n",
      "train loss:0.0005652959025674139\n",
      "train loss:0.0008088106077076047\n",
      "train loss:0.0003125953892456686\n",
      "train loss:0.0008067486101775216\n",
      "train loss:0.0014996058942147855\n",
      "train loss:0.0028351709664974716\n",
      "train loss:0.0012806734483314765\n",
      "train loss:0.0034052603511801533\n",
      "train loss:0.001023979804989312\n",
      "train loss:0.00023243101605331413\n",
      "train loss:0.006335590900864873\n",
      "train loss:0.0004796559634953235\n",
      "train loss:0.0009336826796513746\n",
      "train loss:0.0005065569322086649\n",
      "train loss:0.006599191139768507\n",
      "train loss:0.0004472881596957768\n",
      "train loss:0.001313104587238982\n",
      "train loss:0.0033608061455482397\n",
      "train loss:0.0005154400764564702\n",
      "train loss:0.00044082128491757726\n",
      "train loss:0.0035803101217085475\n",
      "train loss:0.008995299619863785\n",
      "train loss:0.0020503623008195\n",
      "train loss:0.0017741106689966339\n",
      "train loss:0.0005948714581948865\n",
      "train loss:0.0008497616377368286\n",
      "train loss:0.0013738646826981402\n",
      "train loss:0.0008756319066711511\n",
      "train loss:0.0015013146236303055\n",
      "train loss:0.0022228241500326295\n",
      "train loss:0.0021613936809774283\n",
      "train loss:0.0005571799528834229\n",
      "train loss:0.0005241674873009667\n",
      "train loss:0.0022018304558454834\n",
      "train loss:0.016840517483594077\n",
      "train loss:0.0019970781738086795\n",
      "train loss:0.006059899251004827\n",
      "train loss:0.0006485561156228258\n",
      "train loss:0.0015804222670231073\n",
      "train loss:0.0005638399125722808\n",
      "train loss:0.0013307349812396402\n",
      "train loss:0.0032505866878602937\n",
      "train loss:0.0018256713493740022\n",
      "train loss:0.0012321825864335318\n",
      "train loss:0.008596729725366084\n",
      "train loss:0.012395721889158781\n",
      "train loss:0.0021242708617692004\n",
      "train loss:0.002834964166977675\n",
      "train loss:0.0009761769909455765\n",
      "train loss:0.0003843670337238652\n",
      "train loss:0.0030797668342872043\n",
      "train loss:0.002040819935115203\n",
      "train loss:0.010972174454348749\n",
      "train loss:0.009279091424219302\n",
      "train loss:0.0019831340210086142\n",
      "train loss:0.001009830449249619\n",
      "train loss:0.00028969584094575414\n",
      "train loss:0.007865646946277983\n",
      "current iter num:  8400\n",
      "=== epoch:15, train acc:0.997, test acc:0.99 ===\n",
      "train loss:0.006274081479706432\n",
      "train loss:0.002577903571699066\n",
      "train loss:0.00047170862971325354\n",
      "train loss:0.0030499101267535906\n",
      "train loss:0.0009348924405158158\n",
      "train loss:0.000280852213007323\n",
      "train loss:0.0016125648165578696\n",
      "train loss:0.032523111812266844\n",
      "train loss:0.0009106004603540387\n",
      "train loss:0.0007074173622497802\n",
      "train loss:0.0033756950808798442\n",
      "train loss:0.00227529876090047\n",
      "train loss:0.0031478280464977072\n",
      "train loss:0.0014698123808200697\n",
      "train loss:0.00051282689792302\n",
      "train loss:0.004038406587079309\n",
      "train loss:0.0013201579160414167\n",
      "train loss:0.0018033874164158782\n",
      "train loss:0.0027490584144079793\n",
      "train loss:0.002083541936687498\n",
      "train loss:0.0034874866401877615\n",
      "train loss:0.001544959026377901\n",
      "train loss:0.001560588684211112\n",
      "train loss:0.0014167696907010693\n",
      "train loss:0.009934772241091516\n",
      "train loss:0.00210617483039528\n",
      "train loss:0.00046581753553836505\n",
      "train loss:0.002906524545822839\n",
      "train loss:0.005192094261287152\n",
      "train loss:0.01903810788377283\n",
      "train loss:0.004131492285350729\n",
      "train loss:0.0004510282731458096\n",
      "train loss:0.002280179276535591\n",
      "train loss:0.002108982679237924\n",
      "train loss:0.0030825398945508843\n",
      "train loss:0.014541723372636541\n",
      "train loss:0.0018040324670772977\n",
      "train loss:0.0007348143956937278\n",
      "train loss:0.0037793647599727144\n",
      "train loss:0.004204046967228295\n",
      "train loss:0.0027954033108635416\n",
      "train loss:0.005277343649034315\n",
      "train loss:0.00033881549417897423\n",
      "train loss:0.003393178460852407\n",
      "train loss:0.0009969030118979027\n",
      "train loss:0.000900529258141152\n",
      "train loss:0.0019424971690226982\n",
      "train loss:0.0009204860811882945\n",
      "train loss:0.00028762063644506347\n",
      "train loss:0.0004713858726375301\n",
      "train loss:0.004144500438590169\n",
      "train loss:0.004170076014362622\n",
      "train loss:0.007137950087778059\n",
      "train loss:0.0032155115086767994\n",
      "train loss:0.026135722092175913\n",
      "train loss:0.00041697623817164696\n",
      "train loss:0.002525234778249058\n",
      "train loss:0.005662046531364278\n",
      "train loss:0.00048719810779446904\n",
      "train loss:0.0005444318608432464\n",
      "train loss:0.0006468330306557159\n",
      "train loss:0.0066775467196645314\n",
      "train loss:0.0019613642188189753\n",
      "train loss:0.003633145653615169\n",
      "train loss:0.0007280741577752508\n",
      "train loss:0.0003271020403602422\n",
      "train loss:0.0008176508797971971\n",
      "train loss:0.002291739699980339\n",
      "train loss:0.00040843114810547857\n",
      "train loss:0.01481475352736062\n",
      "train loss:0.0017391122567702686\n",
      "train loss:0.003135470050265014\n",
      "train loss:0.0008865702800874179\n",
      "train loss:0.0005431022955718587\n",
      "train loss:0.0009903996258437697\n",
      "train loss:0.0026380994512444304\n",
      "train loss:0.00020373253395689794\n",
      "train loss:0.008523691088650247\n",
      "train loss:0.0043189913844287236\n",
      "train loss:0.03630459540347594\n",
      "train loss:0.0036210517158359063\n",
      "train loss:0.00757554750403859\n",
      "train loss:0.004057179207497272\n",
      "train loss:0.0024521625395339947\n",
      "train loss:0.009586716954125251\n",
      "train loss:9.671830999688392e-05\n",
      "train loss:0.0006139431525409543\n",
      "train loss:0.008964214308863403\n",
      "train loss:0.0014550977476866182\n",
      "train loss:0.0037304850200385077\n",
      "train loss:0.005059356031203202\n",
      "train loss:0.009098639816056549\n",
      "train loss:0.00048679109617853864\n",
      "train loss:0.003405216166706483\n",
      "train loss:0.001948592681158156\n",
      "train loss:4.3430421160526886e-05\n",
      "train loss:0.0018494006295205264\n",
      "train loss:0.0018518182817709521\n",
      "train loss:0.00688252137015407\n",
      "train loss:0.0025008354209236474\n",
      "train loss:0.00026681780526746916\n",
      "train loss:0.0005843880254212369\n",
      "train loss:0.004813385587080204\n",
      "train loss:0.029464551608658164\n",
      "train loss:0.0007082190363616186\n",
      "train loss:0.0044903887416667225\n",
      "train loss:0.001386912594042849\n",
      "train loss:0.0004885188422005909\n",
      "train loss:0.0034146745572341164\n",
      "train loss:0.005520332233289336\n",
      "train loss:0.00046834544952579495\n",
      "train loss:0.0031335936047903316\n",
      "train loss:0.0062926054676919994\n",
      "train loss:0.002789564486447502\n",
      "train loss:0.024105602862777364\n",
      "train loss:0.0016433395861329108\n",
      "train loss:0.0006268830062879659\n",
      "train loss:0.0017298388576043428\n",
      "train loss:0.0029906066366339685\n",
      "train loss:0.0028779832153753467\n",
      "train loss:0.0014691559507884525\n",
      "train loss:0.005581858547144045\n",
      "train loss:0.00035065029366748794\n",
      "train loss:0.0012477442656996923\n",
      "train loss:0.0049991817901174915\n",
      "train loss:0.0003016646249207475\n",
      "train loss:0.0013397026980170616\n",
      "train loss:0.00032286256776094973\n",
      "train loss:0.010667271392095412\n",
      "train loss:0.0004015800708990825\n",
      "train loss:0.00219234833773\n",
      "train loss:0.0006567655577491824\n",
      "train loss:0.0002224397402533524\n",
      "train loss:0.0005487535336125407\n",
      "train loss:0.0025116007177132865\n",
      "train loss:0.00041640733299643505\n",
      "train loss:0.0008311061403860054\n",
      "train loss:0.03262664714867562\n",
      "train loss:0.0051657692625877195\n",
      "train loss:0.00132803609547732\n",
      "train loss:0.0005208088077337097\n",
      "train loss:0.0013373122001689905\n",
      "train loss:0.0011236503751008947\n",
      "train loss:0.005234579252570447\n",
      "train loss:0.0007134057082832868\n",
      "train loss:0.008155891535875325\n",
      "train loss:0.021755950691912464\n",
      "train loss:0.00106352843615018\n",
      "train loss:0.0017473069598213343\n",
      "train loss:0.001367302959510046\n",
      "train loss:0.010189688894816331\n",
      "train loss:0.0003005868309310613\n",
      "train loss:0.0328295452269688\n",
      "train loss:0.002503078619307215\n",
      "train loss:0.001632978691424003\n",
      "train loss:0.0007004138303079925\n",
      "train loss:0.001339865183770522\n",
      "train loss:0.03144446735917405\n",
      "train loss:0.001490644348791172\n",
      "train loss:0.0025505659890434707\n",
      "train loss:0.0024540216311000093\n",
      "train loss:0.003351457309008194\n",
      "train loss:0.004422520738773917\n",
      "train loss:0.0025244774350272146\n",
      "train loss:0.00843188045571296\n",
      "train loss:0.0019735689645265036\n",
      "train loss:0.0013821694334006204\n",
      "train loss:0.004936289582351513\n",
      "train loss:0.004003848876663261\n",
      "train loss:0.0064330925219670395\n",
      "train loss:0.00184473843199587\n",
      "train loss:0.0005239853489207645\n",
      "train loss:0.00031700597089041784\n",
      "train loss:0.003126309172447334\n",
      "train loss:0.0008678548261683131\n",
      "train loss:0.0008488618308586937\n",
      "train loss:0.002150393846249131\n",
      "train loss:0.0004764627090175023\n",
      "train loss:0.009744841582609753\n",
      "train loss:0.0037562821934300707\n",
      "train loss:0.0013241238600240075\n",
      "train loss:0.005715074376237829\n",
      "train loss:0.0010183163083502566\n",
      "train loss:0.002082335576102431\n",
      "train loss:0.0009589457003737547\n",
      "train loss:0.0024356617761494078\n",
      "train loss:0.0007159203890258229\n",
      "train loss:0.002441368768549714\n",
      "train loss:0.0004648268706272717\n",
      "train loss:0.001179361354152762\n",
      "train loss:0.008960333501937943\n",
      "train loss:0.0006309958945663453\n",
      "train loss:0.0013537898121187416\n",
      "train loss:0.0006416726087124289\n",
      "train loss:0.0019637690445728044\n",
      "train loss:0.0022508490693556593\n",
      "train loss:0.0016706161793235052\n",
      "train loss:0.00040575073678449347\n",
      "train loss:0.0027932967770176874\n",
      "train loss:0.00046679483196018334\n",
      "train loss:0.049274554292041556\n",
      "train loss:0.00323454397404608\n",
      "train loss:0.0027425984264892018\n",
      "train loss:0.0007306369277401991\n",
      "train loss:0.021550296042474062\n",
      "train loss:0.0002766292399298637\n",
      "train loss:0.005082005048162753\n",
      "train loss:0.0064945283536233965\n",
      "train loss:0.00046817629240038456\n",
      "train loss:0.0029991430349668767\n",
      "train loss:0.001832168365972283\n",
      "train loss:0.0017462038043910158\n",
      "train loss:0.00021956199211746105\n",
      "train loss:0.0018964981452224503\n",
      "train loss:0.002492946639685096\n",
      "train loss:0.0035842188548469655\n",
      "train loss:0.000878696012101525\n",
      "train loss:0.00018598041422245593\n",
      "train loss:0.0007579236079295882\n",
      "train loss:0.005063014610034733\n",
      "train loss:0.0005695433265733379\n",
      "train loss:0.004855080405500905\n",
      "train loss:0.0026921953560761036\n",
      "train loss:0.004708688262992749\n",
      "train loss:0.00093550436256775\n",
      "train loss:0.005811404180713537\n",
      "train loss:0.00259849147003721\n",
      "train loss:0.0019018751775705903\n",
      "train loss:0.00011950803949939644\n",
      "train loss:0.002174645628331515\n",
      "train loss:0.0006165451106346417\n",
      "train loss:0.0036851007838505064\n",
      "train loss:0.0010223078249990026\n",
      "train loss:0.0015220373463677459\n",
      "train loss:0.0006567948720408961\n",
      "train loss:0.002969648220728232\n",
      "train loss:0.00019345541592608213\n",
      "train loss:0.003901894181313301\n",
      "train loss:0.00021621413449495605\n",
      "train loss:0.0021481449537340404\n",
      "train loss:0.0012940318919838834\n",
      "train loss:0.008451346141000994\n",
      "train loss:0.0005108117709998008\n",
      "train loss:0.003464820819004481\n",
      "train loss:0.00020612447328441317\n",
      "train loss:0.0004749424054473436\n",
      "train loss:0.0029433729836530393\n",
      "train loss:0.00040670794577731873\n",
      "train loss:0.02068216420811608\n",
      "train loss:0.017565070087027256\n",
      "train loss:0.0022343625860150886\n",
      "train loss:8.970400773197018e-05\n",
      "train loss:0.00032803360274669536\n",
      "train loss:0.001725058899249005\n",
      "train loss:0.00041545093325511223\n",
      "train loss:0.0005846481905677852\n",
      "train loss:0.0012716101182452273\n",
      "train loss:0.0053732912850872235\n",
      "train loss:0.0015471219085423581\n",
      "train loss:0.0065382445441098695\n",
      "train loss:0.002402234126593379\n",
      "train loss:0.002193841220882432\n",
      "train loss:0.0002603619721232223\n",
      "train loss:0.00013816710702009387\n",
      "train loss:0.01954222672860516\n",
      "train loss:0.0006361484728270544\n",
      "train loss:0.0006707271987279622\n",
      "train loss:0.001071480508865393\n",
      "train loss:0.0007297690189163334\n",
      "train loss:0.000849636225062138\n",
      "train loss:0.0031495089310139158\n",
      "train loss:0.003541368816757585\n",
      "train loss:0.00050654807738763\n",
      "train loss:0.04121205603981377\n",
      "train loss:0.00014902409729597362\n",
      "train loss:0.0005624519168234524\n",
      "train loss:0.00402534498819686\n",
      "train loss:0.0013597406420469132\n",
      "train loss:0.0006683262188101348\n",
      "train loss:0.003997662933417409\n",
      "train loss:0.0009066961959498783\n",
      "train loss:0.001703514728390603\n",
      "train loss:0.0004154809298848524\n",
      "train loss:0.0010173649993528377\n",
      "train loss:0.0005194539291467253\n",
      "train loss:0.0001561093340256341\n",
      "train loss:0.0014311800433640621\n",
      "train loss:0.002870457820841671\n",
      "train loss:0.0016699204644017582\n",
      "train loss:0.0006060051543561416\n",
      "train loss:0.005353857974064913\n",
      "train loss:0.00252274897922596\n",
      "train loss:0.0037865109894600012\n",
      "train loss:0.002848224073108299\n",
      "train loss:0.001397692990924764\n",
      "train loss:0.002141941276907371\n",
      "train loss:0.00048771466321471073\n",
      "train loss:0.0003477469275861614\n",
      "train loss:0.0011566920200112322\n",
      "train loss:0.09099747128402051\n",
      "train loss:0.0007366303109717691\n",
      "train loss:0.0028787629523794674\n",
      "train loss:0.003450902231344685\n",
      "train loss:0.004058201055231261\n",
      "train loss:0.00032145316897200504\n",
      "train loss:0.0008665566310247239\n",
      "train loss:0.0036283331405054043\n",
      "train loss:0.001624274385286012\n",
      "train loss:0.04407282404595086\n",
      "train loss:0.0032253310214365506\n",
      "train loss:0.0022184727083429975\n",
      "train loss:0.0023446854034638153\n",
      "train loss:0.0008899031475754668\n",
      "train loss:0.0012498578361278656\n",
      "train loss:0.0016192129111854007\n",
      "train loss:0.0010347392453460692\n",
      "train loss:0.0015417408233798385\n",
      "train loss:0.012050109410620681\n",
      "train loss:0.0037183301894772834\n",
      "train loss:0.002396932158086895\n",
      "train loss:0.000411367255784096\n",
      "train loss:0.005172738674993324\n",
      "train loss:0.007155469913175501\n",
      "train loss:0.0032388488531275633\n",
      "train loss:0.0025714872728146436\n",
      "train loss:0.00231063069306044\n",
      "train loss:0.0029254026129408367\n",
      "train loss:0.00021893021220811913\n",
      "train loss:0.0004029762271076264\n",
      "train loss:0.00029982213548312704\n",
      "train loss:0.004744406661887342\n",
      "train loss:0.0010026335040850397\n",
      "train loss:0.0006433653677909221\n",
      "train loss:0.0005563911316410796\n",
      "train loss:0.0023727759463687383\n",
      "train loss:0.0023566080323233346\n",
      "train loss:0.004842788473730627\n",
      "train loss:0.00027838091798503223\n",
      "train loss:0.001408974610136405\n",
      "train loss:0.0007099208894658652\n",
      "train loss:0.0006056469035169918\n",
      "train loss:0.004012696469572194\n",
      "train loss:0.00075160017420729\n",
      "train loss:0.0023477768215529222\n",
      "train loss:0.004029524030041195\n",
      "train loss:0.0013644742453260566\n",
      "train loss:0.00051802704663774\n",
      "train loss:0.0016671067981670605\n",
      "train loss:0.002513837036825194\n",
      "train loss:0.0012852611482377053\n",
      "train loss:0.0010016601646549263\n",
      "train loss:0.0009078476607514069\n",
      "train loss:0.006616354222256514\n",
      "train loss:0.0035004615795085434\n",
      "train loss:0.0007216525331968128\n",
      "train loss:0.0006423534448061853\n",
      "train loss:0.016949467454756565\n",
      "train loss:0.0010684793050384097\n",
      "train loss:0.0018217934961650226\n",
      "train loss:0.0007648160878135651\n",
      "train loss:0.0007254607914069347\n",
      "train loss:0.004268063919934239\n",
      "train loss:0.002766536297554578\n",
      "train loss:0.0061404182856767\n",
      "train loss:0.003518894802314143\n",
      "train loss:0.004742997622366674\n",
      "train loss:0.0021861312158820193\n",
      "train loss:0.042018902739492124\n",
      "train loss:0.002755672205472302\n",
      "train loss:0.0005173725897580883\n",
      "train loss:6.144132226558468e-05\n",
      "train loss:8.640061465845007e-05\n",
      "train loss:0.004579454658517728\n",
      "train loss:0.002789532774052851\n",
      "train loss:0.0001611097673083316\n",
      "train loss:0.009686728696401548\n",
      "train loss:1.1866237424920061e-05\n",
      "train loss:0.0008576443626584441\n",
      "train loss:0.0005592524349992035\n",
      "train loss:0.003496912846679912\n",
      "train loss:0.00044107108797617205\n",
      "train loss:0.008246098642302321\n",
      "train loss:0.010916307444774079\n",
      "train loss:0.004338020264945313\n",
      "train loss:0.0025245429122977996\n",
      "train loss:0.00018311827421307357\n",
      "train loss:0.00030192845141588864\n",
      "train loss:0.0022121673098355835\n",
      "train loss:0.00030291332445388946\n",
      "train loss:0.002243161823465121\n",
      "train loss:0.0019618876814258214\n",
      "train loss:0.0004841985636022592\n",
      "train loss:0.0017392403253682562\n",
      "train loss:0.003396698165846857\n",
      "train loss:0.0004655052211008963\n",
      "train loss:0.0005330345245193308\n",
      "train loss:0.0018965014989521004\n",
      "train loss:0.000623517098005899\n",
      "train loss:0.0032484181620308235\n",
      "train loss:0.007002644725569299\n",
      "train loss:0.002491878139830417\n",
      "train loss:0.001780891172993631\n",
      "train loss:0.002899598170914932\n",
      "train loss:0.002791537699639338\n",
      "train loss:0.00010529784911864296\n",
      "train loss:0.002619035898519889\n",
      "train loss:0.0017132631075753937\n",
      "train loss:0.0008420876564257572\n",
      "train loss:0.0014702488418436648\n",
      "train loss:0.005405182905231368\n",
      "train loss:0.003921621916205239\n",
      "train loss:0.0014243919630881833\n",
      "train loss:0.0067814606648380436\n",
      "train loss:0.003927521261152178\n",
      "train loss:0.0001609243743397999\n",
      "train loss:0.0002591161848644864\n",
      "train loss:0.0005769023111000151\n",
      "train loss:0.0005647811804379104\n",
      "train loss:0.0011542671982513338\n",
      "train loss:0.00035221898626905114\n",
      "train loss:0.004191181892325245\n",
      "train loss:0.0018052973613029545\n",
      "train loss:0.0013149240476910192\n",
      "train loss:0.0014224925968473053\n",
      "train loss:0.0027322796036320872\n",
      "train loss:0.004277322645552762\n",
      "train loss:0.041523231936362516\n",
      "train loss:0.0017437360020802273\n",
      "train loss:0.0004016293038076401\n",
      "train loss:0.0016442079415573188\n",
      "train loss:0.01660287806667872\n",
      "train loss:0.0004824921763395829\n",
      "train loss:0.002012779898360176\n",
      "train loss:0.004915692085604288\n",
      "train loss:0.007454192985544831\n",
      "train loss:0.002019857716902426\n",
      "train loss:0.0007112202542886577\n",
      "train loss:0.002165958300354295\n",
      "train loss:0.0007791409413211651\n",
      "train loss:0.0011009350299437057\n",
      "train loss:0.003873287558566536\n",
      "train loss:0.0009174634789026833\n",
      "train loss:0.0028237676439056437\n",
      "train loss:0.00024270905793465493\n",
      "train loss:0.001254802942645473\n",
      "train loss:0.0035264844712075616\n",
      "train loss:0.004883298671852826\n",
      "train loss:0.00288140743537617\n",
      "train loss:0.007964964229890743\n",
      "train loss:0.0022834585005354667\n",
      "train loss:0.015479725327885538\n",
      "train loss:0.001999054597245578\n",
      "train loss:0.0027079741940993778\n",
      "train loss:0.002349885055864714\n",
      "train loss:0.005447547277660008\n",
      "train loss:0.0030309442759447773\n",
      "train loss:0.0016372095163265572\n",
      "train loss:0.002149149904801804\n",
      "train loss:0.00892447093037192\n",
      "train loss:0.0034195224460622205\n",
      "train loss:0.0037822580164187125\n",
      "train loss:0.001006154263299708\n",
      "train loss:0.00039142155150127887\n",
      "train loss:0.0010936265250119201\n",
      "train loss:0.003390764960584928\n",
      "train loss:0.0007948320888920027\n",
      "train loss:0.0007174316299373158\n",
      "train loss:0.011273978150496168\n",
      "train loss:0.0012896161544605645\n",
      "train loss:0.0012388922625072804\n",
      "train loss:0.000346522149486388\n",
      "train loss:0.00034784142841430505\n",
      "train loss:0.0027820318963540274\n",
      "train loss:0.05194549570609315\n",
      "train loss:0.005453443887759685\n",
      "train loss:0.0021783382089060038\n",
      "train loss:0.0001182683024475441\n",
      "train loss:0.00032064330593189333\n",
      "train loss:0.002718667989393927\n",
      "train loss:0.005297468747861986\n",
      "train loss:0.0022934087247892566\n",
      "train loss:0.00047098110533335216\n",
      "train loss:0.002873027528522516\n",
      "train loss:0.0011426895195937967\n",
      "train loss:0.001242086291531401\n",
      "train loss:0.0008517526545222967\n",
      "train loss:0.003006968240450627\n",
      "train loss:0.0014567347863804875\n",
      "train loss:0.00013053769309374048\n",
      "train loss:0.0010055268937049042\n",
      "train loss:0.0002689327415482539\n",
      "train loss:0.00025109905573922857\n",
      "train loss:0.0008032881055845962\n",
      "train loss:0.0002355394973687191\n",
      "train loss:0.00033360478975751474\n",
      "train loss:0.0027407132926125636\n",
      "train loss:0.00021852176572387513\n",
      "train loss:0.004723466755007285\n",
      "train loss:0.001689807475633786\n",
      "train loss:0.020155879825189283\n",
      "train loss:0.0022000928520869724\n",
      "train loss:0.0005010407750699154\n",
      "train loss:0.0016228672346295552\n",
      "train loss:0.006831736535530531\n",
      "train loss:0.003575326089407935\n",
      "train loss:0.0002863618952263566\n",
      "train loss:0.003981279744995339\n",
      "train loss:0.0008039599893571504\n",
      "train loss:0.006207482885196276\n",
      "train loss:0.000394072581678764\n",
      "train loss:0.0014905112336796639\n",
      "train loss:0.006852802221766014\n",
      "train loss:0.0008235060929784968\n",
      "train loss:0.003625771761257163\n",
      "train loss:0.0021401820016568036\n",
      "train loss:0.00018219677746949073\n",
      "train loss:0.003100283107453076\n",
      "train loss:0.0002627216478358159\n",
      "train loss:3.85039117577893e-05\n",
      "train loss:0.00038225837725140705\n",
      "train loss:0.0012156830799300192\n",
      "train loss:0.000264416557319623\n",
      "train loss:0.00041696376769602596\n",
      "train loss:0.009075131898377048\n",
      "train loss:0.0029335311402363057\n",
      "train loss:0.0016566406866801412\n",
      "train loss:0.010759787237103942\n",
      "train loss:0.001748832552265476\n",
      "train loss:0.002559424630108702\n",
      "train loss:0.005186292182767244\n",
      "train loss:0.0010640964975496125\n",
      "train loss:0.0015616066565267914\n",
      "train loss:0.0007492050503777885\n",
      "train loss:0.0002736647627875139\n",
      "train loss:0.03503623740705302\n",
      "train loss:0.0006930179359309979\n",
      "train loss:0.0003224703981821928\n",
      "train loss:0.0004979416362125983\n",
      "train loss:0.000643857439097837\n",
      "train loss:0.0005520713252849201\n",
      "train loss:0.0011298057482827238\n",
      "train loss:0.0008520875021382078\n",
      "train loss:0.0004819023293478212\n",
      "train loss:0.010228923971172612\n",
      "train loss:0.0008577550694091775\n",
      "train loss:0.006441017007289329\n",
      "train loss:0.0040278217061894914\n",
      "train loss:0.0005340748623530552\n",
      "train loss:0.005493732617367938\n",
      "train loss:0.004452141746391889\n",
      "train loss:0.005634971524231896\n",
      "train loss:0.002408216004255008\n",
      "train loss:0.00155421258833243\n",
      "train loss:0.00042563031470099674\n",
      "train loss:0.0006783775966129544\n",
      "train loss:0.0013633423384670899\n",
      "train loss:0.0008101611557536492\n",
      "train loss:0.0004392237598342936\n",
      "train loss:0.0006746743706883361\n",
      "train loss:0.0013144784999553436\n",
      "train loss:0.00022031399478315242\n",
      "train loss:0.002162481200017793\n",
      "train loss:0.025061038230938935\n",
      "train loss:0.00034002907553137536\n",
      "train loss:0.0029912985139937316\n",
      "train loss:0.028133211820280123\n",
      "train loss:0.002636157620264966\n",
      "train loss:0.00033903306212682677\n",
      "train loss:0.0025016893391110826\n",
      "train loss:0.00036454951812084157\n",
      "train loss:0.002590967620703427\n",
      "train loss:0.0004224059623675725\n",
      "train loss:0.005255078383142989\n",
      "train loss:0.0029512943425176203\n",
      "train loss:0.05041934207143767\n",
      "train loss:0.0007738275713844588\n",
      "train loss:0.0016124292912494617\n",
      "train loss:0.0007979874450130503\n",
      "train loss:0.0067044699610434326\n",
      "train loss:0.00021039883028878582\n",
      "train loss:0.001056651295679042\n",
      "train loss:0.0008813811884941119\n",
      "train loss:0.0022004072198247766\n",
      "train loss:0.024829027514945822\n",
      "train loss:0.000922819750254203\n",
      "train loss:0.00016469484250373776\n",
      "train loss:0.00017461899364056528\n",
      "train loss:0.0015146850001428822\n",
      "train loss:0.00032519727343882865\n",
      "train loss:0.004199799365112507\n",
      "train loss:0.0008142382068935385\n",
      "train loss:0.004646479213192709\n",
      "train loss:0.0024305123973745067\n",
      "train loss:0.0004050944295343512\n",
      "train loss:0.0008121698463805872\n",
      "train loss:0.0008312191110372064\n",
      "train loss:0.014689745429922957\n",
      "train loss:0.00028178108640456925\n",
      "train loss:0.004647662106632722\n",
      "train loss:0.0007855378297988207\n",
      "current iter num:  9000\n",
      "=== epoch:16, train acc:1.0, test acc:0.985 ===\n",
      "train loss:0.0005214895018043548\n",
      "train loss:0.00043284170415564254\n",
      "train loss:0.0003396066463346722\n",
      "train loss:0.0011959389702164928\n",
      "train loss:0.0011626590321646688\n",
      "train loss:0.00013794916994392734\n",
      "train loss:0.0006113855826443469\n",
      "train loss:0.002687172746452744\n",
      "train loss:0.0006312779296460978\n",
      "train loss:0.00111701604319383\n",
      "train loss:0.002944521776715291\n",
      "train loss:0.00037663554254429877\n",
      "train loss:0.002496947463021538\n",
      "train loss:0.0028452905498025432\n",
      "train loss:0.0013214140064401678\n",
      "train loss:0.0041057431723565165\n",
      "train loss:0.0003367048223287624\n",
      "train loss:0.004457933058699769\n",
      "train loss:0.008741060948663145\n",
      "train loss:0.000644303604123966\n",
      "train loss:0.00369918644378226\n",
      "train loss:0.0008605818197331366\n",
      "train loss:0.00039133132778051055\n",
      "train loss:0.0034357747830832453\n",
      "train loss:0.002411709655971595\n",
      "train loss:0.000849252965792714\n",
      "train loss:0.0011380914311406448\n",
      "train loss:0.006748778384533305\n",
      "train loss:0.0031154294482161854\n",
      "train loss:0.020938577548386482\n",
      "train loss:0.001450961178723452\n",
      "train loss:0.03242457451779293\n",
      "train loss:0.0002861203917354996\n",
      "train loss:0.005160125206423456\n",
      "train loss:0.00021952489548025217\n",
      "train loss:0.001712045827896667\n",
      "train loss:0.0006088620613775416\n",
      "train loss:0.00046050116786609666\n",
      "train loss:0.015866717169666335\n",
      "train loss:0.0006816701032883329\n",
      "train loss:0.0006898448164614178\n",
      "train loss:0.0003356073122864322\n",
      "train loss:0.010250187507304507\n",
      "train loss:0.0028353581616701855\n",
      "train loss:0.001827942426957114\n",
      "train loss:0.007503035292686926\n",
      "train loss:0.00045117301783224547\n",
      "train loss:0.00031707023291098414\n",
      "train loss:0.00348188195495517\n",
      "train loss:0.0007359433078439785\n",
      "train loss:0.004695305893689835\n",
      "train loss:0.0012405442191851833\n",
      "train loss:0.0024018498481193163\n",
      "train loss:0.0053906293284720986\n",
      "train loss:0.005704471024494239\n",
      "train loss:0.003009278728198725\n",
      "train loss:0.0014799387222984431\n",
      "train loss:0.007796535170698916\n",
      "train loss:0.00723208015779772\n",
      "train loss:0.031874994776301444\n",
      "train loss:0.0012012460891782154\n",
      "train loss:0.0009755746329195689\n",
      "train loss:0.0015130291028419027\n",
      "train loss:0.0023691760979238944\n",
      "train loss:0.002730914429851281\n",
      "train loss:0.001226912830813755\n",
      "train loss:0.0028711184971060987\n",
      "train loss:0.00024958786018452784\n",
      "train loss:0.0044583931186126195\n",
      "train loss:0.006354202946555172\n",
      "train loss:0.002011619873024874\n",
      "train loss:0.012441386026999741\n",
      "train loss:0.011537013288115204\n",
      "train loss:0.006197737068141173\n",
      "train loss:0.0003997108687260249\n",
      "train loss:0.0003561420873804214\n",
      "train loss:0.0016892847614808507\n",
      "train loss:0.003513990271308985\n",
      "train loss:0.0025459443636524836\n",
      "train loss:0.002354815918512972\n",
      "train loss:0.0016482005865458681\n",
      "train loss:0.000581316905849207\n",
      "train loss:0.001382337112317735\n",
      "train loss:0.011023210484241567\n",
      "train loss:0.001343723539754087\n",
      "train loss:0.0006358588940371791\n",
      "train loss:0.004835215371485607\n",
      "train loss:0.0033515840164120457\n",
      "train loss:0.002479779761114939\n",
      "train loss:0.0019114032123539856\n",
      "train loss:0.00027955157674636443\n",
      "train loss:0.0006235646594061058\n",
      "train loss:0.013938448559557847\n",
      "train loss:0.0014739712243342492\n",
      "train loss:0.0005189620726826282\n",
      "train loss:0.0002988438572325845\n",
      "train loss:0.001437936001111935\n",
      "train loss:0.000664628973817257\n",
      "train loss:0.001000392171434863\n",
      "train loss:0.0030845644897642393\n",
      "train loss:0.0029947455047752603\n",
      "train loss:0.00417559200502507\n",
      "train loss:0.0006645793996496289\n",
      "train loss:0.0004564656158631277\n",
      "train loss:0.0005850895828652274\n",
      "train loss:0.0022613288613929456\n",
      "train loss:0.0005694741898140468\n",
      "train loss:0.0010527155719774854\n",
      "train loss:0.004094201040968172\n",
      "train loss:0.00254775658344701\n",
      "train loss:0.003182538601087688\n",
      "train loss:0.000684005291910382\n",
      "train loss:0.00027779643878504917\n",
      "train loss:0.0023134417363860006\n",
      "train loss:0.001429731633964815\n",
      "train loss:0.00010389351418444982\n",
      "train loss:0.00289720935430246\n",
      "train loss:0.0005590107655263132\n",
      "train loss:0.0010485400188529864\n",
      "train loss:0.0011005374690933871\n",
      "train loss:0.0031309653042786217\n",
      "train loss:0.002213400206797302\n",
      "train loss:0.00014126667240335123\n",
      "train loss:4.9094670334325585e-05\n",
      "train loss:0.00046758112366614114\n",
      "train loss:0.00011102097580231276\n",
      "train loss:8.839937679717066e-05\n",
      "train loss:0.0015553826425581218\n",
      "train loss:0.00028350460888650097\n",
      "train loss:0.00038554612492016987\n",
      "train loss:0.0034679600522958655\n",
      "train loss:0.0008205177104928192\n",
      "train loss:0.0021601709574031415\n",
      "train loss:0.0007934496376251547\n",
      "train loss:0.007285185366204494\n",
      "train loss:0.0007055116772760141\n",
      "train loss:0.009751213139259841\n",
      "train loss:0.0007331776948539023\n",
      "train loss:0.001213112383218336\n",
      "train loss:0.003217628325957705\n",
      "train loss:0.0013925227017196468\n",
      "train loss:0.005984109982346956\n",
      "train loss:0.0001409183377851544\n",
      "train loss:0.008490047130844404\n",
      "train loss:0.0006707714645877291\n",
      "train loss:0.005604766139641241\n",
      "train loss:0.0017008071222116246\n",
      "train loss:0.0028487546641211337\n",
      "train loss:0.0027422693503490335\n",
      "train loss:0.00017494459906472655\n",
      "train loss:0.006153750865753215\n",
      "train loss:0.021951450667735455\n",
      "train loss:0.0001241012918098776\n",
      "train loss:0.0018744790663138067\n",
      "train loss:0.009892862796365571\n",
      "train loss:0.0014321749267642374\n",
      "train loss:0.001969626153788599\n",
      "train loss:0.012885081539764935\n",
      "train loss:0.0005025688804047137\n",
      "train loss:0.002783696406968649\n",
      "train loss:0.0008905263463159513\n",
      "train loss:0.003308356679259092\n",
      "train loss:0.00411071004973817\n",
      "train loss:0.003443707650326913\n",
      "train loss:0.008462910301726448\n",
      "train loss:0.0004764538308446457\n",
      "train loss:0.0006929170524735539\n",
      "train loss:0.003266636390273416\n",
      "train loss:0.00165448071093256\n",
      "train loss:0.0011760256204204889\n",
      "train loss:0.003134584084123062\n",
      "train loss:0.01630402550119453\n",
      "train loss:0.0031826465519688035\n",
      "train loss:0.0003383561266099131\n",
      "train loss:0.00550507338870079\n",
      "train loss:0.0008606198110402321\n",
      "train loss:0.003986762315011012\n",
      "train loss:0.003036283470411461\n",
      "train loss:0.0013069048800153976\n",
      "train loss:0.0014746150163561181\n",
      "train loss:0.0003251733669708122\n",
      "train loss:0.0001360857529822935\n",
      "train loss:0.0013496574747616445\n",
      "train loss:0.0007331567857027105\n",
      "train loss:0.0008683418988026345\n",
      "train loss:0.00018617807096777772\n",
      "train loss:0.014129428081304811\n",
      "train loss:0.002163099523459545\n",
      "train loss:0.002575882447468368\n",
      "train loss:0.0021794482001843223\n",
      "train loss:0.008211438344857372\n",
      "train loss:0.0018925658471173159\n",
      "train loss:0.0022954941756688956\n",
      "train loss:0.00037612098308500747\n",
      "train loss:0.005444210144178028\n",
      "train loss:0.0028794888810988566\n",
      "train loss:0.00011477514801717088\n",
      "train loss:0.0015031216144264926\n",
      "train loss:0.01032935247299533\n",
      "train loss:0.0007290712468263577\n",
      "train loss:0.0009635550810516024\n",
      "train loss:0.003734655306105943\n",
      "train loss:0.0019295889479919212\n",
      "train loss:0.002474624465769583\n",
      "train loss:0.0003876353526066817\n",
      "train loss:0.00894686915478216\n",
      "train loss:0.0005307590349784094\n",
      "train loss:0.004021992768375497\n",
      "train loss:0.0024359556949370424\n",
      "train loss:0.0009921756248823502\n",
      "train loss:0.0002550164028261609\n",
      "train loss:0.0029893793324910958\n",
      "train loss:0.0010707208609084724\n",
      "train loss:0.01652001150088435\n",
      "train loss:0.0007152028576128418\n",
      "train loss:0.0020290399447712225\n",
      "train loss:0.0008935739626202669\n",
      "train loss:0.0007461762239505656\n",
      "train loss:0.0018409306528220511\n",
      "train loss:0.0010012043688227\n",
      "train loss:0.0006526470845989021\n",
      "train loss:0.0013595118655413537\n",
      "train loss:0.003976182719716593\n",
      "train loss:0.0011174162228703862\n",
      "train loss:0.0023850470834367783\n",
      "train loss:0.0007289483823502087\n",
      "train loss:0.002096088022209728\n",
      "train loss:0.0014886394160679877\n",
      "train loss:0.0013832895150922406\n",
      "train loss:0.0003707207175007662\n",
      "train loss:0.0010626270002356362\n",
      "train loss:0.00404552242248756\n",
      "train loss:0.001339689260259217\n",
      "train loss:0.0006511294875939016\n",
      "train loss:0.000781268924762234\n",
      "train loss:0.002678735803911823\n",
      "train loss:0.0027689467053016153\n",
      "train loss:0.006347922428594368\n",
      "train loss:8.253059583042319e-05\n",
      "train loss:0.00017241207169362273\n",
      "train loss:0.002277828911290442\n",
      "train loss:0.00036297564284294205\n",
      "train loss:0.0011094058751512231\n",
      "train loss:0.0034810153800151344\n",
      "train loss:0.0003221714197201687\n",
      "train loss:0.0005461421500364358\n",
      "train loss:0.0014343351764804627\n",
      "train loss:0.00014636657328018424\n",
      "train loss:0.001541298571139685\n",
      "train loss:0.0002660786578393535\n",
      "train loss:0.0006564463766865149\n",
      "train loss:0.0037169746732460774\n",
      "train loss:0.0011839575685629485\n",
      "train loss:0.0004507504760734083\n",
      "train loss:0.002045902725724297\n",
      "train loss:0.02530872520417267\n",
      "train loss:0.004264674305509462\n",
      "train loss:0.0006903093153001477\n",
      "train loss:0.0006040699210557924\n",
      "train loss:0.001967526667349441\n",
      "train loss:0.000608861554483768\n",
      "train loss:0.0006548018666006583\n",
      "train loss:0.00021012970748130708\n",
      "train loss:0.0031146646523530354\n",
      "train loss:0.0026650685034620286\n",
      "train loss:0.006865473385357328\n",
      "train loss:0.034802067042512215\n",
      "train loss:0.0005499919774621674\n",
      "train loss:0.0019987139080334653\n",
      "train loss:0.00018732932266153748\n",
      "train loss:0.0005505419450821428\n",
      "train loss:0.006447793592801426\n",
      "train loss:0.004550911073826472\n",
      "train loss:0.0012463127263335292\n",
      "train loss:0.0025012262825486655\n",
      "train loss:0.0003486797270082131\n",
      "train loss:0.0020544908263364194\n",
      "train loss:0.00739910684434775\n",
      "train loss:0.002487471263729556\n",
      "train loss:0.016786550930169855\n",
      "train loss:0.0032151961418839577\n",
      "train loss:0.0007051426784302872\n",
      "train loss:0.0007154487966280816\n",
      "train loss:0.0019384672904609321\n",
      "train loss:0.0025213646866612044\n",
      "train loss:0.0006693744490005272\n",
      "train loss:0.0014170472809075798\n",
      "train loss:0.0001312137867143314\n",
      "train loss:0.00462457491148813\n",
      "train loss:0.0005091936296964037\n",
      "train loss:0.0010563999181249521\n",
      "train loss:0.0021184537224607815\n",
      "train loss:0.00019905204939822555\n",
      "train loss:0.007333310533256338\n",
      "train loss:0.0021763910677127293\n",
      "train loss:0.004932741393754662\n",
      "train loss:0.001107262514035258\n",
      "train loss:0.00044363687745286673\n",
      "train loss:0.0009230346894094811\n",
      "train loss:0.00012142250539038722\n",
      "train loss:0.007617785545813042\n",
      "train loss:0.0006104482437042449\n",
      "train loss:0.002107528474201274\n",
      "train loss:0.0015324720809392163\n",
      "train loss:0.013602258568086301\n",
      "train loss:0.004520488828070785\n",
      "train loss:0.005908818145382664\n",
      "train loss:0.00041488926130201295\n",
      "train loss:0.0009527525436874612\n",
      "train loss:0.001055480373758752\n",
      "train loss:0.0008346921390642035\n",
      "train loss:0.0009661627786996564\n",
      "train loss:0.003390480504434089\n",
      "train loss:0.0011622046843924646\n",
      "train loss:0.0006752379296176582\n",
      "train loss:0.002024849757957966\n",
      "train loss:0.004781765868735205\n",
      "train loss:0.0011560715098342625\n",
      "train loss:0.0014226111067511243\n",
      "train loss:0.000668661904831087\n",
      "train loss:0.0023905648385458933\n",
      "train loss:0.0016349916322699748\n",
      "train loss:0.001303652759390616\n",
      "train loss:0.00015783294666920025\n",
      "train loss:0.0017256064889184031\n",
      "train loss:0.0004148717261261493\n",
      "train loss:0.008949293040893223\n",
      "train loss:0.0002670869671325068\n",
      "train loss:0.0002368299892570071\n",
      "train loss:0.0004000714832933528\n",
      "train loss:0.0004148174650070177\n",
      "train loss:0.0037625064695589547\n",
      "train loss:0.00017393857143062407\n",
      "train loss:0.0022021031349258656\n",
      "train loss:0.0013097913344496134\n",
      "train loss:0.0008406979671411428\n",
      "train loss:0.0009518000811392299\n",
      "train loss:0.002392060474730081\n",
      "train loss:0.0015486535515985495\n",
      "train loss:0.006541818756458001\n",
      "train loss:0.0016671894225833495\n",
      "train loss:0.00531879074610864\n",
      "train loss:0.00019193123604260842\n",
      "train loss:0.0005689157854159295\n",
      "train loss:0.0038505662631266504\n",
      "train loss:0.007733798891924164\n",
      "train loss:0.0016136473657832845\n",
      "train loss:0.0007265876519565282\n",
      "train loss:0.002049199748627129\n",
      "train loss:0.0017661188244655537\n",
      "train loss:0.001495337944537666\n",
      "train loss:0.0005002957615648392\n",
      "train loss:0.0028138257425753686\n",
      "train loss:0.00018809433822304918\n",
      "train loss:0.0009300295751539357\n",
      "train loss:0.002055550381005907\n",
      "train loss:0.0006386986151470684\n",
      "train loss:0.00045172370686363773\n",
      "train loss:0.0003080090684230242\n",
      "train loss:0.0005506399755852584\n",
      "train loss:3.2921523503956194e-05\n",
      "train loss:0.0005536203457028927\n",
      "train loss:0.0008245192004785679\n",
      "train loss:0.0002779162392806145\n",
      "train loss:0.0015986341921017536\n",
      "train loss:0.0024050296900659323\n",
      "train loss:0.00018359052078502274\n",
      "train loss:0.000397281026261111\n",
      "train loss:0.004129465988521889\n",
      "train loss:0.00014091234489007644\n",
      "train loss:0.0003022545675440841\n",
      "train loss:0.0002925385378964337\n",
      "train loss:0.0004021590615557316\n",
      "train loss:0.00014199766921631438\n",
      "train loss:0.0013689120750801282\n",
      "train loss:4.5944503323264e-05\n",
      "train loss:0.00032556817060721237\n",
      "train loss:0.0014718899276945172\n",
      "train loss:0.002173952663974445\n",
      "train loss:4.354170174404988e-05\n",
      "train loss:0.006034429387468973\n",
      "train loss:0.0016768079128661679\n",
      "train loss:0.0023860186706466118\n",
      "train loss:0.00013783300402113653\n",
      "train loss:0.0015182673804603356\n",
      "train loss:0.0027505733293503986\n",
      "train loss:0.003641763929850243\n",
      "train loss:0.0001521123277923365\n",
      "train loss:0.010920308105024661\n",
      "train loss:0.0014374493904505412\n",
      "train loss:0.002094576505302313\n",
      "train loss:0.00040905814469540525\n",
      "train loss:0.0001450495883869362\n",
      "train loss:0.0032192518556685597\n",
      "train loss:0.00052911207290634\n",
      "train loss:0.003107156775719007\n",
      "train loss:0.0008454831458100912\n",
      "train loss:0.001884003218462406\n",
      "train loss:4.082357794736066e-05\n",
      "train loss:0.0057693260309859265\n",
      "train loss:0.00030639654865238726\n",
      "train loss:0.0003587545381954243\n",
      "train loss:0.004145235534954233\n",
      "train loss:0.0027839173220391682\n",
      "train loss:0.0028887455596757067\n",
      "train loss:0.00032742066013192653\n",
      "train loss:0.002545782123885063\n",
      "train loss:0.001701729198104198\n",
      "train loss:0.0007165679803611736\n",
      "train loss:0.0010250414956945219\n",
      "train loss:0.0001346960828578381\n",
      "train loss:0.000291179953392188\n",
      "train loss:0.006835273386496656\n",
      "train loss:0.0012081126731712055\n",
      "train loss:0.0032801675483917995\n",
      "train loss:0.0040159031276395165\n",
      "train loss:0.0001608468473623377\n",
      "train loss:0.00035015989655926667\n",
      "train loss:0.00022525783036324868\n",
      "train loss:8.949687495649644e-05\n",
      "train loss:0.0023915929880568717\n",
      "train loss:0.0001985777350212844\n",
      "train loss:0.0029134363672898705\n",
      "train loss:0.0019344794165225057\n",
      "train loss:0.0001707193930942202\n",
      "train loss:0.0004884491102820882\n",
      "train loss:0.0006415319984104671\n",
      "train loss:0.004680137682962231\n",
      "train loss:0.000901466612004056\n",
      "train loss:0.010388474084354499\n",
      "train loss:0.0009329307491781417\n",
      "train loss:0.0015431304734783722\n",
      "train loss:0.00047018707721439417\n",
      "train loss:0.0002484090604046783\n",
      "train loss:0.007277836550515325\n",
      "train loss:0.0008680765413568363\n",
      "train loss:0.00021881478659573634\n",
      "train loss:0.0019706712319804837\n",
      "train loss:0.001487872089740594\n",
      "train loss:0.0023252725774184015\n",
      "train loss:0.00017268635333638299\n",
      "train loss:0.0009860200979238951\n",
      "train loss:9.673417852649967e-05\n",
      "train loss:0.0034567349176789853\n",
      "train loss:0.0008631182625165708\n",
      "train loss:0.0034785259462493047\n",
      "train loss:6.451102083354579e-05\n",
      "train loss:0.0025653443890494253\n",
      "train loss:0.0036428309208584096\n",
      "train loss:0.01738478474505041\n",
      "train loss:0.0005782016363234221\n",
      "train loss:0.00022766525181201386\n",
      "train loss:0.0002939467792616952\n",
      "train loss:0.0005337576332325614\n",
      "train loss:0.04854946844537536\n",
      "train loss:0.0010455716306618297\n",
      "train loss:0.0026426189344178285\n",
      "train loss:0.0013729625836451662\n",
      "train loss:0.004293394353079461\n",
      "train loss:0.004775280606740921\n",
      "train loss:0.0013787602193693362\n",
      "train loss:0.0008199348901456703\n",
      "train loss:0.00014925441316363962\n",
      "train loss:0.0010198656930358932\n",
      "train loss:0.01010404721927838\n",
      "train loss:0.001912538661723476\n",
      "train loss:0.0013523212961115286\n",
      "train loss:0.0006685329161572694\n",
      "train loss:0.0036020857513274084\n",
      "train loss:0.00016905353591397824\n",
      "train loss:0.00021736054451276785\n",
      "train loss:0.0009639301691935144\n",
      "train loss:0.002120539757231614\n",
      "train loss:0.00047649333215565387\n",
      "train loss:0.0032699770863607856\n",
      "train loss:0.0011141468089008652\n",
      "train loss:0.0022783666703408887\n",
      "train loss:0.00040216992941930895\n",
      "train loss:0.00012543380053995948\n",
      "train loss:0.002333654773717896\n",
      "train loss:0.0004418321802418148\n",
      "train loss:0.004169272114397325\n",
      "train loss:0.0015362819362415975\n",
      "train loss:0.0036435417738089598\n",
      "train loss:0.0027221131823264673\n",
      "train loss:0.00022714977316460328\n",
      "train loss:0.0020482974799526125\n",
      "train loss:0.0022637932390578708\n",
      "train loss:0.0024173599780887944\n",
      "train loss:0.015407357928800077\n",
      "train loss:0.002837786418256086\n",
      "train loss:0.0008455971102111387\n",
      "train loss:0.00010331066473748152\n",
      "train loss:0.0008258465739357658\n",
      "train loss:0.000623093779839121\n",
      "train loss:0.0019381172381023979\n",
      "train loss:0.0005499488134772407\n",
      "train loss:0.00017938547651009606\n",
      "train loss:0.002375547263577213\n",
      "train loss:0.004476635594021258\n",
      "train loss:0.0002685241954903221\n",
      "train loss:0.0006709327762372312\n",
      "train loss:0.0011275021029357566\n",
      "train loss:0.007650570677397089\n",
      "train loss:0.0019794220450449237\n",
      "train loss:0.011987713319887459\n",
      "train loss:0.0016075323100044308\n",
      "train loss:0.0005592726294086912\n",
      "train loss:0.0008055510689367329\n",
      "train loss:0.0005818709481174169\n",
      "train loss:0.008280452569327587\n",
      "train loss:0.0002077401364200376\n",
      "train loss:0.004562128647135766\n",
      "train loss:0.002117172125157508\n",
      "train loss:0.00034803736171108707\n",
      "train loss:0.0004939575130143622\n",
      "train loss:0.003918205880180067\n",
      "train loss:0.0023346199148578744\n",
      "train loss:0.0005912796988054854\n",
      "train loss:0.0012483373259328614\n",
      "train loss:0.0005553168339741591\n",
      "train loss:0.0023858457449436503\n",
      "train loss:0.0007156835322954573\n",
      "train loss:0.0002487644617345551\n",
      "train loss:0.00097015915432555\n",
      "train loss:0.002119662118286803\n",
      "train loss:0.0012708176399498156\n",
      "train loss:0.0032213786609239076\n",
      "train loss:0.004781349879362623\n",
      "train loss:0.009073428865666243\n",
      "train loss:0.0028558703811138748\n",
      "train loss:0.0032024576027292417\n",
      "train loss:0.01576775344048889\n",
      "train loss:0.005232519820055267\n",
      "train loss:0.001446532350831059\n",
      "train loss:0.00231192400165715\n",
      "train loss:0.0024579306481144693\n",
      "train loss:0.0006928801585345004\n",
      "train loss:0.0017686805448300922\n",
      "train loss:0.00280412052556421\n",
      "train loss:0.00028157665592403705\n",
      "train loss:0.004943018519149878\n",
      "train loss:0.00019445844931013464\n",
      "train loss:0.007669529536443914\n",
      "train loss:0.0010614119148162973\n",
      "train loss:0.00023438606517109956\n",
      "train loss:0.001081855413465992\n",
      "train loss:0.00028874904181283383\n",
      "train loss:0.003562078008389084\n",
      "train loss:0.0011744140782577173\n",
      "train loss:0.006623311150738069\n",
      "train loss:0.0004802973800282803\n",
      "train loss:0.0007090503677355288\n",
      "train loss:0.00023474258331020935\n",
      "train loss:0.0005853328620128306\n",
      "train loss:0.001751893572628249\n",
      "train loss:0.001007246441530039\n",
      "train loss:0.000182837306604471\n",
      "train loss:0.0066746561874605585\n",
      "train loss:0.00025636917185265873\n",
      "train loss:0.0004918996671071751\n",
      "train loss:0.0002744534759291435\n",
      "train loss:0.00010401533640981442\n",
      "train loss:6.656073860673724e-05\n",
      "train loss:0.0007173699067993467\n",
      "train loss:0.012725921829482283\n",
      "train loss:0.0033535134787880017\n",
      "train loss:0.01191149289529528\n",
      "train loss:0.0003293683604835902\n",
      "train loss:0.00029796865015755547\n",
      "train loss:0.000733387144270311\n",
      "train loss:0.0002829435743665323\n",
      "train loss:7.660430413643842e-05\n",
      "train loss:0.001459922436824007\n",
      "train loss:0.009583105464403007\n",
      "train loss:0.0009303198665243592\n",
      "train loss:9.703182614416852e-05\n",
      "train loss:6.310757239135655e-05\n",
      "train loss:0.0013584567956168839\n",
      "train loss:0.0016907136903715379\n",
      "train loss:0.00020661433241614722\n",
      "train loss:0.003185079172327335\n",
      "train loss:0.001567640785699233\n",
      "train loss:0.0011521480349293528\n",
      "train loss:0.008491690963510974\n",
      "train loss:0.0023519814265003263\n",
      "train loss:0.0013494507571449015\n",
      "train loss:0.0004081259383365411\n",
      "train loss:0.0004894797588266928\n",
      "train loss:0.012954258656233255\n",
      "train loss:0.0013921179961710341\n",
      "train loss:0.014062801806774283\n",
      "train loss:0.0025132795563367017\n",
      "train loss:0.011295225140621321\n",
      "train loss:0.0011704550724038736\n",
      "train loss:0.00065700080077084\n",
      "train loss:0.0019507119344078158\n",
      "train loss:0.00020342547247503707\n",
      "train loss:0.006660959510257513\n",
      "train loss:0.0008806075058112605\n",
      "current iter num:  9600\n",
      "=== epoch:17, train acc:0.999, test acc:0.984 ===\n",
      "train loss:0.002192636331457782\n",
      "train loss:0.0001793869279669408\n",
      "train loss:0.0006741694021740681\n",
      "train loss:0.0012342971360222695\n",
      "train loss:0.009523982442206529\n",
      "train loss:0.0012175311406342803\n",
      "train loss:0.000938134452335214\n",
      "train loss:0.002042142233903027\n",
      "train loss:0.0012083946172478915\n",
      "train loss:0.0005603417414928133\n",
      "train loss:7.227916809684504e-05\n",
      "train loss:8.95426937893943e-05\n",
      "train loss:0.001141864361772724\n",
      "train loss:0.0012251856417655825\n",
      "train loss:0.002009001933104204\n",
      "train loss:0.0004979112632168916\n",
      "train loss:0.0018311104448493322\n",
      "train loss:0.00286497034391955\n",
      "train loss:0.0038937748196800374\n",
      "train loss:0.004517702262539366\n",
      "train loss:0.00010729745366081371\n",
      "train loss:0.001950272951275345\n",
      "train loss:0.001142477083015589\n",
      "train loss:0.001088251489118816\n",
      "train loss:0.0002002905402283597\n",
      "train loss:0.0006314513153069958\n",
      "train loss:0.010891169238622182\n",
      "train loss:0.0004948787847835163\n",
      "train loss:8.966731955532294e-05\n",
      "train loss:0.0002214249156411559\n",
      "train loss:0.002080738039286733\n",
      "train loss:0.0035216939232380843\n",
      "train loss:0.0001368968819675317\n",
      "train loss:0.0015437859641067903\n",
      "train loss:0.0006530889610651258\n",
      "train loss:0.0016204650698442152\n",
      "train loss:0.0027556370817094257\n",
      "train loss:0.001051400415583938\n",
      "train loss:0.000692477396556372\n",
      "train loss:0.0002254843120328332\n",
      "train loss:0.0003319070061900084\n",
      "train loss:0.00025779539071113245\n",
      "train loss:0.0016888867443762556\n",
      "train loss:0.0002369564616872823\n",
      "train loss:6.298959018022083e-05\n",
      "train loss:0.003970647721448978\n",
      "train loss:0.001938303904179027\n",
      "train loss:0.0010379567943660415\n",
      "train loss:0.00174954179374335\n",
      "train loss:0.00021468338583718657\n",
      "train loss:0.0005170949363553327\n",
      "train loss:0.00034701906234598586\n",
      "train loss:0.0004884649520246055\n",
      "train loss:0.003280753043535742\n",
      "train loss:0.002346429343779847\n",
      "train loss:0.0004165044141009275\n",
      "train loss:0.0006150995321386102\n",
      "train loss:0.000794038138111026\n",
      "train loss:0.0014607416069229188\n",
      "train loss:8.137492368201737e-05\n",
      "train loss:0.001322352391996032\n",
      "train loss:0.0009576289870683171\n",
      "train loss:0.003256232406818465\n",
      "train loss:0.0042620423333384755\n",
      "train loss:0.00012847881259090197\n",
      "train loss:0.0031025326995164897\n",
      "train loss:0.0009877018352836447\n",
      "train loss:0.0017965496056713453\n",
      "train loss:0.005688887163409383\n",
      "train loss:0.0018465714831758955\n",
      "train loss:0.00012022391239772164\n",
      "train loss:0.00042265722527936575\n",
      "train loss:0.00020152369130610261\n",
      "train loss:0.0001496420791402681\n",
      "train loss:0.0010721983554616151\n",
      "train loss:0.0007789961974491294\n",
      "train loss:0.0018065779239356194\n",
      "train loss:0.0038249837180976847\n",
      "train loss:0.00017907752259652512\n",
      "train loss:0.0001504112069444186\n",
      "train loss:0.0003698396624619571\n",
      "train loss:0.0002638152740376417\n",
      "train loss:0.0001515854357246156\n",
      "train loss:0.00018887344920175562\n",
      "train loss:0.0006811293991944203\n",
      "train loss:0.0027580762350125497\n",
      "train loss:0.0002566523076255083\n",
      "train loss:0.0022621303880266497\n",
      "train loss:0.0011814804485272817\n",
      "train loss:0.000718187553527877\n",
      "train loss:0.001547203195686842\n",
      "train loss:0.0003839030909367758\n",
      "train loss:0.0025488454980869824\n",
      "train loss:0.0011025136387326192\n",
      "train loss:0.0002291132339800853\n",
      "train loss:0.0019837129064718636\n",
      "train loss:0.003422952556349251\n",
      "train loss:0.00449491707059589\n",
      "train loss:0.005162156544586396\n",
      "train loss:0.0006368542380338371\n",
      "train loss:0.002299709099019554\n",
      "train loss:0.00018152007900412075\n",
      "train loss:0.0015516013965176662\n",
      "train loss:0.0031452866718800344\n",
      "train loss:4.181389446354727e-05\n",
      "train loss:0.001278185528166176\n",
      "train loss:0.0031671671610114687\n",
      "train loss:0.0017661324665557978\n",
      "train loss:0.0009352159659365492\n",
      "train loss:0.00858563202944656\n",
      "train loss:0.0005256345922997908\n",
      "train loss:0.0015872600502694729\n",
      "train loss:0.001809980810182164\n",
      "train loss:0.00026654279115507834\n",
      "train loss:0.0019450656685049454\n",
      "train loss:0.0008360712573963312\n",
      "train loss:0.0003527287439436724\n",
      "train loss:0.00046344265773208445\n",
      "train loss:0.0008724980374236731\n",
      "train loss:0.000777769257509505\n",
      "train loss:0.0021951914741246987\n",
      "train loss:0.000293249548875298\n",
      "train loss:0.0021383381405101244\n",
      "train loss:0.0021403285991737297\n",
      "train loss:0.000320919388527388\n",
      "train loss:0.0012359144829709437\n",
      "train loss:0.0007891667076657254\n",
      "train loss:0.001379804882185902\n",
      "train loss:0.0002985817592072676\n",
      "train loss:0.00015000809275018644\n",
      "train loss:0.0026747234285210654\n",
      "train loss:0.00041043191227677664\n",
      "train loss:0.006685382585030336\n",
      "train loss:0.003348427973333719\n",
      "train loss:0.000598570235759799\n",
      "train loss:0.0014847202429218213\n",
      "train loss:0.015482934088594153\n",
      "train loss:0.0030138355380013742\n",
      "train loss:0.0008904372307667343\n",
      "train loss:0.0015172976019557898\n",
      "train loss:0.0024137480692939008\n",
      "train loss:0.0001524416058096184\n",
      "train loss:0.00031193908672531\n",
      "train loss:0.001719126948813622\n",
      "train loss:0.00026503959933465\n",
      "train loss:0.0006879995563482054\n",
      "train loss:0.0005215842828222802\n",
      "train loss:0.0009867687357420218\n",
      "train loss:0.00012436743865936668\n",
      "train loss:0.0012545953517170455\n",
      "train loss:0.0006207991966560205\n",
      "train loss:0.014010552555387993\n",
      "train loss:0.013988193927258943\n",
      "train loss:0.0009922508050873224\n",
      "train loss:0.008633543550533342\n",
      "train loss:0.001234769341080548\n",
      "train loss:0.00012799211257983248\n",
      "train loss:8.033995369939079e-05\n",
      "train loss:0.0018056362361445074\n",
      "train loss:0.0008091697005424503\n",
      "train loss:0.002799891160077539\n",
      "train loss:0.0007097309024521133\n",
      "train loss:0.011396204431486747\n",
      "train loss:8.151861140252129e-05\n",
      "train loss:0.002075489566411418\n",
      "train loss:0.0002599025939290529\n",
      "train loss:0.003557341463461397\n",
      "train loss:0.0023133504076769218\n",
      "train loss:0.00025230545739891337\n",
      "train loss:0.0016545171888912504\n",
      "train loss:0.0002626829610269835\n",
      "train loss:0.0007420494885726022\n",
      "train loss:0.0029644615973355116\n",
      "train loss:0.0009326674077016552\n",
      "train loss:0.000963371041415428\n",
      "train loss:0.004599250924518256\n",
      "train loss:0.0016695256170972316\n",
      "train loss:5.315977265094317e-05\n",
      "train loss:0.006741748320931626\n",
      "train loss:0.0029690366534234048\n",
      "train loss:0.001071225482419495\n",
      "train loss:0.003209894114216217\n",
      "train loss:0.0041851614406804025\n",
      "train loss:0.009415432170243113\n",
      "train loss:0.0040666597375874644\n",
      "train loss:0.02076343603405344\n",
      "train loss:0.0016537701481375378\n",
      "train loss:0.0012003863489449258\n",
      "train loss:0.11051993226837079\n",
      "train loss:0.0014210715274697117\n",
      "train loss:0.000642102438769444\n",
      "train loss:0.0023952216127099517\n",
      "train loss:0.0004530677915090624\n",
      "train loss:0.012703042092608457\n",
      "train loss:0.004896890545087747\n",
      "train loss:0.0046419585302913055\n",
      "train loss:0.00039211841601882806\n",
      "train loss:0.016896707548194726\n",
      "train loss:0.002043680350338857\n",
      "train loss:0.004910499415483642\n",
      "train loss:0.003785669941173209\n",
      "train loss:0.00017117897579862527\n",
      "train loss:0.0009823694060278295\n",
      "train loss:0.0009053009087837027\n",
      "train loss:0.0007446928242357844\n",
      "train loss:0.002525169121144705\n",
      "train loss:0.0011039613700245853\n",
      "train loss:0.004893663707663052\n",
      "train loss:0.0007843178271251003\n",
      "train loss:0.002125002968536354\n",
      "train loss:0.00016677872022942975\n",
      "train loss:0.0006063419041880738\n",
      "train loss:0.0005867541847736517\n",
      "train loss:0.0013221929454744002\n",
      "train loss:0.0030997331632422894\n",
      "train loss:5.8228020734166634e-05\n",
      "train loss:0.0009453812192131801\n",
      "train loss:0.0014500692266130196\n",
      "train loss:0.0005109035281682737\n",
      "train loss:0.0007120530794820277\n",
      "train loss:0.003192753811683296\n",
      "train loss:0.001019315333038267\n",
      "train loss:0.0005668345661259497\n",
      "train loss:0.005451041532488761\n",
      "train loss:0.0023247102147333845\n",
      "train loss:0.011188455932287353\n",
      "train loss:0.0014585102562136455\n",
      "train loss:0.0010057296022989077\n",
      "train loss:0.0010367654975880832\n",
      "train loss:0.000788375936535024\n",
      "train loss:0.00035973674610291056\n",
      "train loss:0.0015922573016200778\n",
      "train loss:0.002612778983178025\n",
      "train loss:0.01770681536315042\n",
      "train loss:0.0010777173802029832\n",
      "train loss:0.005889654009950943\n",
      "train loss:0.0013037434824894258\n",
      "train loss:0.002587201135058587\n",
      "train loss:0.0010606167614430207\n",
      "train loss:0.0020868129466063605\n",
      "train loss:0.00013820841842263052\n",
      "train loss:0.0009071770952669585\n",
      "train loss:0.0025704505972743074\n",
      "train loss:0.005076071861865043\n",
      "train loss:0.00015284157643142443\n",
      "train loss:0.005825844443632754\n",
      "train loss:0.00025426114557411844\n",
      "train loss:0.0017077614629764397\n",
      "train loss:0.0018341784669159528\n",
      "train loss:0.006377990713504506\n",
      "train loss:0.006195339901924272\n",
      "train loss:0.001277844402957044\n",
      "train loss:0.0029477659159660515\n",
      "train loss:0.002759295863720188\n",
      "train loss:0.0005477911999342465\n",
      "train loss:0.008688907905035207\n",
      "train loss:0.0006534999162720274\n",
      "train loss:0.008011804020894346\n",
      "train loss:0.0005295922333170987\n",
      "train loss:0.0057839516384538\n",
      "train loss:0.0016508345980709032\n",
      "train loss:0.0010180752224304152\n",
      "train loss:0.00021701748010390597\n",
      "train loss:0.0007944767120925033\n",
      "train loss:0.001813084061778669\n",
      "train loss:0.0030598665395571594\n",
      "train loss:0.0016680645013394091\n",
      "train loss:0.0025313452727028767\n",
      "train loss:0.0006284116372732708\n",
      "train loss:0.00071851479346534\n",
      "train loss:0.0004284177166475939\n",
      "train loss:0.0017571232661405115\n",
      "train loss:0.0025438547770422525\n",
      "train loss:0.00029679889552917027\n",
      "train loss:0.0003886089729780276\n",
      "train loss:0.0013672761256266824\n",
      "train loss:0.03407177079629072\n",
      "train loss:8.184692095440891e-05\n",
      "train loss:0.002634422467721374\n",
      "train loss:0.0028934091861542517\n",
      "train loss:0.0011217948354710068\n",
      "train loss:0.0011986976938353942\n",
      "train loss:0.001846813226916394\n",
      "train loss:0.0012968506543076417\n",
      "train loss:5.546776990559701e-05\n",
      "train loss:0.0028478565268902794\n",
      "train loss:0.0004919774192265454\n",
      "train loss:0.0024007545801666154\n",
      "train loss:0.00035853400579301795\n",
      "train loss:0.005341843708330518\n",
      "train loss:0.0004883201066085896\n",
      "train loss:0.0020143820257840176\n",
      "train loss:0.0003527042195255875\n",
      "train loss:0.001102545346052261\n",
      "train loss:0.003895777924408184\n",
      "train loss:8.871778946389752e-05\n",
      "train loss:0.003675514412458974\n",
      "train loss:0.000840640843452731\n",
      "train loss:0.0002619548152582615\n",
      "train loss:0.0011666833962238757\n",
      "train loss:0.0011746232591096516\n",
      "train loss:0.0004968952202949293\n",
      "train loss:0.002730831159389346\n",
      "train loss:0.0007867062638976951\n",
      "train loss:0.003899916438790502\n",
      "train loss:0.003395852615955341\n",
      "train loss:0.004354147719788375\n",
      "train loss:0.0007676314881105125\n",
      "train loss:0.0016512946245780542\n",
      "train loss:0.0004677022246287201\n",
      "train loss:0.010753263881408865\n",
      "train loss:0.00220871940174655\n",
      "train loss:0.0014811368092374148\n",
      "train loss:0.002432274029538084\n",
      "train loss:0.009654477120972449\n",
      "train loss:0.0001005515559392081\n",
      "train loss:0.0018128020969240992\n",
      "train loss:0.0016707900955176738\n",
      "train loss:0.0055429829515728024\n",
      "train loss:0.0036293167595010307\n",
      "train loss:0.006361538570874269\n",
      "train loss:0.0017082587180944056\n",
      "train loss:0.002522315289910361\n",
      "train loss:0.0012752058727370202\n",
      "train loss:0.008878548058894743\n",
      "train loss:4.334358583944647e-05\n",
      "train loss:0.000792891597508551\n",
      "train loss:0.0020309653952569635\n",
      "train loss:0.00569048631599613\n",
      "train loss:0.006322034177727173\n",
      "train loss:0.002276730008977761\n",
      "train loss:0.002436076459886451\n",
      "train loss:0.0007920026744803034\n",
      "train loss:0.000302828461144103\n",
      "train loss:0.002473090272514763\n",
      "train loss:0.0005170628669850619\n",
      "train loss:0.00465450249826211\n",
      "train loss:0.0024195794792764703\n",
      "train loss:0.0029054333446603365\n",
      "train loss:0.0012174672180336617\n",
      "train loss:0.0023504532395840563\n",
      "train loss:0.0025677969294630714\n",
      "train loss:0.0001017632373477093\n",
      "train loss:0.001980831027476048\n",
      "train loss:0.0006377037873206687\n",
      "train loss:0.0014595778770998282\n",
      "train loss:0.001765681511642519\n",
      "train loss:0.00020205588339141877\n",
      "train loss:0.000771324193015126\n",
      "train loss:0.0013691981750241558\n",
      "train loss:0.00010791937314842074\n",
      "train loss:0.0014171187194543466\n",
      "train loss:0.0022641396170450657\n",
      "train loss:0.005015439732671426\n",
      "train loss:0.0010901489751796274\n",
      "train loss:0.009790794090081816\n",
      "train loss:0.001647528830938253\n",
      "train loss:0.001796407809038357\n",
      "train loss:0.0019040303713513825\n",
      "train loss:0.0011325963356740337\n",
      "train loss:0.0012896969340338807\n",
      "train loss:0.0055198352047431745\n",
      "train loss:0.0010013855443081047\n",
      "train loss:0.00022168463569331856\n",
      "train loss:0.009103006524197196\n",
      "train loss:0.0012071023518090555\n",
      "train loss:0.0027147007503189784\n",
      "train loss:0.002011504269028137\n",
      "train loss:0.0009448104474575144\n",
      "train loss:0.0006061161536642227\n",
      "train loss:0.005695021690531018\n",
      "train loss:0.0026489305822117778\n",
      "train loss:0.001708550841325924\n",
      "train loss:0.0007623615503076913\n",
      "train loss:0.004249756230070887\n",
      "train loss:0.0003530176699359471\n",
      "train loss:0.0003511530221950113\n",
      "train loss:0.0005299312674264677\n",
      "train loss:0.0007834340827225046\n",
      "train loss:0.005507001760123043\n",
      "train loss:0.0006322266217397784\n",
      "train loss:0.005806105756818067\n",
      "train loss:0.0009390509261984152\n",
      "train loss:0.00033686381842627295\n",
      "train loss:0.002036743243620445\n",
      "train loss:0.00014421116021180632\n",
      "train loss:0.0017298050519606262\n",
      "train loss:0.002152279090870043\n",
      "train loss:0.0031243713085722198\n",
      "train loss:0.001791234999334685\n",
      "train loss:0.003749614182024143\n",
      "train loss:0.002083807764197693\n",
      "train loss:0.00040243411945268273\n",
      "train loss:0.0013910244172076537\n",
      "train loss:0.002101517279930371\n",
      "train loss:0.001011563148265533\n",
      "train loss:0.0006805872950093878\n",
      "train loss:0.003343124781173262\n",
      "train loss:0.00023492286409309648\n",
      "train loss:0.001853622288783982\n",
      "train loss:0.00034238674498927326\n",
      "train loss:0.0004877491331199371\n",
      "train loss:0.00016816186353664747\n",
      "train loss:0.004707971382150097\n",
      "train loss:0.006134894519757781\n",
      "train loss:0.003228953761268515\n",
      "train loss:0.0008512827168164107\n",
      "train loss:0.00571485990595923\n",
      "train loss:0.0009462161616010803\n",
      "train loss:0.0007517610403735602\n",
      "train loss:0.0017172307038117573\n",
      "train loss:0.003046027868991084\n",
      "train loss:0.006869078386666881\n",
      "train loss:0.0014300491611115255\n",
      "train loss:0.007548360696850609\n",
      "train loss:0.00020580823465341414\n",
      "train loss:0.00042477211902390214\n",
      "train loss:0.0019189696712986295\n",
      "train loss:0.0014789510253062229\n",
      "train loss:0.001078115121621872\n",
      "train loss:0.0029955343249495935\n",
      "train loss:0.0006171073021477287\n",
      "train loss:0.0005162887035842969\n",
      "train loss:0.001293019902727311\n",
      "train loss:0.001968343439306612\n",
      "train loss:0.0007784948858427915\n",
      "train loss:0.0035371363505772406\n",
      "train loss:0.007020317443097006\n",
      "train loss:0.00678085340889581\n",
      "train loss:0.00043467404437361196\n",
      "train loss:0.0014909031420851965\n",
      "train loss:0.001124370352433013\n",
      "train loss:0.00027851031169164975\n",
      "train loss:0.002410284899748708\n",
      "train loss:0.0006392737265012836\n",
      "train loss:0.003567830776203861\n",
      "train loss:0.00252861971611936\n",
      "train loss:0.0026009355501463802\n",
      "train loss:0.00015113175364709234\n",
      "train loss:0.00033615383402628137\n",
      "train loss:0.0005279416550568437\n",
      "train loss:0.001261759413433524\n",
      "train loss:0.0027998917003357643\n",
      "train loss:0.002154590001094975\n",
      "train loss:0.0003533138891237683\n",
      "train loss:0.0031587405575902684\n",
      "train loss:0.003527298388822022\n",
      "train loss:0.0021786771360908803\n",
      "train loss:0.0026460234784017383\n",
      "train loss:0.00043749712766235726\n",
      "train loss:0.0013163587792355508\n",
      "train loss:0.0017080466639443437\n",
      "train loss:0.004854663442513429\n",
      "train loss:0.0031154651124426936\n",
      "train loss:0.0011875272925376537\n",
      "train loss:0.00011028185315308569\n",
      "train loss:0.00035319891386815865\n",
      "train loss:0.002633138032155729\n",
      "train loss:0.0020328998367489873\n",
      "train loss:0.0018876911923952878\n",
      "train loss:0.002691413564803227\n",
      "train loss:0.0005968588886246279\n",
      "train loss:0.00035751163055589474\n",
      "train loss:0.0038466556559822824\n",
      "train loss:0.0026670337962952217\n",
      "train loss:0.0008217006260743702\n",
      "train loss:0.0023498152759694347\n",
      "train loss:0.00010486432748927759\n",
      "train loss:0.00022956808504901696\n",
      "train loss:0.001917847374087845\n",
      "train loss:0.0003092110087929889\n",
      "train loss:0.002502379106636355\n",
      "train loss:0.0018920792098519473\n",
      "train loss:0.0005631943796010806\n",
      "train loss:0.00032604320864774777\n",
      "train loss:0.0020004784407399476\n",
      "train loss:0.0040190421876645285\n",
      "train loss:0.005738769388261142\n",
      "train loss:0.004088602807863582\n",
      "train loss:0.00022966530120464718\n",
      "train loss:0.0025107011432416344\n",
      "train loss:0.0007848879681700818\n",
      "train loss:0.002695021510633476\n",
      "train loss:0.0004886415531082809\n",
      "train loss:0.0034527635868606276\n",
      "train loss:0.004008723286752323\n",
      "train loss:0.0009444685632000848\n",
      "train loss:0.00017539197245468408\n",
      "train loss:0.0014996410423659945\n",
      "train loss:0.001802500908983333\n",
      "train loss:9.992130160820095e-05\n",
      "train loss:0.03599041854427545\n",
      "train loss:0.000578034993698834\n",
      "train loss:0.0012003509830708203\n",
      "train loss:0.0019750207418491377\n",
      "train loss:0.001592358117728105\n",
      "train loss:0.003334759009353215\n",
      "train loss:0.005208826345435838\n",
      "train loss:0.007298088850545737\n",
      "train loss:0.0008219612299036501\n",
      "train loss:0.001285633090269649\n",
      "train loss:0.0015655278761522527\n",
      "train loss:0.009627502570224926\n",
      "train loss:0.0002002173548564758\n",
      "train loss:0.0012596844525030548\n",
      "train loss:0.0011191276113213936\n",
      "train loss:0.00019161332150431805\n",
      "train loss:0.0005459405083888959\n",
      "train loss:0.0036987289396271487\n",
      "train loss:0.0009466084382765624\n",
      "train loss:0.000630301429475052\n",
      "train loss:0.005854751136217912\n",
      "train loss:0.0014755656040173909\n",
      "train loss:0.0027300915198818147\n",
      "train loss:0.005906543679614775\n",
      "train loss:0.004347684825510383\n",
      "train loss:0.00029565216286280634\n",
      "train loss:5.7151711524134115e-05\n",
      "train loss:0.0025228069589534555\n",
      "train loss:0.0015814640480834533\n",
      "train loss:0.0022193620213952015\n",
      "train loss:0.008819040633331278\n",
      "train loss:0.002117803520295446\n",
      "train loss:0.0005631779075110022\n",
      "train loss:0.00024142746933495635\n",
      "train loss:0.00021921792027869278\n",
      "train loss:0.004037040258201009\n",
      "train loss:0.001944869915544053\n",
      "train loss:0.0010133223079532294\n",
      "train loss:0.0020715794581017246\n",
      "train loss:0.0017018187311236496\n",
      "train loss:0.0019656110542557045\n",
      "train loss:0.0017402603274772185\n",
      "train loss:0.0005971954734152739\n",
      "train loss:0.0027493738795306465\n",
      "train loss:0.0015616491676992636\n",
      "train loss:0.0015495031994578285\n",
      "train loss:0.0013280144514365455\n",
      "train loss:0.0007149715502508384\n",
      "train loss:0.0014851231000807235\n",
      "train loss:0.003983523388564775\n",
      "train loss:0.002994219302889567\n",
      "train loss:0.0012500037079423523\n",
      "train loss:3.925025613103419e-05\n",
      "train loss:0.004107713677539857\n",
      "train loss:0.003198392625248499\n",
      "train loss:5.544948275785379e-05\n",
      "train loss:0.0016415892516966415\n",
      "train loss:0.0006380371121542774\n",
      "train loss:0.0041473693382346865\n",
      "train loss:0.003568110462122599\n",
      "train loss:0.0018277196803238955\n",
      "train loss:0.001062343235513171\n",
      "train loss:4.765071646639659e-05\n",
      "train loss:0.0026012975127907008\n",
      "train loss:0.0023911925236630677\n",
      "train loss:3.76976874995247e-05\n",
      "train loss:0.0031942638500642474\n",
      "train loss:0.003455873675407337\n",
      "train loss:0.011494421957116838\n",
      "train loss:0.0028241584073888594\n",
      "train loss:0.0011892098748634895\n",
      "train loss:0.0002739384625720784\n",
      "train loss:0.0006252840409707865\n",
      "train loss:0.00047860019295341017\n",
      "train loss:9.838603595841171e-05\n",
      "train loss:0.0006749825649014678\n",
      "train loss:0.0005580633498130965\n",
      "train loss:0.009057056447052115\n",
      "train loss:0.0029216841547182325\n",
      "train loss:0.0003663436441270298\n",
      "train loss:0.004097325178600734\n",
      "train loss:0.0019083865620709828\n",
      "train loss:0.006107103182606497\n",
      "train loss:0.00470081303266226\n",
      "train loss:0.0029696924270345283\n",
      "train loss:0.0008126734491721803\n",
      "train loss:0.001726887851567233\n",
      "train loss:0.00029381126288339306\n",
      "train loss:0.001331220572255651\n",
      "train loss:0.00016002738714111114\n",
      "train loss:0.0004113621276183184\n",
      "train loss:0.003882004002102309\n",
      "train loss:0.024220947193157424\n",
      "train loss:0.0036959754749930494\n",
      "train loss:0.0021153815730112252\n",
      "train loss:0.005250085649613646\n",
      "train loss:0.04061591949514429\n",
      "train loss:0.016474599211470056\n",
      "train loss:0.0011243724723018539\n",
      "train loss:0.005942076864604059\n",
      "train loss:0.0001067628862334525\n",
      "train loss:0.00035704299935726183\n",
      "train loss:0.0021449899575401877\n",
      "train loss:0.0008384405886464409\n",
      "train loss:0.0005975939007630096\n",
      "train loss:0.0030127431547759703\n",
      "train loss:0.002412727962803242\n",
      "train loss:0.0037553781727731332\n",
      "train loss:0.0020739390855615946\n",
      "current iter num:  10200\n",
      "=== epoch:18, train acc:0.998, test acc:0.99 ===\n",
      "train loss:0.0005375398489015113\n",
      "train loss:0.0006628898368015493\n",
      "train loss:0.0001654049291997319\n",
      "train loss:0.0005654628971406236\n",
      "train loss:0.0013530669274429085\n",
      "train loss:0.0014148890327464626\n",
      "train loss:0.0003155521091951162\n",
      "train loss:0.0019320963383351486\n",
      "train loss:0.02879266378496186\n",
      "train loss:0.00011667224978786053\n",
      "train loss:0.0006959260766695957\n",
      "train loss:0.00012288953827576158\n",
      "train loss:0.0026229801142985173\n",
      "train loss:0.000893440790972698\n",
      "train loss:0.0013885941334456442\n",
      "train loss:0.00034069582521457567\n",
      "train loss:0.0001818081321331517\n",
      "train loss:0.00033502823490980295\n",
      "train loss:0.004390997758822319\n",
      "train loss:0.0006278121550865461\n",
      "train loss:0.000824730390639606\n",
      "train loss:0.0001521603803426607\n",
      "train loss:0.0009096336309892126\n",
      "train loss:0.0007832288335830476\n",
      "train loss:0.0002159486779373131\n",
      "train loss:0.0027342783113598083\n",
      "train loss:0.00022528899436633962\n",
      "train loss:0.00014982650000418434\n",
      "train loss:0.0006433283526964868\n",
      "train loss:0.00043344881202526616\n",
      "train loss:0.0014271942384513508\n",
      "train loss:0.0020349226632001794\n",
      "train loss:6.392925555485797e-05\n",
      "train loss:0.00042567940185320153\n",
      "train loss:6.962831221394894e-05\n",
      "train loss:0.00035931470520607453\n",
      "train loss:0.00032324619809526867\n",
      "train loss:0.0007854521375444045\n",
      "train loss:0.000605698567194475\n",
      "train loss:0.0036788460940886903\n",
      "train loss:0.0005516004101431143\n",
      "train loss:0.05364966987251143\n",
      "train loss:8.961577699535006e-05\n",
      "train loss:0.0030752359696647608\n",
      "train loss:0.0009493932673417188\n",
      "train loss:0.004393401970501765\n",
      "train loss:0.001972231965980081\n",
      "train loss:0.0017126498211615067\n",
      "train loss:0.0005337598065806687\n",
      "train loss:0.0045010209900980215\n",
      "train loss:0.0012018123519475959\n",
      "train loss:0.006535478994709071\n",
      "train loss:0.00012290236150969572\n",
      "train loss:9.40056214750166e-05\n",
      "train loss:5.487631828140536e-05\n",
      "train loss:0.0008540719452340825\n",
      "train loss:0.0012807533580457436\n",
      "train loss:0.001763136674685775\n",
      "train loss:0.0008371341567040024\n",
      "train loss:0.0007643849272310388\n",
      "train loss:0.00046623404035452945\n",
      "train loss:0.00026605514105201383\n",
      "train loss:0.0005499598052021428\n",
      "train loss:0.0012675220700267458\n",
      "train loss:0.015506306177602966\n",
      "train loss:0.0004767112107894835\n",
      "train loss:0.0006850071596081952\n",
      "train loss:0.004462552694122746\n",
      "train loss:9.774047183483037e-05\n",
      "train loss:0.01697372561769364\n",
      "train loss:0.0013851050321494493\n",
      "train loss:0.0001411323725494383\n",
      "train loss:0.002286267870650078\n",
      "train loss:0.0004938071789585704\n",
      "train loss:0.0030354043632113996\n",
      "train loss:0.0013871353126156452\n",
      "train loss:0.0006981021723475619\n",
      "train loss:0.00037203364042054203\n",
      "train loss:0.004595560961674788\n",
      "train loss:0.0012053304733706466\n",
      "train loss:0.0007882251128784348\n",
      "train loss:0.0015239960293808913\n",
      "train loss:0.0019569557729989394\n",
      "train loss:0.004490051551489231\n",
      "train loss:0.0004564445210230985\n",
      "train loss:0.002904203294311666\n",
      "train loss:0.01313033222072565\n",
      "train loss:0.0004876638995728582\n",
      "train loss:0.004535591525300524\n",
      "train loss:0.0003927248263061253\n",
      "train loss:0.002390981453485167\n",
      "train loss:0.0006090222440345765\n",
      "train loss:0.0010163933233641575\n",
      "train loss:0.00026742150791008503\n",
      "train loss:0.0020729408076243655\n",
      "train loss:0.00045002764581919487\n",
      "train loss:0.00747152892852401\n",
      "train loss:0.005502317356063344\n",
      "train loss:1.8834497776919625e-05\n",
      "train loss:0.0008057780988251602\n",
      "train loss:0.002872279587597323\n",
      "train loss:0.00980628761204132\n",
      "train loss:0.005196923938656622\n",
      "train loss:0.0021576192506907728\n",
      "train loss:0.001458957105091385\n",
      "train loss:0.0011578577480759642\n",
      "train loss:0.00021394813826920223\n",
      "train loss:0.00385072045162805\n",
      "train loss:4.6315122532427055e-05\n",
      "train loss:0.0012122965552761968\n",
      "train loss:0.004325084342786704\n",
      "train loss:0.0035630920581179305\n",
      "train loss:0.00518034924848652\n",
      "train loss:0.0005629312898755034\n",
      "train loss:0.0018177336044216815\n",
      "train loss:0.0035705286011005356\n",
      "train loss:0.0027553002799045622\n",
      "train loss:0.0002594101063041336\n",
      "train loss:0.011610755052237898\n",
      "train loss:0.0006304415524569773\n",
      "train loss:0.0021243420074603266\n",
      "train loss:0.00012527476842706816\n",
      "train loss:0.00010232041048019962\n",
      "train loss:0.0009584995303496437\n",
      "train loss:0.001301954974361458\n",
      "train loss:0.000555439810545726\n",
      "train loss:0.001214959184243196\n",
      "train loss:0.0005019583665202268\n",
      "train loss:0.002885022478896053\n",
      "train loss:0.0001690775024290006\n",
      "train loss:0.0018977226226583428\n",
      "train loss:0.00046578599703678\n",
      "train loss:0.00011680368071925534\n",
      "train loss:0.002793837970831396\n",
      "train loss:0.0032565317709802176\n",
      "train loss:0.0020875706971143246\n",
      "train loss:0.00262535941308921\n",
      "train loss:0.03301808676053804\n",
      "train loss:0.0038047593929924823\n",
      "train loss:0.015913443203532097\n",
      "train loss:0.00018554222139498773\n",
      "train loss:0.0013594771410366377\n",
      "train loss:0.0002482267969478108\n",
      "train loss:7.619435900531427e-05\n",
      "train loss:0.0001493894986053173\n",
      "train loss:0.0016223440464741721\n",
      "train loss:0.001987460555912011\n",
      "train loss:0.002314883955093342\n",
      "train loss:0.003333865346917411\n",
      "train loss:0.0005535480038117868\n",
      "train loss:0.0013431582761661304\n",
      "train loss:0.00048741457999052477\n",
      "train loss:0.00015642926859219176\n",
      "train loss:0.00220004763474492\n",
      "train loss:0.0011990030410221908\n",
      "train loss:0.00014376835837374676\n",
      "train loss:0.0003160229289865641\n",
      "train loss:0.0016190875972124422\n",
      "train loss:0.0002870590221161505\n",
      "train loss:0.001677405216828315\n",
      "train loss:0.0002281726191550542\n",
      "train loss:0.0002994104439007706\n",
      "train loss:0.00024959736289961596\n",
      "train loss:0.00044260673127765373\n",
      "train loss:0.0010654861082390913\n",
      "train loss:2.1385602916723362e-05\n",
      "train loss:1.0598001102376968e-05\n",
      "train loss:0.0005252355073300431\n",
      "train loss:0.003309558266728247\n",
      "train loss:0.0010024922755637142\n",
      "train loss:4.899761947789671e-05\n",
      "train loss:0.001968848760675684\n",
      "train loss:0.0005705316111838391\n",
      "train loss:0.0003890773503667038\n",
      "train loss:0.002469091598218342\n",
      "train loss:0.0030124997619673088\n",
      "train loss:0.00017745059213815725\n",
      "train loss:0.00014352333163167267\n",
      "train loss:0.01472984635900974\n",
      "train loss:0.0008799838374903924\n",
      "train loss:0.0001907138660389527\n",
      "train loss:0.003972623064225367\n",
      "train loss:0.0005269001035385315\n",
      "train loss:0.0001923832564429942\n",
      "train loss:0.0009448650961993664\n",
      "train loss:0.0018898734675557233\n",
      "train loss:0.00019558276139563336\n",
      "train loss:0.00229895065446356\n",
      "train loss:0.0006132781501247116\n",
      "train loss:0.011887981144171764\n",
      "train loss:0.0016010889225420897\n",
      "train loss:6.800868403556277e-05\n",
      "train loss:0.0019371251081317625\n",
      "train loss:0.00026293581538010876\n",
      "train loss:0.00013478086159013488\n",
      "train loss:0.0024193576817255168\n",
      "train loss:0.002182214607416653\n",
      "train loss:0.0009808047696946683\n",
      "train loss:0.0011080750857646031\n",
      "train loss:0.001790018139037258\n",
      "train loss:0.001290977894885162\n",
      "train loss:0.001618156219560668\n",
      "train loss:0.005015068009885702\n",
      "train loss:0.0003264289222927755\n",
      "train loss:0.0015913410416063462\n",
      "train loss:0.0003982712561292209\n",
      "train loss:0.00021505165823542294\n",
      "train loss:0.0009779634002859076\n",
      "train loss:0.0012639663079732743\n",
      "train loss:0.00044117330425776915\n",
      "train loss:0.0007006214189688091\n",
      "train loss:0.0022398788852293675\n",
      "train loss:0.0012115424140312079\n",
      "train loss:0.001753652200135059\n",
      "train loss:6.884598109591714e-06\n",
      "train loss:0.002160965510640557\n",
      "train loss:0.002090676272094271\n",
      "train loss:0.00019651175543157818\n",
      "train loss:0.00011174498537100049\n",
      "train loss:0.000988095007369371\n",
      "train loss:0.0012574164853378954\n",
      "train loss:0.00039980366841221805\n",
      "train loss:0.002519151358674098\n",
      "train loss:0.0001060511844377273\n",
      "train loss:0.0013411362161575924\n",
      "train loss:0.0007978371233868579\n",
      "train loss:0.005121491835255526\n",
      "train loss:0.00047931138673818754\n",
      "train loss:0.0020655676132499095\n",
      "train loss:0.0011589019988798143\n",
      "train loss:0.0005654868062257148\n",
      "train loss:0.0007886779800809555\n",
      "train loss:0.0030273964947363347\n",
      "train loss:7.873733621713069e-05\n",
      "train loss:0.0018197156954956242\n",
      "train loss:0.0009972067878134241\n",
      "train loss:0.00010348827511753639\n",
      "train loss:0.004303306733503622\n",
      "train loss:0.0007106117386271463\n",
      "train loss:0.0028947798000260668\n",
      "train loss:0.0016579367684430898\n",
      "train loss:0.0015789757566741492\n",
      "train loss:0.0011714091632681375\n",
      "train loss:0.00018882411423497652\n",
      "train loss:0.0002211619346114319\n",
      "train loss:0.0011161110099470767\n",
      "train loss:0.00015783543297948126\n",
      "train loss:0.0004405588205844046\n",
      "train loss:0.00047333772023496623\n",
      "train loss:0.0007085295621524371\n",
      "train loss:0.00012784732952435932\n",
      "train loss:0.0021319207407639587\n",
      "train loss:0.0019351322645886331\n",
      "train loss:0.0011390151441197137\n",
      "train loss:0.0036077073544730732\n",
      "train loss:0.0014711907632451561\n",
      "train loss:8.544798237686071e-05\n",
      "train loss:0.000252508076242173\n",
      "train loss:0.0004281861499019758\n",
      "train loss:0.0013432950631994964\n",
      "train loss:0.0017779989229662052\n",
      "train loss:8.077933330555062e-05\n",
      "train loss:0.0007094272651652173\n",
      "train loss:0.0006315419095514564\n",
      "train loss:0.00118202872681556\n",
      "train loss:0.0041990091096168\n",
      "train loss:0.0014399915236055357\n",
      "train loss:0.0008342015341436662\n",
      "train loss:0.001134127425370312\n",
      "train loss:0.0002509439730857273\n",
      "train loss:0.000977419804271977\n",
      "train loss:0.0002533130834182116\n",
      "train loss:0.0015998878658388842\n",
      "train loss:0.00011308705324910587\n",
      "train loss:0.00043763281456681984\n",
      "train loss:8.28294476646415e-05\n",
      "train loss:0.0011356631650169561\n",
      "train loss:0.0010445285237200232\n",
      "train loss:0.0012764242848318343\n",
      "train loss:0.009510657403259622\n",
      "train loss:0.0026335956187339394\n",
      "train loss:0.0004037599168414666\n",
      "train loss:0.0010406964017254628\n",
      "train loss:0.0014848902457002462\n",
      "train loss:7.216512393546942e-05\n",
      "train loss:0.0002359829075862603\n",
      "train loss:0.00206858476801619\n",
      "train loss:0.00012072506034658982\n",
      "train loss:0.0002432719271153338\n",
      "train loss:0.0008622574215672916\n",
      "train loss:0.001122709677612622\n",
      "train loss:0.00040769680336182745\n",
      "train loss:0.0001427973159822901\n",
      "train loss:6.93869422343313e-05\n",
      "train loss:0.00463109428303857\n",
      "train loss:0.0022733436876080796\n",
      "train loss:0.003422983899064656\n",
      "train loss:6.379002349591596e-05\n",
      "train loss:0.0009959304091225257\n",
      "train loss:0.0006316904943727686\n",
      "train loss:0.00058258404051436\n",
      "train loss:0.0010500582479201484\n",
      "train loss:9.687074393510902e-05\n",
      "train loss:0.00010056124637648356\n",
      "train loss:0.0011506548346255107\n",
      "train loss:0.0009661304225634161\n",
      "train loss:0.00023363875753417332\n",
      "train loss:0.0006859140490760392\n",
      "train loss:0.000334553019085294\n",
      "train loss:0.00014498475370468468\n",
      "train loss:0.0003441498980418186\n",
      "train loss:0.00026234212961212414\n",
      "train loss:0.0001808740688831418\n",
      "train loss:0.00015366418225065543\n",
      "train loss:0.004372379292446738\n",
      "train loss:0.00033165439395556383\n",
      "train loss:0.001517331368592624\n",
      "train loss:0.00010025793523300172\n",
      "train loss:0.0005716419779624625\n",
      "train loss:2.644714403912472e-05\n",
      "train loss:0.005724781957184969\n",
      "train loss:4.4665259700095115e-05\n",
      "train loss:0.0010183895857319518\n",
      "train loss:0.0002719915859499582\n",
      "train loss:0.001791702922705932\n",
      "train loss:0.0012566409554542984\n",
      "train loss:0.006045424781479347\n",
      "train loss:0.0008636681939191922\n",
      "train loss:0.0037342463152088183\n",
      "train loss:2.86930026778841e-05\n",
      "train loss:0.0005929819618087338\n",
      "train loss:0.00010236423173456513\n",
      "train loss:0.0002493411016879436\n",
      "train loss:0.00013624406694833128\n",
      "train loss:0.0007161065941669408\n",
      "train loss:0.00013744271539492244\n",
      "train loss:0.004049770370598216\n",
      "train loss:0.0011793080638713932\n",
      "train loss:0.004588103126773908\n",
      "train loss:0.0012685406153248194\n",
      "train loss:0.0004732181598585438\n",
      "train loss:0.0002671545806058169\n",
      "train loss:0.001178321704163727\n",
      "train loss:0.0002667380998165408\n",
      "train loss:0.0004516393696518233\n",
      "train loss:0.0002452847683319676\n",
      "train loss:0.0013463411064175235\n",
      "train loss:0.0012489207828271483\n",
      "train loss:0.0020059762183123656\n",
      "train loss:0.0006832037486448388\n",
      "train loss:0.004975152334833967\n",
      "train loss:0.006564138657385261\n",
      "train loss:0.005406882563926359\n",
      "train loss:0.0022477016369242776\n",
      "train loss:0.0034635884108917725\n",
      "train loss:0.0003767614050560142\n",
      "train loss:0.0025113835574770964\n",
      "train loss:0.0003598721592544829\n",
      "train loss:0.0008906091110337002\n",
      "train loss:0.0013610103847992569\n",
      "train loss:0.000572647916331393\n",
      "train loss:0.0007193223467133168\n",
      "train loss:0.0005048234112432159\n",
      "train loss:0.0014029002839377144\n",
      "train loss:0.00024047628758925113\n",
      "train loss:0.003741331831114793\n",
      "train loss:0.0008393924699708618\n",
      "train loss:0.0008963759962819858\n",
      "train loss:0.0004229119019617124\n",
      "train loss:0.0007523439721226281\n",
      "train loss:0.0019663958826539956\n",
      "train loss:2.7862868261073613e-05\n",
      "train loss:0.000294548595150206\n",
      "train loss:0.0019335842586652053\n",
      "train loss:0.0021195734772247047\n",
      "train loss:0.00044753209446936565\n",
      "train loss:0.0026020888217765416\n",
      "train loss:0.001966838481353734\n",
      "train loss:0.0001267584013755427\n",
      "train loss:0.0005330956026959131\n",
      "train loss:0.0011043725190320272\n",
      "train loss:0.0005650795708936783\n",
      "train loss:0.0034568762736601603\n",
      "train loss:0.0012379040953577668\n",
      "train loss:0.0018659470905831527\n",
      "train loss:0.0004122284300970356\n",
      "train loss:0.00040961794057759\n",
      "train loss:0.0001782106538865184\n",
      "train loss:0.0016994862638136606\n",
      "train loss:0.0001815579801230379\n",
      "train loss:0.0004896889648299381\n",
      "train loss:0.0005751249832389346\n",
      "train loss:0.001488520753221063\n",
      "train loss:0.006255019273597661\n",
      "train loss:0.008047214393910974\n",
      "train loss:0.00086876493866418\n",
      "train loss:0.00011157440063524559\n",
      "train loss:0.0003265151036135089\n",
      "train loss:0.0003065617841630516\n",
      "train loss:0.0004299967605135347\n",
      "train loss:0.0015404140155553473\n",
      "train loss:0.0030065234067050473\n",
      "train loss:0.00018028699024634425\n",
      "train loss:0.0005842659438241971\n",
      "train loss:0.0006400229209805973\n",
      "train loss:0.0009652797770577277\n",
      "train loss:0.003410535063647203\n",
      "train loss:0.0002601736524123346\n",
      "train loss:0.0004930217661865531\n",
      "train loss:0.0019429679861182184\n",
      "train loss:3.058164059697774e-05\n",
      "train loss:0.003924231421701465\n",
      "train loss:0.002848137196840841\n",
      "train loss:0.0017167559546482454\n",
      "train loss:0.00013260474356235888\n",
      "train loss:0.0043625806821146855\n",
      "train loss:0.0010031757275054867\n",
      "train loss:0.0003052106896306639\n",
      "train loss:0.002798484411258926\n",
      "train loss:0.0004682991411645438\n",
      "train loss:0.0016059284290906115\n",
      "train loss:0.00023584054572288248\n",
      "train loss:0.021769374877540562\n",
      "train loss:0.0020008924173786553\n",
      "train loss:0.00013228039616043315\n",
      "train loss:0.008514391684295752\n",
      "train loss:0.000533251011541696\n",
      "train loss:0.0045884573641503746\n",
      "train loss:0.0028158498560710075\n",
      "train loss:0.0001011461876502703\n",
      "train loss:0.04494445890124928\n",
      "train loss:0.00135853024760624\n",
      "train loss:0.00021314101844475378\n",
      "train loss:0.005078612213614665\n",
      "train loss:0.0006438033037372627\n",
      "train loss:0.004797890948687346\n",
      "train loss:0.0021099569183993853\n",
      "train loss:0.0003887399880268753\n",
      "train loss:0.0033221669686692118\n",
      "train loss:0.00018663553156930316\n",
      "train loss:0.0033779393399724846\n",
      "train loss:0.0005919504385320879\n",
      "train loss:0.0009736581764978712\n",
      "train loss:0.006907995433087117\n",
      "train loss:0.0021004058565841774\n",
      "train loss:0.001287588838116245\n",
      "train loss:0.0016319230617462988\n",
      "train loss:0.0006391724640680878\n",
      "train loss:0.0007269985777229527\n",
      "train loss:0.0009888737766811252\n",
      "train loss:0.0010707703145513925\n",
      "train loss:0.0014435799953151712\n",
      "train loss:0.001118513328006472\n",
      "train loss:0.0004416762432684312\n",
      "train loss:0.0009951725732416398\n",
      "train loss:0.003156971160830837\n",
      "train loss:0.003518343780901072\n",
      "train loss:0.007438491271635072\n",
      "train loss:0.00033848417868582087\n",
      "train loss:0.0011805499530045433\n",
      "train loss:0.0020106046119408925\n",
      "train loss:0.003922294372817112\n",
      "train loss:0.0029941967304596574\n",
      "train loss:0.003907092517416602\n",
      "train loss:0.002116969263431423\n",
      "train loss:0.004993484226104163\n",
      "train loss:0.00012088841263693715\n",
      "train loss:0.0018567340745622757\n",
      "train loss:0.0033972587188400983\n",
      "train loss:0.001014987363768906\n",
      "train loss:0.0011759359655202583\n",
      "train loss:0.0006449152573378492\n",
      "train loss:0.00012398904106950286\n",
      "train loss:0.0040143442092831225\n",
      "train loss:0.0017111850686338784\n",
      "train loss:0.00041145631313723135\n",
      "train loss:3.6305174054069394e-05\n",
      "train loss:0.00012908160849909588\n",
      "train loss:0.00010429874379837354\n",
      "train loss:0.0012357526780011469\n",
      "train loss:0.00016191227181952777\n",
      "train loss:0.0012283582938823843\n",
      "train loss:0.002015086374658696\n",
      "train loss:0.0012558785480707296\n",
      "train loss:0.0012706705616624151\n",
      "train loss:0.003068325615273004\n",
      "train loss:0.0017640003544492204\n",
      "train loss:0.0031047844627703452\n",
      "train loss:0.0002342890068852337\n",
      "train loss:0.00022483458748243635\n",
      "train loss:0.0001286033194840853\n",
      "train loss:0.002277857885986828\n",
      "train loss:0.00018566760203208216\n",
      "train loss:0.0017634066758241466\n",
      "train loss:7.158130632593828e-05\n",
      "train loss:0.0013577438545489318\n",
      "train loss:2.0916277087202556e-05\n",
      "train loss:0.004155462108978176\n",
      "train loss:0.0004960730589756262\n",
      "train loss:0.000479486138630127\n",
      "train loss:7.988595024316078e-05\n",
      "train loss:0.00016731129795378972\n",
      "train loss:4.1908281122650006e-05\n",
      "train loss:0.00024882123545300565\n",
      "train loss:0.008040313646881802\n",
      "train loss:0.0023442838816434834\n",
      "train loss:0.00020207911783055212\n",
      "train loss:0.0007970339673278016\n",
      "train loss:0.000109246314609889\n",
      "train loss:0.0002379204692645053\n",
      "train loss:0.003938543883063614\n",
      "train loss:0.0005134166652200301\n",
      "train loss:0.0008664724974678656\n",
      "train loss:9.14688109817899e-05\n",
      "train loss:0.0009638662924533653\n",
      "train loss:0.0005262004934847332\n",
      "train loss:0.00016172803528570058\n",
      "train loss:0.0003578103645716285\n",
      "train loss:0.000159042239201247\n",
      "train loss:0.00029469448238858743\n",
      "train loss:0.0001739444857289923\n",
      "train loss:0.0004742170397394517\n",
      "train loss:0.0019390570516869246\n",
      "train loss:0.0031708532888954028\n",
      "train loss:6.360957048791812e-05\n",
      "train loss:0.0012985650086936723\n",
      "train loss:0.00048204963656055705\n",
      "train loss:0.00013477062839349228\n",
      "train loss:0.0016474108714862896\n",
      "train loss:0.0004761368207879051\n",
      "train loss:4.999011466664583e-05\n",
      "train loss:0.001349585056208639\n",
      "train loss:0.01600788028986864\n",
      "train loss:0.0019516810215351305\n",
      "train loss:0.004442466273123653\n",
      "train loss:0.0001996771351308875\n",
      "train loss:0.0004285476486252851\n",
      "train loss:0.0023149112448992057\n",
      "train loss:0.00017262502172315963\n",
      "train loss:0.0001732145766192808\n",
      "train loss:0.00018482704173611742\n",
      "train loss:0.0007504309771393669\n",
      "train loss:0.0012443226960751623\n",
      "train loss:0.0013375671590615374\n",
      "train loss:0.00038562125924869093\n",
      "train loss:0.004140495034398193\n",
      "train loss:0.0030693043822304232\n",
      "train loss:6.450245683020305e-05\n",
      "train loss:0.0021495020620181\n",
      "train loss:0.0006634805188023241\n",
      "train loss:0.0006642478869396454\n",
      "train loss:0.0022357526662535442\n",
      "train loss:0.00017229534044444706\n",
      "train loss:0.004535451287911753\n",
      "train loss:0.00042087624385440925\n",
      "train loss:7.971721183992964e-05\n",
      "train loss:0.001761204181424584\n",
      "train loss:0.0012928814836697174\n",
      "train loss:0.004150976706323718\n",
      "train loss:0.0002849663755635966\n",
      "train loss:0.00014161999829366014\n",
      "train loss:0.0002913776209464573\n",
      "train loss:0.00020363225537883\n",
      "train loss:0.00047669995951982495\n",
      "train loss:0.00025119747536364795\n",
      "train loss:0.0008088532961546439\n",
      "train loss:0.00017938717916987038\n",
      "train loss:0.0006740529689241386\n",
      "train loss:5.508904884340039e-05\n",
      "train loss:0.0004315682339469454\n",
      "train loss:9.390348656041484e-05\n",
      "train loss:0.0003850054012908488\n",
      "train loss:0.006205244561198507\n",
      "train loss:0.0006093444070245812\n",
      "train loss:3.32152657075746e-05\n",
      "train loss:0.00012566579965742086\n",
      "train loss:0.00266676738352908\n",
      "train loss:0.0010836716237936498\n",
      "train loss:0.0008596562489188074\n",
      "train loss:0.000936919969990594\n",
      "train loss:0.00032240359412179486\n",
      "train loss:0.00035145977911512396\n",
      "train loss:4.045067228053368e-05\n",
      "train loss:0.00028577087649579047\n",
      "train loss:0.00043728200722224374\n",
      "train loss:0.00011924177604610869\n",
      "train loss:0.008752815487587254\n",
      "train loss:0.0014281273353521909\n",
      "train loss:0.0005680368579718297\n",
      "train loss:0.012876381155841355\n",
      "train loss:0.0023533706527661797\n",
      "train loss:0.0018183438212673686\n",
      "train loss:0.000903758420680048\n",
      "train loss:0.0009717843668405116\n",
      "train loss:0.00015935117410194354\n",
      "train loss:0.0002324498207257324\n",
      "train loss:0.0007420227257682239\n",
      "train loss:0.0009412703892700064\n",
      "train loss:0.003826622117590584\n",
      "train loss:0.00023132297940696333\n",
      "current iter num:  10800\n",
      "=== epoch:19, train acc:0.998, test acc:0.988 ===\n",
      "train loss:0.005906424965338684\n",
      "train loss:0.00242770410142793\n",
      "train loss:0.00023419142000744814\n",
      "train loss:0.0007324478867465527\n",
      "train loss:0.0008536470408125324\n",
      "train loss:0.0001260795690880743\n",
      "train loss:0.00010621327124039884\n",
      "train loss:0.0005537249348225172\n",
      "train loss:0.003155412155953429\n",
      "train loss:0.000382362884069449\n",
      "train loss:0.0013914165616156471\n",
      "train loss:0.0002926836886619662\n",
      "train loss:0.00027513101192456774\n",
      "train loss:0.00017980339217185063\n",
      "train loss:0.00021782735118583557\n",
      "train loss:0.000792761507150208\n",
      "train loss:0.004492104854048117\n",
      "train loss:0.0004047968648465593\n",
      "train loss:0.001254593391953689\n",
      "train loss:0.0030901282394436348\n",
      "train loss:6.40059180516046e-05\n",
      "train loss:0.0011690917049570217\n",
      "train loss:0.0011567651400121758\n",
      "train loss:0.0009813718015067251\n",
      "train loss:0.001349670651513944\n",
      "train loss:0.003761248162481705\n",
      "train loss:0.0004658936806942648\n",
      "train loss:0.0003495582542566137\n",
      "train loss:0.0007636394415967244\n",
      "train loss:8.213372991724286e-06\n",
      "train loss:0.0007145066679297426\n",
      "train loss:0.0003986632166378912\n",
      "train loss:0.000810177316923178\n",
      "train loss:0.0009617021265152234\n",
      "train loss:0.004621370533933789\n",
      "train loss:0.00016229460395240903\n",
      "train loss:0.0011398043743619877\n",
      "train loss:7.672392494466595e-05\n",
      "train loss:0.0010256575833066104\n",
      "train loss:8.206362236182163e-05\n",
      "train loss:0.00014177851044288358\n",
      "train loss:0.00012911000014312663\n",
      "train loss:0.00048191631951304215\n",
      "train loss:0.002791438835107205\n",
      "train loss:0.007368821690033059\n",
      "train loss:0.000264966174713246\n",
      "train loss:0.0004092324599719413\n",
      "train loss:0.0004011794770844705\n",
      "train loss:0.00037461355126295346\n",
      "train loss:0.0016067971885063053\n",
      "train loss:0.001912962137890105\n",
      "train loss:0.00024825167217418893\n",
      "train loss:0.0016026536064943323\n",
      "train loss:0.0004196108941376852\n",
      "train loss:0.0005132426522288029\n",
      "train loss:0.0035406582861708395\n",
      "train loss:0.001518441662949437\n",
      "train loss:0.0025029138506233828\n",
      "train loss:0.0014437064107653708\n",
      "train loss:0.0033342030037322324\n",
      "train loss:0.001093353351441705\n",
      "train loss:0.0013039408408570489\n",
      "train loss:0.003425077267171488\n",
      "train loss:0.00018989600773997156\n",
      "train loss:0.0001342023554844516\n",
      "train loss:0.00023373220499186637\n",
      "train loss:0.00018519018112098706\n",
      "train loss:0.001230905777358974\n",
      "train loss:0.0010968021445969347\n",
      "train loss:0.00015817427038356918\n",
      "train loss:0.0016591557914387922\n",
      "train loss:0.00035928271177427455\n",
      "train loss:0.005544924121649004\n",
      "train loss:0.000614216877038105\n",
      "train loss:0.0019954192015167326\n",
      "train loss:0.0005890710721832618\n",
      "train loss:0.0004908462929213613\n",
      "train loss:0.0304482198692072\n",
      "train loss:0.00044774393749802405\n",
      "train loss:0.0009389706390685365\n",
      "train loss:0.009769556022855336\n",
      "train loss:0.0003917274427866697\n",
      "train loss:3.271778487641378e-05\n",
      "train loss:0.0002787987572169401\n",
      "train loss:6.714295293989499e-05\n",
      "train loss:0.007592942979973478\n",
      "train loss:0.001661261096658401\n",
      "train loss:0.0008170882965223117\n",
      "train loss:0.00022763659798015235\n",
      "train loss:0.0009009571788566347\n",
      "train loss:0.0003677750673955492\n",
      "train loss:7.729930007628138e-05\n",
      "train loss:0.0015070910177165109\n",
      "train loss:0.00010137238541137507\n",
      "train loss:0.0010391669199484423\n",
      "train loss:8.393009297972732e-05\n",
      "train loss:0.001080402393446321\n",
      "train loss:0.0026293002641088036\n",
      "train loss:0.002346787981362872\n",
      "train loss:0.002172222321191219\n",
      "train loss:0.001090070459055184\n",
      "train loss:0.0007994561129650536\n",
      "train loss:0.0007634981825597168\n",
      "train loss:0.0019984916493743644\n",
      "train loss:0.009427651126787527\n",
      "train loss:0.0009664806838628677\n",
      "train loss:0.0004331087306322283\n",
      "train loss:0.0019407018684002458\n",
      "train loss:7.150253492581233e-05\n",
      "train loss:0.001046038703200488\n",
      "train loss:0.0009987325504820488\n",
      "train loss:0.001147240617695279\n",
      "train loss:0.0003231807365441098\n",
      "train loss:0.019932835309261945\n",
      "train loss:8.506997746508383e-05\n",
      "train loss:4.949590185983551e-05\n",
      "train loss:0.0011532817101167633\n",
      "train loss:0.002143725687092634\n",
      "train loss:0.000631378189180381\n",
      "train loss:0.0006868733298625644\n",
      "train loss:0.00013525520193646896\n",
      "train loss:0.00035752820651053276\n",
      "train loss:0.00015899891381814985\n",
      "train loss:0.004663402569425255\n",
      "train loss:0.00038414957457120173\n",
      "train loss:0.0016920484678593884\n",
      "train loss:0.004013843230332434\n",
      "train loss:0.006465966446001277\n",
      "train loss:0.0010476410438400214\n",
      "train loss:0.003897868497872561\n",
      "train loss:0.001663980509638296\n",
      "train loss:0.0004455955234687487\n",
      "train loss:0.0013567655710402854\n",
      "train loss:0.0004940684746187058\n",
      "train loss:0.0015800402432188825\n",
      "train loss:0.007710224740108703\n",
      "train loss:0.0011018495138985914\n",
      "train loss:0.007575655558146061\n",
      "train loss:0.007165974996887534\n",
      "train loss:0.00040023648304380834\n",
      "train loss:0.004811028691628982\n",
      "train loss:0.00017294965829919346\n",
      "train loss:0.0059350327647704195\n",
      "train loss:0.0009256261408625478\n",
      "train loss:0.0011444972620476038\n",
      "train loss:0.00015190531089212406\n",
      "train loss:0.0022934321401212685\n",
      "train loss:0.0007711673694282456\n",
      "train loss:0.0027059258464123314\n",
      "train loss:0.0002364556437761773\n",
      "train loss:9.605747760121348e-05\n",
      "train loss:0.002562551474416699\n",
      "train loss:0.0006292677063881597\n",
      "train loss:0.0003551982799399886\n",
      "train loss:0.00156255507234485\n",
      "train loss:0.001248631761629809\n",
      "train loss:0.00041757792651563806\n",
      "train loss:0.00013569450597087488\n",
      "train loss:0.0002236433584696843\n",
      "train loss:0.0026885908878537057\n",
      "train loss:0.0004319104663887196\n",
      "train loss:0.00029306861584274454\n",
      "train loss:0.00024166158047651297\n",
      "train loss:0.0009070406936466221\n",
      "train loss:0.0013144334464409474\n",
      "train loss:0.001105477967191816\n",
      "train loss:6.32196241581191e-05\n",
      "train loss:0.0003551177924348695\n",
      "train loss:0.0020359466705210335\n",
      "train loss:0.00019481067931630334\n",
      "train loss:0.00012674553781157709\n",
      "train loss:0.001093155960562815\n",
      "train loss:0.002581923597022884\n",
      "train loss:0.0009399564757587616\n",
      "train loss:0.010457298736394243\n",
      "train loss:2.7788232252520793e-05\n",
      "train loss:0.00026721480770363254\n",
      "train loss:0.00017012200156163684\n",
      "train loss:0.0014734136402300172\n",
      "train loss:0.0001824446272312523\n",
      "train loss:6.081215468789847e-05\n",
      "train loss:0.0005392210577322342\n",
      "train loss:0.002226532814585971\n",
      "train loss:0.0014432166645837233\n",
      "train loss:0.0028193386019166513\n",
      "train loss:0.0033917567239111573\n",
      "train loss:6.881820129074922e-05\n",
      "train loss:0.0001986322792740934\n",
      "train loss:0.0034843043799056184\n",
      "train loss:0.00010915965299690766\n",
      "train loss:0.00030661735871373213\n",
      "train loss:0.0009657808408977196\n",
      "train loss:0.000639269295962185\n",
      "train loss:0.0004876287813776747\n",
      "train loss:5.865263845018542e-05\n",
      "train loss:0.0006519690270512781\n",
      "train loss:0.0018700607534457348\n",
      "train loss:0.0001231767337468183\n",
      "train loss:0.0012469967562347675\n",
      "train loss:0.0004331851910871898\n",
      "train loss:0.006468328645221289\n",
      "train loss:0.00024068791122919503\n",
      "train loss:0.0020780131444664593\n",
      "train loss:0.0009254209504421127\n",
      "train loss:9.705802112268812e-05\n",
      "train loss:0.00014148628681975539\n",
      "train loss:0.0006937635660480205\n",
      "train loss:0.0023738378931738276\n",
      "train loss:0.00019550455087256855\n",
      "train loss:0.0017333787181850358\n",
      "train loss:0.0006959318696729614\n",
      "train loss:6.346865965264735e-05\n",
      "train loss:0.000865329986961682\n",
      "train loss:0.0032366818818129247\n",
      "train loss:9.522881514198947e-05\n",
      "train loss:0.0032774808042995607\n",
      "train loss:0.000596564787127519\n",
      "train loss:0.003408023095818899\n",
      "train loss:0.003576320616769175\n",
      "train loss:0.0017228418330348736\n",
      "train loss:0.00020778477427124708\n",
      "train loss:0.0016730667848129047\n",
      "train loss:0.001423212167280427\n",
      "train loss:5.2172896440606934e-05\n",
      "train loss:0.0010065666707415322\n",
      "train loss:0.000704203032747056\n",
      "train loss:3.631261607434241e-05\n",
      "train loss:7.568010541914003e-05\n",
      "train loss:0.0005500351330643048\n",
      "train loss:0.000855007677428951\n",
      "train loss:0.00135713105567337\n",
      "train loss:0.0032664976928737217\n",
      "train loss:0.00027206849566142714\n",
      "train loss:0.0015068078405283038\n",
      "train loss:0.005472178412356525\n",
      "train loss:0.002113361255502824\n",
      "train loss:0.0009618600169845169\n",
      "train loss:8.791181630992977e-05\n",
      "train loss:0.00011335444955872031\n",
      "train loss:0.00034881430879561475\n",
      "train loss:0.001527600633979168\n",
      "train loss:0.0015336989929485402\n",
      "train loss:0.0018874619563272667\n",
      "train loss:0.00038163291796112386\n",
      "train loss:0.0003131393183274978\n",
      "train loss:0.0012666465705842042\n",
      "train loss:0.0028127396658814896\n",
      "train loss:0.001339840458896147\n",
      "train loss:4.444399869089607e-05\n",
      "train loss:0.004526499041939755\n",
      "train loss:0.0018386683744024879\n",
      "train loss:0.0012424543503346162\n",
      "train loss:0.00046240754430757375\n",
      "train loss:0.00016753602622958258\n",
      "train loss:0.00048639669012443015\n",
      "train loss:0.0003032415930316077\n",
      "train loss:0.0043912489241883556\n",
      "train loss:0.00027066863335573765\n",
      "train loss:0.0003272369790831519\n",
      "train loss:0.000977576171916971\n",
      "train loss:0.00046239206543785723\n",
      "train loss:0.0014898377390468998\n",
      "train loss:0.0013023077540939016\n",
      "train loss:1.790983006902127e-05\n",
      "train loss:0.0011869299209698336\n",
      "train loss:0.00577285940712743\n",
      "train loss:0.0021700787864877783\n",
      "train loss:0.00028448195324420883\n",
      "train loss:0.0025484056160296853\n",
      "train loss:0.0027003458581710206\n",
      "train loss:0.0001226624605527128\n",
      "train loss:0.0011375458391483157\n",
      "train loss:0.004467264950868185\n",
      "train loss:0.0006464615577763425\n",
      "train loss:0.00037787848905871687\n",
      "train loss:0.0019810004461536515\n",
      "train loss:4.149057651901915e-05\n",
      "train loss:0.0004534223250381431\n",
      "train loss:9.428029715624256e-05\n",
      "train loss:0.00014901897151646965\n",
      "train loss:0.0007233919299195872\n",
      "train loss:0.0003063628598854487\n",
      "train loss:0.0001603051906885462\n",
      "train loss:0.0003554115723709547\n",
      "train loss:0.0013316311867796959\n",
      "train loss:0.002991357821022205\n",
      "train loss:0.0047965906317395356\n",
      "train loss:0.0015108840425073452\n",
      "train loss:0.0010072289456474583\n",
      "train loss:0.0005190625382245999\n",
      "train loss:0.0002724702382605127\n",
      "train loss:0.004442166325814571\n",
      "train loss:0.004008074275631606\n",
      "train loss:0.004554065370820173\n",
      "train loss:0.001455603786109052\n",
      "train loss:0.0007941825148697855\n",
      "train loss:0.0040645924013394275\n",
      "train loss:6.864734240025971e-05\n",
      "train loss:0.0008594379161676077\n",
      "train loss:0.0005070649278487703\n",
      "train loss:9.345215316329453e-05\n",
      "train loss:0.0001564145272836968\n",
      "train loss:0.015577733123504942\n",
      "train loss:0.0008975108610327284\n",
      "train loss:0.0002069842114885807\n",
      "train loss:0.002689375732010651\n",
      "train loss:0.0006281522820513553\n",
      "train loss:0.026059025352515714\n",
      "train loss:0.0009387989795793636\n",
      "train loss:0.0018682561202454891\n",
      "train loss:0.0028482233403003127\n",
      "train loss:3.698656541829817e-05\n",
      "train loss:0.00028070436810434535\n",
      "train loss:0.0004726881995653706\n",
      "train loss:8.343715384931615e-05\n",
      "train loss:6.707469166857189e-05\n",
      "train loss:0.0007476214829511408\n",
      "train loss:0.0006581460599642542\n",
      "train loss:0.0018344395053647652\n",
      "train loss:0.0018627719047495839\n",
      "train loss:0.00011586606355895998\n",
      "train loss:0.000703361603010797\n",
      "train loss:4.944733194118202e-05\n",
      "train loss:0.0028432840862985353\n",
      "train loss:0.00024928199095347\n",
      "train loss:0.001065167439263744\n",
      "train loss:0.02375512263121481\n",
      "train loss:0.003154407616665318\n",
      "train loss:0.00033933882618174255\n",
      "train loss:0.005421627016810502\n",
      "train loss:0.0013752578149133546\n",
      "train loss:0.02669413511051622\n",
      "train loss:0.0013712650988537698\n",
      "train loss:0.0007163952466089487\n",
      "train loss:0.0026946835438380317\n",
      "train loss:0.0002034653968010365\n",
      "train loss:0.00014783519039339202\n",
      "train loss:0.002525585865005825\n",
      "train loss:0.0003162729660804713\n",
      "train loss:0.001968745800793312\n",
      "train loss:0.0015130846568625872\n",
      "train loss:0.0012625752479260741\n",
      "train loss:0.002404427350952335\n",
      "train loss:0.001840798047196471\n",
      "train loss:0.0017533623087639654\n",
      "train loss:0.005765064483439109\n",
      "train loss:0.0001496994914658858\n",
      "train loss:0.0008960830383025217\n",
      "train loss:0.0005203307050573734\n",
      "train loss:0.0014032042610338344\n",
      "train loss:0.001671927865926611\n",
      "train loss:0.00013030112071296267\n",
      "train loss:0.0006294903488353507\n",
      "train loss:0.0004885467543999926\n",
      "train loss:0.007928149514006288\n",
      "train loss:0.001799092563099153\n",
      "train loss:0.0019169031170993315\n",
      "train loss:0.002430309796888854\n",
      "train loss:0.001084527445935186\n",
      "train loss:0.00023441112862044949\n",
      "train loss:0.000140075510434829\n",
      "train loss:0.0007238952813470777\n",
      "train loss:0.00013851583547680856\n",
      "train loss:0.0018833431654197946\n",
      "train loss:0.0007863606977095835\n",
      "train loss:0.0008769831391433777\n",
      "train loss:0.00016836244888167277\n",
      "train loss:0.0008879627049819387\n",
      "train loss:0.0007747537493113306\n",
      "train loss:4.50925768555892e-05\n",
      "train loss:0.0011951497410192175\n",
      "train loss:0.000767926503754773\n",
      "train loss:0.005538264165523448\n",
      "train loss:7.872790307773776e-05\n",
      "train loss:0.0006100185671447503\n",
      "train loss:0.0035198091161446955\n",
      "train loss:0.0010409849921740536\n",
      "train loss:0.0011818843061489849\n",
      "train loss:0.001328247064698329\n",
      "train loss:0.0005464978697181091\n",
      "train loss:0.0005381968590806133\n",
      "train loss:0.0038925272865524994\n",
      "train loss:0.00373926689487591\n",
      "train loss:0.00020576842298513222\n",
      "train loss:0.00021280285077226286\n",
      "train loss:0.0016942108992044444\n",
      "train loss:0.00039312498403405835\n",
      "train loss:0.00070834719153858\n",
      "train loss:0.024482310556635724\n",
      "train loss:6.423105642309675e-05\n",
      "train loss:0.0017993110170370424\n",
      "train loss:0.0005282145455698711\n",
      "train loss:0.0035517217357592928\n",
      "train loss:0.001959482172920164\n",
      "train loss:0.0019284732104995295\n",
      "train loss:0.0025664228432710993\n",
      "train loss:0.0013697965140619043\n",
      "train loss:0.001554654305299354\n",
      "train loss:0.00201847921574676\n",
      "train loss:0.0011117042496761856\n",
      "train loss:0.00012038039427775626\n",
      "train loss:0.0024043215163738864\n",
      "train loss:0.0010500442563111932\n",
      "train loss:0.0034313599566209475\n",
      "train loss:0.005366933270682547\n",
      "train loss:0.0033845345439583946\n",
      "train loss:0.00012427507484080838\n",
      "train loss:0.002779360688620971\n",
      "train loss:0.0004387598002869063\n",
      "train loss:0.0008198015242699276\n",
      "train loss:0.00016592188037282623\n",
      "train loss:0.0019077262023319596\n",
      "train loss:0.0002882232034605013\n",
      "train loss:0.01275500990630847\n",
      "train loss:0.0022037737838999528\n",
      "train loss:0.008670475706792285\n",
      "train loss:0.0026570394512431842\n",
      "train loss:0.002331477109376522\n",
      "train loss:0.0016785699138914644\n",
      "train loss:0.0043370966659698\n",
      "train loss:0.00030143625880282594\n",
      "train loss:0.0012470309716603337\n",
      "train loss:0.0006074498853022505\n",
      "train loss:0.00024431939015968505\n",
      "train loss:0.004009239541135131\n",
      "train loss:0.003720019946857232\n",
      "train loss:0.0035844104228305662\n",
      "train loss:0.0009220451334846723\n",
      "train loss:0.00020471702705422647\n",
      "train loss:0.0025469839313129123\n",
      "train loss:0.0008920473302773663\n",
      "train loss:0.00154029456771109\n",
      "train loss:0.009535086713192012\n",
      "train loss:3.247929111576204e-05\n",
      "train loss:0.0017842354236436884\n",
      "train loss:0.001575209924310492\n",
      "train loss:0.00028400586603115184\n",
      "train loss:0.00033376066461950765\n",
      "train loss:0.0004872793284725816\n",
      "train loss:0.0012181205560292591\n",
      "train loss:0.0006531843396566919\n",
      "train loss:0.0006491093309430397\n",
      "train loss:0.0003689293679720118\n",
      "train loss:0.0017389765540365442\n",
      "train loss:0.00047962955492469395\n",
      "train loss:0.003625431984555123\n",
      "train loss:0.00042892507508178576\n",
      "train loss:0.0003265153213623481\n",
      "train loss:0.0002657805005779043\n",
      "train loss:0.0003663794420528913\n",
      "train loss:0.0023727870447154058\n",
      "train loss:0.0025966490624359743\n",
      "train loss:0.0009740228274588584\n",
      "train loss:0.001122118945145585\n",
      "train loss:0.0015936531395669832\n",
      "train loss:0.00016396627175912202\n",
      "train loss:0.0017444775579926156\n",
      "train loss:0.00327550780903304\n",
      "train loss:0.012334418669104608\n",
      "train loss:0.002297079670350209\n",
      "train loss:0.00015162229440693494\n",
      "train loss:0.030062734159630348\n",
      "train loss:0.00012935402659184495\n",
      "train loss:0.00846444049017473\n",
      "train loss:0.0030957582371406367\n",
      "train loss:0.00021678817631608907\n",
      "train loss:0.018513839301842933\n",
      "train loss:0.0017461755746343824\n",
      "train loss:0.005414333480652778\n",
      "train loss:0.001709637796793225\n",
      "train loss:0.0014329421833692974\n",
      "train loss:0.0009054340144053926\n",
      "train loss:0.0024774305647350544\n",
      "train loss:0.006464185530752088\n",
      "train loss:0.0005889908768674846\n",
      "train loss:0.0026874092662630715\n",
      "train loss:0.01741035540607496\n",
      "train loss:0.001970926722762617\n",
      "train loss:0.006082084593942542\n",
      "train loss:0.0006015834506730924\n",
      "train loss:0.001875060182168189\n",
      "train loss:9.213494969533074e-05\n",
      "train loss:0.0022831991492129546\n",
      "train loss:0.0021934789781118567\n",
      "train loss:0.00025217677065194764\n",
      "train loss:0.007787687529030179\n",
      "train loss:0.0001564294468446191\n",
      "train loss:0.00021699490823033678\n",
      "train loss:0.000585917096195123\n",
      "train loss:0.0012225242542227822\n",
      "train loss:0.0010707412018517363\n",
      "train loss:0.0007418478520874853\n",
      "train loss:0.0005150947708021845\n",
      "train loss:0.0015792204438083671\n",
      "train loss:0.0028579881932688715\n",
      "train loss:0.002700127521138213\n",
      "train loss:0.00016993207225769923\n",
      "train loss:0.0035297620921155913\n",
      "train loss:0.011328731969713932\n",
      "train loss:0.002987380303127146\n",
      "train loss:0.0006450300312843247\n",
      "train loss:0.0030965025523229084\n",
      "train loss:8.893267524197271e-05\n",
      "train loss:0.005621261428269653\n",
      "train loss:0.0026391061339459537\n",
      "train loss:0.001984470947893773\n",
      "train loss:0.001316462846791196\n",
      "train loss:0.0027319076127714313\n",
      "train loss:0.011260907393302655\n",
      "train loss:0.010721118178290719\n",
      "train loss:0.0020063644001047493\n",
      "train loss:0.001916730754878192\n",
      "train loss:0.001977218422123143\n",
      "train loss:0.0023196656861972785\n",
      "train loss:0.000897610434024825\n",
      "train loss:0.0007290796505967054\n",
      "train loss:0.0017935744371219428\n",
      "train loss:0.001453181504951842\n",
      "train loss:0.0006046348883967142\n",
      "train loss:0.003921446248035863\n",
      "train loss:0.002375316882812143\n",
      "train loss:0.0008604225499122418\n",
      "train loss:0.0016348662274231248\n",
      "train loss:0.0015065621969168783\n",
      "train loss:0.0008418236020658739\n",
      "train loss:0.002848903170871119\n",
      "train loss:0.0006444553717152217\n",
      "train loss:0.0019510832132886788\n",
      "train loss:0.006191032762734786\n",
      "train loss:0.0005804329216995723\n",
      "train loss:0.0026128865920916123\n",
      "train loss:0.0009169761273318776\n",
      "train loss:0.0006726058946638367\n",
      "train loss:0.0004898877110868831\n",
      "train loss:0.0007167367156874735\n",
      "train loss:0.00047756429654766884\n",
      "train loss:0.010007761340885076\n",
      "train loss:0.0007657184309103328\n",
      "train loss:0.001254815294945896\n",
      "train loss:0.0016315931152553588\n",
      "train loss:0.001100406698185894\n",
      "train loss:0.00022102187438180598\n",
      "train loss:0.0007738110930333378\n",
      "train loss:0.0026490913925393067\n",
      "train loss:0.0012125211725511984\n",
      "train loss:0.00041321006902014817\n",
      "train loss:0.0001093777314614076\n",
      "train loss:0.006355511544429048\n",
      "train loss:0.0001863343020153779\n",
      "train loss:0.002394094221567265\n",
      "train loss:0.0006714885664848082\n",
      "train loss:0.0002760476324505732\n",
      "train loss:0.00048059824825862177\n",
      "train loss:0.0004960814890900535\n",
      "train loss:0.0024328974948997534\n",
      "train loss:0.0014425230495244369\n",
      "train loss:0.00022986867878783024\n",
      "train loss:0.0007864382718869656\n",
      "train loss:0.0017401569819921838\n",
      "train loss:0.0012648365787867176\n",
      "train loss:0.001011645556595236\n",
      "train loss:0.0036253779505959795\n",
      "train loss:0.00023615357527694537\n",
      "train loss:0.002250205358263475\n",
      "train loss:0.0016268188326080233\n",
      "train loss:0.0004361838457847855\n",
      "train loss:0.0021595204785547694\n",
      "train loss:6.111707856317341e-05\n",
      "train loss:0.009398296971054477\n",
      "train loss:0.00040607419122216037\n",
      "train loss:0.00025672770943067685\n",
      "train loss:0.0012059564198862774\n",
      "train loss:3.959347285621157e-05\n",
      "train loss:0.0013323511786024183\n",
      "train loss:0.0009511702149326506\n",
      "train loss:0.0001698145463491415\n",
      "train loss:0.0012021750342566763\n",
      "train loss:0.0013566667007680882\n",
      "train loss:0.0006190202847664672\n",
      "train loss:0.002891425439114421\n",
      "train loss:0.0012237634169602008\n",
      "train loss:0.0009497240298338033\n",
      "train loss:0.0010667983548446858\n",
      "train loss:0.0020111886951112963\n",
      "train loss:0.0069266075923522805\n",
      "train loss:0.0015692119602460806\n",
      "train loss:0.00042081630553226644\n",
      "train loss:0.0022145441953185\n",
      "train loss:0.00014444037597702473\n",
      "train loss:0.0005442280106821262\n",
      "train loss:0.00016918812782074\n",
      "train loss:0.001750967119791311\n",
      "train loss:0.016059143170815623\n",
      "train loss:0.0034522234643230123\n",
      "train loss:0.002506202865079759\n",
      "train loss:0.002905148475304818\n",
      "train loss:3.8909480043929476e-05\n",
      "train loss:0.0013157430965081013\n",
      "train loss:0.0005062665621791872\n",
      "train loss:0.0016460532010394227\n",
      "current iter num:  11400\n",
      "=== epoch:20, train acc:0.997, test acc:0.985 ===\n",
      "train loss:0.0002475069699494131\n",
      "train loss:9.116218095509896e-05\n",
      "train loss:0.0006031013931233202\n",
      "train loss:0.0016053158793433745\n",
      "train loss:0.0004504210854599249\n",
      "train loss:0.003789537640256939\n",
      "train loss:0.0029709484020681194\n",
      "train loss:0.00013219356193577657\n",
      "train loss:0.0005585279377368549\n",
      "train loss:2.870089471707572e-05\n",
      "train loss:0.0003459822911794766\n",
      "train loss:0.00013876902355895417\n",
      "train loss:0.0008298122239650011\n",
      "train loss:0.0005037246894757062\n",
      "train loss:0.0003800546672142003\n",
      "train loss:0.0008851221822042467\n",
      "train loss:9.87810714076192e-05\n",
      "train loss:0.0008872956503857617\n",
      "train loss:0.002082280067233407\n",
      "train loss:9.778437340050246e-05\n",
      "train loss:6.782251850147488e-05\n",
      "train loss:0.00047230414040861686\n",
      "train loss:0.0006017253731137887\n",
      "train loss:0.004291300997847671\n",
      "train loss:8.43052203031554e-05\n",
      "train loss:0.0001707545956081219\n",
      "train loss:0.005058072029165664\n",
      "train loss:0.00018441228935219968\n",
      "train loss:0.0003740127405712583\n",
      "train loss:0.0002332871799068065\n",
      "train loss:0.0006555871840712542\n",
      "train loss:0.0018618151106960773\n",
      "train loss:5.297557028364535e-05\n",
      "train loss:0.006239433043504176\n",
      "train loss:0.0001367872821221274\n",
      "train loss:0.0009237880474698681\n",
      "train loss:0.00030258499407217636\n",
      "train loss:0.0003189245998311002\n",
      "train loss:0.0002652853467920087\n",
      "train loss:0.0006956554208083509\n",
      "train loss:0.0005443655385209945\n",
      "train loss:0.0026019667734298087\n",
      "train loss:0.000720051770948307\n",
      "train loss:0.007364384661375456\n",
      "train loss:0.0003593920098416241\n",
      "train loss:0.0006238864462583437\n",
      "train loss:0.00019625789817402214\n",
      "train loss:0.002028814316457538\n",
      "train loss:0.0008671761726939758\n",
      "train loss:0.00022898396776478109\n",
      "train loss:0.0018383093413170707\n",
      "train loss:0.006646848320925844\n",
      "train loss:0.0005247635425413594\n",
      "train loss:0.00014901091233343914\n",
      "train loss:0.0011062006479341056\n",
      "train loss:0.0001761886502590956\n",
      "train loss:0.0005237550052769091\n",
      "train loss:0.0014070246612871578\n",
      "train loss:0.0003819659983002748\n",
      "train loss:0.0001128329178048972\n",
      "train loss:0.00019516615542064817\n",
      "train loss:0.00028659682858465935\n",
      "train loss:0.00023109297516368547\n",
      "train loss:0.0004962739522280983\n",
      "train loss:0.0002797125756514268\n",
      "train loss:0.0010981550373472815\n",
      "train loss:0.004430588853499488\n",
      "train loss:0.00033895375120228513\n",
      "train loss:0.00013240800473164415\n",
      "train loss:0.0005373043405573682\n",
      "train loss:0.0011696185732356656\n",
      "train loss:0.0031809700092893926\n",
      "train loss:6.365504145871623e-05\n",
      "train loss:0.0014045017717214093\n",
      "train loss:0.00027640831456667155\n",
      "train loss:0.0002037705050272595\n",
      "train loss:0.0036630702764932736\n",
      "train loss:0.0002396911823869149\n",
      "train loss:0.003373360974844933\n",
      "train loss:8.973389759906578e-05\n",
      "train loss:0.0026487095201580984\n",
      "train loss:0.0005685849275623275\n",
      "train loss:0.0005661420954540261\n",
      "train loss:8.556992258511713e-05\n",
      "train loss:0.0010802363404618675\n",
      "train loss:0.00024168904165799786\n",
      "train loss:0.0012840530900853195\n",
      "train loss:1.7829061317758974e-05\n",
      "train loss:0.001336074522604467\n",
      "train loss:0.00017305224350481357\n",
      "train loss:0.0004370958702143398\n",
      "train loss:0.0015814924718002324\n",
      "train loss:0.0005612422184209701\n",
      "train loss:0.0009816381755755423\n",
      "train loss:0.002107905771058435\n",
      "train loss:0.0039365528490961955\n",
      "train loss:0.00029208140930361593\n",
      "train loss:0.0005987267741539172\n",
      "train loss:0.00010736796650108823\n",
      "train loss:8.04892970570735e-05\n",
      "train loss:0.0016743743133165393\n",
      "train loss:0.0007601912989248446\n",
      "train loss:2.762989982084359e-05\n",
      "train loss:0.0001268199625543268\n",
      "train loss:0.0007290567730582669\n",
      "train loss:0.0008017167211220038\n",
      "train loss:0.0001457293199908549\n",
      "train loss:0.0058630601982982535\n",
      "train loss:0.0001223046045676006\n",
      "train loss:0.0001401474778091839\n",
      "train loss:0.0016546514820317568\n",
      "train loss:0.001140054006841433\n",
      "train loss:0.0012556384956243455\n",
      "train loss:6.743715804576168e-05\n",
      "train loss:0.0008164486905289504\n",
      "train loss:0.003934588546181075\n",
      "train loss:0.000524806449095945\n",
      "train loss:0.0001280008746250565\n",
      "train loss:0.0008925859781392738\n",
      "train loss:0.010844796505632037\n",
      "train loss:0.00018561427126478046\n",
      "train loss:0.0017113644369998828\n",
      "train loss:9.592927006917472e-05\n",
      "train loss:9.781607363789321e-05\n",
      "train loss:0.00012687047642201342\n",
      "train loss:0.0018722014370697005\n",
      "train loss:0.001239767470954539\n",
      "train loss:0.0007160036593487632\n",
      "train loss:6.668575751322567e-05\n",
      "train loss:0.0012376234729722015\n",
      "train loss:8.380122036757683e-05\n",
      "train loss:0.000141126792765389\n",
      "train loss:0.0001308919630698611\n",
      "train loss:0.00016970711483901195\n",
      "train loss:0.00014654092584165686\n",
      "train loss:0.00019329987081157744\n",
      "train loss:0.00030994824103948085\n",
      "train loss:0.0002718569364502686\n",
      "train loss:0.0006211929192047372\n",
      "train loss:0.0009489985046251275\n",
      "train loss:0.0015123463202296778\n",
      "train loss:0.00024072164378885626\n",
      "train loss:0.0008615999599194227\n",
      "train loss:0.0005277726027785053\n",
      "train loss:0.0016398895218198892\n",
      "train loss:0.00013677335313835086\n",
      "train loss:0.0006591397748876283\n",
      "train loss:0.001948025414787944\n",
      "train loss:0.0007005021107520268\n",
      "train loss:0.0007683253951878764\n",
      "train loss:0.00077105990077334\n",
      "train loss:0.00021926690607084465\n",
      "train loss:0.00012364560384369613\n",
      "train loss:0.001702507521709628\n",
      "train loss:0.00037851395046936264\n",
      "train loss:0.00011683570861734501\n",
      "train loss:0.015245692702631621\n",
      "train loss:0.0011873560308728688\n",
      "train loss:0.00018558373790606268\n",
      "train loss:0.00011494050950427061\n",
      "train loss:0.0030070293899890983\n",
      "train loss:0.0019777903563660253\n",
      "train loss:0.0012851663918103105\n",
      "train loss:5.95571126425297e-05\n",
      "train loss:0.0029890811761595377\n",
      "train loss:0.0012931497985193064\n",
      "train loss:0.0001961359417406367\n",
      "train loss:0.0005738494529036995\n",
      "train loss:0.00010334269157489113\n",
      "train loss:0.0012933057523057498\n",
      "train loss:0.0009622128721117538\n",
      "train loss:0.0009149766852393146\n",
      "train loss:0.002102521049343734\n",
      "train loss:0.000446623369701838\n",
      "train loss:0.002057134023990968\n",
      "train loss:0.0006458803682965584\n",
      "train loss:0.001007957591995467\n",
      "train loss:0.0030310160615092396\n",
      "train loss:0.0012258194122942282\n",
      "train loss:0.0005785532676610268\n",
      "train loss:0.0017047941864436586\n",
      "train loss:0.006927291521904003\n",
      "train loss:0.00015061034974247283\n",
      "train loss:0.0011796058861956319\n",
      "train loss:0.0004525404987252617\n",
      "train loss:0.0004903475157040702\n",
      "train loss:0.006768997812608714\n",
      "train loss:0.0004278678867086394\n",
      "train loss:0.0003551263273994225\n",
      "train loss:0.00042240965042651755\n",
      "train loss:0.0005777979482771605\n",
      "train loss:0.0002757191051441643\n",
      "train loss:0.0007501857745418626\n",
      "train loss:0.0003359021262684597\n",
      "train loss:0.000428621122787617\n",
      "train loss:1.3734879866684237e-05\n",
      "train loss:0.0006842626740814101\n",
      "train loss:0.0008057475894577598\n",
      "train loss:0.023338180312474824\n",
      "train loss:0.0016658301045080896\n",
      "train loss:0.0005305484611081919\n",
      "train loss:0.004251061222327935\n",
      "train loss:0.00047202558801711004\n",
      "train loss:0.0012700215764898562\n",
      "train loss:0.0003122039650912619\n",
      "train loss:0.0001636599613169798\n",
      "train loss:0.001995910698061928\n",
      "train loss:0.0023528875554306218\n",
      "train loss:5.3197272717884055e-05\n",
      "train loss:0.00283786276289911\n",
      "train loss:0.0004929394612789072\n",
      "train loss:0.001254189523890095\n",
      "train loss:0.004345524864351571\n",
      "train loss:0.0004422098763770959\n",
      "train loss:0.0005682303131787084\n",
      "train loss:0.00013719825089552847\n",
      "train loss:0.0016324687918288234\n",
      "train loss:0.002172478400555275\n",
      "train loss:0.0025510365852813026\n",
      "train loss:0.0003830833912882804\n",
      "train loss:0.0010823758265833556\n",
      "train loss:0.0025851687437345987\n",
      "train loss:0.0016110580454312537\n",
      "train loss:0.0013037238062003588\n",
      "train loss:0.0003056618855886262\n",
      "train loss:6.710305292426255e-05\n",
      "train loss:6.425713879749894e-05\n",
      "train loss:0.007301231983181845\n",
      "train loss:0.0001509971984656872\n",
      "train loss:0.00017061697618450642\n",
      "train loss:0.0029445901333121503\n",
      "train loss:0.00024988679715147563\n",
      "train loss:0.00372116571544106\n",
      "train loss:0.00012713101757824067\n",
      "train loss:0.00047794671704027924\n",
      "train loss:0.00015703742067244963\n",
      "train loss:0.002251049152922214\n",
      "train loss:6.896519822463335e-05\n",
      "train loss:0.0005232659165120021\n",
      "train loss:0.0003650281926732029\n",
      "train loss:0.00011406085927204034\n",
      "train loss:0.022303102852210935\n",
      "train loss:0.0012024376107934538\n",
      "train loss:0.00032423574587931174\n",
      "train loss:0.003518326497328932\n",
      "train loss:9.609777903653626e-05\n",
      "train loss:0.0006616488681021017\n",
      "train loss:0.0012567965288866826\n",
      "train loss:0.0002587516758642197\n",
      "train loss:0.0008253351585004695\n",
      "train loss:9.669815974434961e-05\n",
      "train loss:0.0006069839718864091\n",
      "train loss:0.00042752506338835034\n",
      "train loss:0.00021581541157300192\n",
      "train loss:0.0012124587573632505\n",
      "train loss:0.0001485197927593408\n",
      "train loss:0.0005608388218648012\n",
      "train loss:0.0006615234881038993\n",
      "train loss:0.0011577576325900651\n",
      "train loss:0.00025544393218053697\n",
      "train loss:0.0010913471381564174\n",
      "train loss:0.0003855738855067381\n",
      "train loss:0.012033659731923973\n",
      "train loss:1.4040520999481383e-05\n",
      "train loss:0.0004399488621615522\n",
      "train loss:0.004475675774571057\n",
      "train loss:0.0007639138149397888\n",
      "train loss:0.0005716260175013267\n",
      "train loss:0.006859552095809658\n",
      "train loss:0.0007872485317626331\n",
      "train loss:0.0010989951974685222\n",
      "train loss:0.0002646181992930802\n",
      "train loss:0.00021249851145383605\n",
      "train loss:0.00017882592704961638\n",
      "train loss:0.0034624086650824224\n",
      "train loss:0.004422634280934193\n",
      "train loss:0.0005200668522757146\n",
      "train loss:0.0026351968788644103\n",
      "train loss:0.0014765927467699927\n",
      "train loss:0.00020239054897938153\n",
      "train loss:0.00023024485476879874\n",
      "train loss:0.0010863613470543764\n",
      "train loss:0.0008282113640482745\n",
      "train loss:0.0024956450150434594\n",
      "train loss:0.00036645583792770326\n",
      "train loss:0.0016588553278629628\n",
      "train loss:0.0007824740642355592\n",
      "train loss:0.00045006687485901574\n",
      "train loss:0.0024312641848175647\n",
      "train loss:0.004938801970956975\n",
      "train loss:0.0004632601241930593\n",
      "train loss:0.0017492572170260709\n",
      "train loss:0.0003914733836858461\n",
      "train loss:0.00013795924376966136\n",
      "train loss:0.00018027752223287723\n",
      "train loss:0.000886805079895022\n",
      "train loss:0.00044218196924194537\n",
      "train loss:0.00029180525226467374\n",
      "train loss:7.561616803252877e-05\n",
      "train loss:0.00013939073629781054\n",
      "train loss:0.0008756434615661385\n",
      "train loss:0.0001463058345611032\n",
      "train loss:7.711884178773495e-05\n",
      "train loss:0.002429654652827651\n",
      "train loss:0.00036087822166109245\n",
      "train loss:9.03635793826673e-05\n",
      "train loss:0.0010164178487956374\n",
      "train loss:0.0005509291958710194\n",
      "train loss:0.005575216582839553\n",
      "train loss:0.00077378335069674\n",
      "train loss:0.0016349667631972182\n",
      "train loss:0.00014816656852240909\n",
      "train loss:0.0002954451359813192\n",
      "train loss:0.00021751203920169742\n",
      "train loss:5.5525446539046985e-05\n",
      "train loss:0.0001588712305961523\n",
      "train loss:0.0004209635073697639\n",
      "train loss:0.0027827944910350432\n",
      "train loss:3.024713151498933e-05\n",
      "train loss:0.013743085900262903\n",
      "train loss:0.0021648074470682573\n",
      "train loss:0.0017109059711857052\n",
      "train loss:4.5079249568057985e-05\n",
      "train loss:4.591843989178965e-05\n",
      "train loss:0.001778111628006267\n",
      "train loss:0.00270775409148081\n",
      "train loss:0.0010394850206519892\n",
      "train loss:0.013088188526909295\n",
      "train loss:0.00019892460377839388\n",
      "train loss:0.0006118738768383033\n",
      "train loss:0.0018070695943544649\n",
      "train loss:0.00017339950332424296\n",
      "train loss:0.001159351270438475\n",
      "train loss:0.0003720702869190165\n",
      "train loss:0.00023296422024052105\n",
      "train loss:0.0036331547893200178\n",
      "train loss:0.0026642441463380893\n",
      "train loss:0.0013290651639429327\n",
      "train loss:0.0006551541239538856\n",
      "train loss:0.002217111487949805\n",
      "train loss:0.004012665001466515\n",
      "train loss:0.002772065578818537\n",
      "train loss:0.0009661776430028427\n",
      "train loss:4.219108551300217e-05\n",
      "train loss:0.0003748714497963885\n",
      "train loss:3.979294967271594e-05\n",
      "train loss:0.0019178343623520886\n",
      "train loss:0.000296586473142141\n",
      "train loss:0.004676095390269048\n",
      "train loss:0.0010285816267183547\n",
      "train loss:0.0004476189462491267\n",
      "train loss:0.0005336174215724382\n",
      "train loss:0.0021722536003176128\n",
      "train loss:0.0008404371163927965\n",
      "train loss:0.0026313901448072052\n",
      "train loss:0.003434998219970705\n",
      "train loss:0.0009641336502125227\n",
      "train loss:0.0003916722898432045\n",
      "train loss:0.0003319551956105173\n",
      "train loss:0.0004352158816938955\n",
      "train loss:0.025902475127194412\n",
      "train loss:0.007410658890466434\n",
      "train loss:6.084106543501712e-05\n",
      "train loss:0.0027946343776142097\n",
      "train loss:3.6325771652379956e-05\n",
      "train loss:4.3237537403846534e-05\n",
      "train loss:0.0001310236108420301\n",
      "train loss:0.0010223215104885198\n",
      "train loss:0.000967751959247419\n",
      "train loss:0.0002081294304211077\n",
      "train loss:0.002850551880672097\n",
      "train loss:0.0013206447207436299\n",
      "train loss:0.002103757845325071\n",
      "train loss:0.003412177241400047\n",
      "train loss:0.0068760398013778725\n",
      "train loss:0.006579431389109447\n",
      "train loss:0.0032761902104791935\n",
      "train loss:0.00035933636811164265\n",
      "train loss:0.0008268222748272094\n",
      "train loss:0.0016810988679328395\n",
      "train loss:0.00010140493185676964\n",
      "train loss:0.002922325079888112\n",
      "train loss:0.0004717651618014247\n",
      "train loss:8.08618014334259e-05\n",
      "train loss:0.01766893124980266\n",
      "train loss:0.00048328422992744856\n",
      "train loss:0.001787672475108709\n",
      "train loss:0.00034730824774329886\n",
      "train loss:0.00349555688578699\n",
      "train loss:0.00027367751986500624\n",
      "train loss:0.0009414076477904555\n",
      "train loss:0.0006001030937036235\n",
      "train loss:5.9956205964462594e-05\n",
      "train loss:0.002264046822625823\n",
      "train loss:0.00026550069669889893\n",
      "train loss:0.0016629651032158748\n",
      "train loss:0.0016294646895856602\n",
      "train loss:0.0014339954411054798\n",
      "train loss:0.0006441164379230778\n",
      "train loss:0.0001435122611581146\n",
      "train loss:2.6236397667696977e-05\n",
      "train loss:0.0003091615691910499\n",
      "train loss:0.0009387368336204654\n",
      "train loss:0.0016442019646173208\n",
      "train loss:7.889529737052761e-05\n",
      "train loss:0.0015812080503099792\n",
      "train loss:0.0062934800096606895\n",
      "train loss:4.8764369625297725e-05\n",
      "train loss:0.0004540617907151058\n",
      "train loss:0.0006595147315466165\n",
      "train loss:0.0023184839683626675\n",
      "train loss:0.0009557371984423006\n",
      "train loss:0.00011889884832603192\n",
      "train loss:0.0001523687704786262\n",
      "train loss:4.712081696252814e-05\n",
      "train loss:0.00021412116357640682\n",
      "train loss:8.654417737441786e-05\n",
      "train loss:0.0011892296200926946\n",
      "train loss:0.0014170980672011354\n",
      "train loss:0.0007615580510999096\n",
      "train loss:0.0005104761027733814\n",
      "train loss:0.00011638273767671362\n",
      "train loss:0.00019896658103334654\n",
      "train loss:0.030557462578610313\n",
      "train loss:0.0011124895534770842\n",
      "train loss:7.233777795473699e-05\n",
      "train loss:0.0005943852094354577\n",
      "train loss:0.00041302126253788213\n",
      "train loss:0.003613751614861157\n",
      "train loss:0.0019757873949677826\n",
      "train loss:3.237445447373842e-05\n",
      "train loss:0.0013228327073131834\n",
      "train loss:0.00035676421146626716\n",
      "train loss:0.0004380491129486469\n",
      "train loss:0.007519571242324004\n",
      "train loss:0.00037155722889202584\n",
      "train loss:0.0033021206925893942\n",
      "train loss:0.0021343135108143952\n",
      "train loss:0.007915637398187271\n",
      "train loss:0.00030682215001136834\n",
      "train loss:0.00048539019764252185\n",
      "train loss:0.00014934323597944873\n",
      "train loss:0.000289851156515885\n",
      "train loss:0.00016569592656684153\n",
      "train loss:0.000390080568673068\n",
      "train loss:0.0003265717856983453\n",
      "train loss:0.00010226538307367022\n",
      "train loss:0.00021420351802883917\n",
      "train loss:0.000133081441041346\n",
      "train loss:0.02322438287831314\n",
      "train loss:0.0007953348658340827\n",
      "train loss:0.0007711381839452707\n",
      "train loss:0.0003919736428530916\n",
      "train loss:0.0008003381264135459\n",
      "train loss:0.00016256612131338897\n",
      "train loss:0.0010964166340821154\n",
      "train loss:0.0008301750836944696\n",
      "train loss:0.00015059019113465805\n",
      "train loss:0.0012496465855048283\n",
      "train loss:0.0002899904643491097\n",
      "train loss:0.000135722372486031\n",
      "train loss:0.0010422171860702256\n",
      "train loss:0.0009141919003128221\n",
      "train loss:0.0010801841540996468\n",
      "train loss:0.0010327453721712977\n",
      "train loss:0.0008666751940082197\n",
      "train loss:0.0011609789593854081\n",
      "train loss:0.001062617708150416\n",
      "train loss:0.0004356338036573606\n",
      "train loss:0.0019284178471716692\n",
      "train loss:0.0025506362739005483\n",
      "train loss:0.00010871989211200325\n",
      "train loss:0.0005255451170893951\n",
      "train loss:0.00010622836246000839\n",
      "train loss:1.81991124247032e-05\n",
      "train loss:0.0018197237989724033\n",
      "train loss:0.001192675051150578\n",
      "train loss:0.0001741463603970166\n",
      "train loss:0.0008163542839042378\n",
      "train loss:0.000660817561175339\n",
      "train loss:0.0002759896257370579\n",
      "train loss:0.0008799114175607946\n",
      "train loss:0.0017875176378432194\n",
      "train loss:0.0027194460614952324\n",
      "train loss:0.0002633778818558363\n",
      "train loss:0.0016747158490442477\n",
      "train loss:0.00020622790959466764\n",
      "train loss:0.00035494216699913416\n",
      "train loss:0.0015494403772315672\n",
      "train loss:6.592597269425635e-05\n",
      "train loss:0.00090591435697292\n",
      "train loss:0.0007988837459320822\n",
      "train loss:0.0019176587414416963\n",
      "train loss:0.0006411226991678474\n",
      "train loss:0.00010020729478533496\n",
      "train loss:0.002058342653565722\n",
      "train loss:0.0015114197576766972\n",
      "train loss:0.0004935388770540463\n",
      "train loss:0.001070893726432896\n",
      "train loss:1.4553855896896342e-05\n",
      "train loss:0.000631403599870723\n",
      "train loss:0.0013076353704220783\n",
      "train loss:8.561198963705252e-06\n",
      "train loss:7.716559229419113e-05\n",
      "train loss:0.0033531901347878456\n",
      "train loss:0.00269987934239448\n",
      "train loss:0.0001517997726117705\n",
      "train loss:0.000338014929695621\n",
      "train loss:0.0031627548043052767\n",
      "train loss:0.0008925463163643882\n",
      "train loss:0.0011843524852143876\n",
      "train loss:0.0014241527809056207\n",
      "train loss:0.0032569730015566145\n",
      "train loss:0.00023575240841162818\n",
      "train loss:0.00013587822443174725\n",
      "train loss:0.00046373620558729943\n",
      "train loss:0.00016004155035599811\n",
      "train loss:0.0001055510184539286\n",
      "train loss:0.0005181239237735955\n",
      "train loss:0.00309064265831273\n",
      "train loss:0.0003237560304521501\n",
      "train loss:0.00018136512577409793\n",
      "train loss:0.0005585622944639064\n",
      "train loss:0.005199493467330143\n",
      "train loss:0.0017387115788395934\n",
      "train loss:0.0002807495268224448\n",
      "train loss:0.0001838242950658042\n",
      "train loss:0.0004220446124794635\n",
      "train loss:0.0004900069692145785\n",
      "train loss:0.0003712604667217969\n",
      "train loss:0.00021569715076703115\n",
      "train loss:0.0017499215962478354\n",
      "train loss:0.0002607883171565667\n",
      "train loss:0.0012914791631835456\n",
      "train loss:0.0037693381957273854\n",
      "train loss:0.003973745284401756\n",
      "train loss:0.0001074502115569086\n",
      "train loss:0.0005109103167571232\n",
      "train loss:0.00020632667429407688\n",
      "train loss:0.00038085239072610093\n",
      "train loss:0.0017407969691928726\n",
      "train loss:0.000389485866729711\n",
      "train loss:0.001959131459149034\n",
      "train loss:0.000298348538705075\n",
      "train loss:0.0007102668019270858\n",
      "train loss:0.003247252686061909\n",
      "train loss:0.0004511128138193173\n",
      "train loss:0.0014624776798514494\n",
      "train loss:0.003493095300483723\n",
      "train loss:0.0007458997838974042\n",
      "train loss:0.00011423038514568119\n",
      "train loss:0.005129901386901151\n",
      "train loss:0.00013057697710058616\n",
      "train loss:0.006258360594743597\n",
      "train loss:0.00328673025722593\n",
      "train loss:0.0005330096021499641\n",
      "train loss:0.0008075310592616842\n",
      "train loss:3.748401766660277e-05\n",
      "train loss:0.00012707131035985284\n",
      "train loss:0.0006213214269982044\n",
      "train loss:0.0004035504124388793\n",
      "train loss:0.001995266864201894\n",
      "train loss:0.0016479636254446504\n",
      "train loss:0.002858648773945487\n",
      "train loss:0.002234523704919041\n",
      "train loss:0.0012824777436450604\n",
      "train loss:0.0003126533516864805\n",
      "train loss:0.0006828024474009525\n",
      "train loss:8.615877018919549e-05\n",
      "train loss:0.0001931566817872372\n",
      "train loss:0.00028578199184494014\n",
      "train loss:0.0007636553817465743\n",
      "train loss:4.6288781972002824e-05\n",
      "train loss:0.011546599214239825\n",
      "train loss:0.0005244798042766707\n",
      "train loss:0.0008299455636372406\n",
      "train loss:0.0008430156490416355\n",
      "train loss:0.0003487816836599071\n",
      "train loss:0.0002784953943008466\n",
      "train loss:0.0008498485536616099\n",
      "train loss:0.0004137315266299995\n",
      "train loss:0.0009610830576576064\n",
      "train loss:0.002357820538376526\n",
      "train loss:0.0029392593286905206\n",
      "train loss:0.002925316603344176\n",
      "train loss:0.0022924158802740554\n",
      "train loss:0.0002629464988590833\n",
      "train loss:0.001705192229150803\n",
      "train loss:0.00216670300847444\n",
      "train loss:0.0009878201842629661\n",
      "train loss:0.018555628482804586\n",
      "train loss:6.847145008991624e-05\n",
      "train loss:0.0036831367800824206\n",
      "train loss:0.0012240809178542887\n",
      "train loss:0.01207083824080164\n",
      "train loss:0.008679756521471835\n",
      "train loss:6.262627775102775e-05\n",
      "train loss:0.00013234938058112947\n",
      "train loss:1.3337875859417141e-05\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9889\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "network.save_params(\"params_1.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 損失関数の値の推移を描画\n",
    "x = np.arange(len(trainer.train_loss_list))\n",
    "plt.plot(x, trainer.train_loss_list, label='loss')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.xlim(left=0)\n",
    "plt.ylim(0, 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Visualization of Convolutional Neural Networks\n",
    "\n",
    "### 7.6.1 Visualization of Convolutional Neural Networks for First Layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# ランダム初期化後の重み\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学習後の重み\n",
    "network.load_params(\"params_1.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
